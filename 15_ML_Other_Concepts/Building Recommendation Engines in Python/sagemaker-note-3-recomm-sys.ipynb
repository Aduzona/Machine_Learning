{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2772ee20",
   "metadata": {},
   "source": [
    "#  Build a semantic content recommendation system with Amazon SageMaker\n",
    "\n",
    "[git_source](https://github.com/Aduzona/amazon-sagemaker-examples/blob/main/introduction_to_applying_machine_learning/ntm_20newsgroups_topic_modeling/ntm_20newsgroups_topic_model.ipynb)\n",
    "\n",
    "[aws_handson_lab](https://aws.amazon.com/getting-started/hands-on/semantic-content-recommendation-system-amazon-sagemaker/4/)\n",
    "\n",
    "**Introduction**\n",
    "\n",
    "In this lab, you learn how to build a semantic, content recommendation system that combines topic modeling and nearest neighbor techniques for information retrieval using Amazon SageMaker built-in algorithms for [Neural Topic Model (NTM)](https://docs.aws.amazon.com/sagemaker/latest/dg/ntm.html) and [K-Nearest Neighbor (K-NN)](https://docs.aws.amazon.com/sagemaker/latest/dg/k-nearest-neighbors.html).\n",
    "\n",
    "Information retrieval is the science of searching for information in a document, searching for documents themselves, or searching for metadata that describe data. This lab combines the techniques of topic modeling and nearest neighbor for information retrieval. This approach uses topic modeling to generate semantic distribution vectors representing the meaning of documents by topics, and then uses the nearest neighbor technique to index topic vectors to retrieve similar documents for a given input document based on topic similarity. By using Amazon SageMaker built-in algorithms, you do not need to label data and the information retrieval is based on semantic meaning similarity instead of simple string matching.\n",
    "\n",
    "Amazon SageMaker is a fully managed end-to-end machine learning platform that covers the entire lifecycle of machine learning. Amazon SageMaker NTM is an unsupervised learning algorithm that is used to organize a corpus of documents into topics that contain word groupings based on their statistical distribution. Amazon K-Nearest Neighbors (k-NN) is a non parametric, index-based, supervised learning algorithm that can be used for classification and regression tasks.\n",
    "\n",
    "**Why use Amazon SageMaker for information retrieval?**\n",
    "\n",
    "One of the key components in Amazon SageMaker is a list of highly scalable built-in algorithms. This lab uses Amazon SageMaker Neural Topic Model (NTM) Algorithm and Amazon SageMaker K-Nearest Neighbors (K-NN) Algorithm to combine information retrieval techniques and build a recommendation system.\n",
    "\n",
    "Some of the key reasons to use Amazon SageMaker for your information retrieval are:\n",
    "\n",
    "Scalable Training: SageMaker fully manages the underlying infrastructure needed to train models at scale by handling the setup of the instances, data movement between storage and compute, as well as between compute instances and the de-provisioning of compute once the job is completed. With spot training, you can save up to 90% in training costs.\n",
    "Scalable Deployment: Once your models are trained, SageMaker can fully manage both offline (batch) inferences and online deployments of your trained models by creating a hosted endpoint for you. SageMaker handles the automatic scaling of your endpoints to scale up and down based on the incoming traffic.\n",
    "Monitoring: With Amazon SageMaker Model Monitor, you can monitor your model endpoints for drift in your data and emit alarms when data drift is detected to suggest retraining your models.\n",
    "Fully Managed Hyperparameter Tuning: Training ML models often requires a time consuming process of tuning hyperparameters. SageMaker manages hyper-parameter tuning jobs for you; you simply select the number of tuning jobs you want to run in parallel and in total, the metrics you want to monitor and Amazon Sagemaker takes care of the rest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2effd828",
   "metadata": {},
   "source": [
    "## Download and prepare dataset\n",
    "\n",
    "In this module, you download your dataset, preprocess the dataset, separate the dataset into training and validation, then stage the dataset in your Amazon S3 bucket.\n",
    "\n",
    "Your ML model will be trained on the 20newsgroups dataset that contains 20,000 newsgroup posts on 20 topics. The 20newsgroups dataset is curated by Carnegie Mellon University School of Computer Science and publically available from scikit-learn.\n",
    "\n",
    "### 1.  Fetch the dataset\n",
    "\n",
    "To prepare the data, train the ML model, and deploy it, you must first import some libraries and define a few environment variables in your Jupyter notebook environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f64eb1a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit_learn==0.22.2.post1\n",
      "  Downloading scikit_learn-0.22.2.post1-cp36-cp36m-manylinux1_x86_64.whl (7.1 MB)\n",
      "     |████████████████████████████████| 7.1 MB 19.8 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.11.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from scikit_learn==0.22.2.post1) (1.19.5)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from scikit_learn==0.22.2.post1) (1.0.1)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from scikit_learn==0.22.2.post1) (1.5.3)\n",
      "Installing collected packages: scikit-learn\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 0.24.1\n",
      "    Uninstalling scikit-learn-0.24.1:\n",
      "      Successfully uninstalled scikit-learn-0.24.1\n",
      "Successfully installed scikit-learn-0.22.2.post1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install \"scikit_learn==0.22.2.post1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8709838b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.datasets.twenty_newsgroups module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.datasets. Anything that cannot be imported from sklearn.datasets is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import sagemaker\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.datasets.twenty_newsgroups import strip_newsgroup_header, strip_newsgroup_quoting, strip_newsgroup_footer\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')['data']\n",
    "newsgroups_test = fetch_20newsgroups(subset = 'test')['data']\n",
    "NUM_TOPICS = 30\n",
    "NUM_NEIGHBORS = 10\n",
    "BUCKET = 'sagemaker-recom-bucket' # my s3 bucket name\n",
    "PREFIX = '20newsgroups'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7f394d",
   "metadata": {},
   "source": [
    "### 2. Preprocess the data\n",
    "\n",
    "In natural language processing, one of the first tasks before training any machine learning model is preprocessing the raw text data into machine readable numeric values. This process usually requires a sequence of steps.\n",
    "\n",
    "First, you use the APIs provided by scikit-learn to strip any headers, footers and quotes from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6f6d35b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"From: lerxst@wam.umd.edu (where's my thing)\\nSubject: WHAT car is this!?\\nNntp-Posting-Host: rac3.wam.umd.edu\\nOrganization: University of Maryland, College Park\\nLines: 15\\n\\n I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is \\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.\\n\\nThanks,\\n- IL\\n   ---- brought to you by your neighborhood Lerxst ----\\n\\n\\n\\n\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_train[0]# before preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f891fc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(newsgroups_train)):\n",
    "    newsgroups_train[i] = strip_newsgroup_header(newsgroups_train[i])\n",
    "    newsgroups_train[i] = strip_newsgroup_quoting(newsgroups_train[i])\n",
    "    newsgroups_train[i] = strip_newsgroup_footer(newsgroups_train[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca7513c",
   "metadata": {},
   "source": [
    "Now, take a look at the training example again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7e760b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is \\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d784b9f",
   "metadata": {},
   "source": [
    "As you can see, the data is simply plain text paragraphs. In order for this to be machine readable, you need to \"tokenize” this data to numeric format by assigning a “token” to each word in the sentence. You can limit the total number of tokens to 2000 by first counting the most frequent tokens and only retaining the top 2000. This limiting is in place because less frequent words will have a diminishing impact on the topic model and can be ignored. Then, for each of the documents you use a Bag of Words (BoW) model to convert the document into a vector which keeps track of the number of times each token appears in that training example.\n",
    "\n",
    "\n",
    "In this example, you use WordNetLemmatizer, a lemmatizer from the nltk package, and use CountVectorizer in scikit-learn to perform the token counting. WordNetLemmatizer uses nouns as the parts of speech (POS) for lemmatizing words into lemmas. Lemmatization aims to return actual words whereas stemming, another preprocessing approach, can often return non-dictionary words or root stems which are often less useful in machine learning.\n",
    "\n",
    "In the list comprehension, you implement a simple rule: only consider words that are longer than 2 characters, start with a letter and match the token_pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9482e03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#!pip install nltk\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import re\n",
    "token_pattern = re.compile(r\"(?u)\\b\\w\\w+\\b\")\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc) if len(t) >= 2 and re.match(\"[a-z].*\",t) \n",
    "                and re.match(token_pattern, t)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6da30c9",
   "metadata": {},
   "source": [
    "With the tokenizer defined, now you can perform token counting while limiting the vocab_size to 2000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6f5c799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing and counting, this may take a few minutes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 2000\n",
      "Done. Time elapsed: 39.65s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vocab_size = 2000\n",
    "print('Tokenizing and counting, this may take a few minutes...')\n",
    "start_time = time.time()\n",
    "vectorizer = CountVectorizer(input='content', analyzer='word', stop_words='english',\n",
    "                             tokenizer=LemmaTokenizer(), max_features=vocab_size, max_df=0.95, min_df=2)\n",
    "vectors = vectorizer.fit_transform(newsgroups_train)\n",
    "vocab_list = vectorizer.get_feature_names()\n",
    "print('vocab size:', len(vocab_list))\n",
    "\n",
    "# random shuffle\n",
    "idx = np.arange(vectors.shape[0])\n",
    "newidx = np.random.permutation(idx) # this will be the labels fed into the KNN model for training\n",
    "# Need to store these permutations:\n",
    "\n",
    "vectors = vectors[newidx]\n",
    "\n",
    "print('Done. Time elapsed: {:.2f}s'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a0af7eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 3, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e40287",
   "metadata": {},
   "source": [
    "The CountVectorizer API uses three hyperparameters that can help with overfitting or underfitting while training a subsequent model.\n",
    "\n",
    "The first hyperparameter is max_features which you set to be the vocabulary size. As noted, a very large vocabulary consisting of infrequent words can add unnecessary noise to the data, which will cause you to train a poor model.\n",
    "\n",
    "The second and third hyperparameters are max_df and min_df. The min_df parameter ignores words that occur in less than min_df % documents and max_df ignores words that occur in more than max_df % of the documents. The parameter max_df ensures that extremely frequent words that are not captured by the stop words are removed. Generally, this approach is a good practice as the topic model is trying to identify topics by finding distinct groups of words that cluster together into topics. If a few words occur in all of the documents, these words will reduce the expressiveness of the model. Conversely, increasing the min_df parameter ensures that extremely rare words are not included, which reduces the tendency of the model to overfit.  \n",
    "\n",
    "To generate training and validation sets, you first shuffle the BOW vectors generated by the CountVectorizer API. While performing the shuffle, you keep track of the original index as well as the shuffled index. Later in this lab, when you use the KNN model to look for similar documents to an unseen document, you need to know the original index associated with the training data prior to shuffling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a6b382",
   "metadata": {},
   "source": [
    "### 3. Stage the training and validation datasets in Amazon S3\n",
    "\n",
    "With your preprocessing in place, you are now ready to create training and validation datasets and stage them in your S3 bucket.\n",
    "\n",
    "First, convert the vectors to a sparse representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9aa92c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'> float32\n"
     ]
    }
   ],
   "source": [
    "import scipy.sparse as sparse\n",
    "vectors = sparse.csr_matrix(vectors, dtype=np.float32)\n",
    "print(type(vectors), vectors.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877a07ed",
   "metadata": {},
   "source": [
    "Now, split it into training data and test data.\n",
    "\n",
    "* The training data (80%) is used during the model training loop. You use gradient-based optimization to iteratively refine the model parameters. Gradient-based optimization is a way to find model parameter values that minimize the model error, using the gradient of the model loss function.\n",
    "* The test data (remaining 20% of customers) is used to evaluate the performance of the model, and measure how well the trained model generalizes to unseen data.\n",
    "\n",
    "**Note:** The following approach for splitting the data only works if your entire dataset has been shuffled first. Optionally, you can use the sklearn.modelselection train_test_split API to perform the split and set the random_state seed to a numerical value to ensure repeatability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56ae16d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9051"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert data into training and validation data\n",
    "n_train = int(0.8 * vectors.shape[0])\n",
    "n_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31d26b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9051, 2000) (2263, 2000)\n"
     ]
    }
   ],
   "source": [
    "# split train and test\n",
    "train_vectors = vectors[:n_train, :]\n",
    "val_vectors = vectors[n_train:, :]\n",
    "\n",
    "# further split test set into validation set (val_vectors) and test  set (test_vectors)\n",
    "\n",
    "print(train_vectors.shape,val_vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4422b1b0",
   "metadata": {},
   "source": [
    "Next, define the training and validation paths, as well as the output path where the NTM artifacts will be stored after model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f17134a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set location s3://sagemaker-recom-bucket/20newsgroups/train\n",
      "Validation set location s3://sagemaker-recom-bucket/20newsgroups/val\n",
      "Trained model will be saved at s3://sagemaker-recom-bucket/20newsgroups/output\n"
     ]
    }
   ],
   "source": [
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "bucket = BUCKET\n",
    "prefix = PREFIX\n",
    "\n",
    "train_prefix = os.path.join(prefix, 'train')\n",
    "val_prefix = os.path.join(prefix, 'val')\n",
    "output_prefix = os.path.join(prefix, 'output')\n",
    "\n",
    "s3_train_data = os.path.join('s3://', bucket, train_prefix)\n",
    "s3_val_data = os.path.join('s3://', bucket, val_prefix)\n",
    "output_path = os.path.join('s3://', bucket, output_prefix)\n",
    "print('Training set location', s3_train_data)\n",
    "print('Validation set location', s3_val_data)\n",
    "print('Trained model will be saved at', output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cf80ad",
   "metadata": {},
   "source": [
    "The NTM supports both CSV and RecordIO protobuf formats for data in the training, validation, and testing channel. The following helper function converts the raw vectors into RecordIO format, and using the n_parts parameter, optionally breaks the dataset into shards which can be used for distributed training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13aae0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded data to s3://sagemaker-recom-bucket/20newsgroups/train/train_part0.pbr\n",
      "Uploaded data to s3://sagemaker-recom-bucket/20newsgroups/train/train_part1.pbr\n",
      "Uploaded data to s3://sagemaker-recom-bucket/20newsgroups/train/train_part2.pbr\n",
      "Uploaded data to s3://sagemaker-recom-bucket/20newsgroups/train/train_part3.pbr\n",
      "Uploaded data to s3://sagemaker-recom-bucket/20newsgroups/train/train_part4.pbr\n",
      "Uploaded data to s3://sagemaker-recom-bucket/20newsgroups/train/train_part5.pbr\n",
      "Uploaded data to s3://sagemaker-recom-bucket/20newsgroups/train/train_part6.pbr\n",
      "Uploaded data to s3://sagemaker-recom-bucket/20newsgroups/train/train_part7.pbr\n",
      "Uploaded data to s3://sagemaker-recom-bucket/20newsgroups/val/val_part0.pbr\n"
     ]
    }
   ],
   "source": [
    "def split_convert_upload(sparray, bucket, prefix, fname_template='data_part{}.pbr', n_parts=2):\n",
    "    import io\n",
    "    import boto3\n",
    "    import sagemaker.amazon.common as smac\n",
    "    \n",
    "    chunk_size = sparray.shape[0]// n_parts\n",
    "    for i in range(n_parts):\n",
    "\n",
    "        # Calculate start and end indices\n",
    "        start = i*chunk_size\n",
    "        end = (i+1)*chunk_size\n",
    "        if i+1 == n_parts:\n",
    "            end = sparray.shape[0]\n",
    "        \n",
    "        # Convert to record protobuf\n",
    "        buf = io.BytesIO()\n",
    "        smac.write_spmatrix_to_sparse_tensor(array=sparray[start:end], file=buf, labels=None)\n",
    "        buf.seek(0)\n",
    "        \n",
    "        # Upload to s3 location specified by bucket and prefix\n",
    "        fname = os.path.join(prefix, fname_template.format(i))\n",
    "        boto3.resource('s3').Bucket(bucket).Object(fname).upload_fileobj(buf)\n",
    "        print('Uploaded data to s3://{}'.format(os.path.join(bucket, fname)))\n",
    "split_convert_upload(train_vectors, bucket=bucket, prefix=train_prefix, fname_template='train_part{}.pbr', n_parts=8)\n",
    "split_convert_upload(val_vectors, bucket=bucket, prefix=val_prefix, fname_template='val_part{}.pbr', n_parts=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22496ab5",
   "metadata": {},
   "source": [
    "Success! You have prepared and staged your dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0df0a0",
   "metadata": {},
   "source": [
    "In this module, you imported and fetched the dataset you use for your content recommendation system. Then, you prepared the dataset through preprocessing, lemmatization and tokenization. Finally, you split the dataset into training and validation sets, then staged them in your Amazon S3 bucket.\n",
    "\n",
    "In the next module, you training your topic model with the Amazon SageMaker NTM Algorithm and deploy the model to Amazon SageMaker."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918895c1",
   "metadata": {},
   "source": [
    "## Train and deploy the topic model\n",
    "\n",
    "In this module, you use the built-in [Amazon SageMaker Neural Topic Model (NTM) Algorithm](https://docs.aws.amazon.com/sagemaker/latest/dg/ntm.html) to train the topic model.\n",
    "\n",
    "[Amazon SageMaker NTM](https://docs.aws.amazon.com/sagemaker/latest/dg/ntm.html) is an unsupervised learning algorithm that is used to organize a corpus of documents into topics that contain word groupings based on their statistical distribution. Documents that contain frequent occurrences of words such as \"bike\", \"car\", \"train\", \"mileage\", and \"speed\" are likely to share a topic on \"transportation\" for example. Topic modeling can be used to classify or summarize documents based on the topics detected or to retrieve information or recommend content based on topic similarities. The topics from documents that NTM learns are characterized as a latent representation because the topics are inferred from the observed word distributions in the corpus. The semantics of topics are usually inferred by examining the top ranking words they contain. Because the method is unsupervised, only the number of topics, not the topics themselves, are prespecified. In addition, the topics are not guaranteed to align with how a human might naturally categorize documents.\n",
    "In the following steps, you specify your NTM algorithm for the training job, specify infrastructure for the model, set the hyperparameter values to tune the model, and run the model. Then, you deploy the model to an endpoint managed by Amazon SageMaker to make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc0ccfa",
   "metadata": {},
   "source": [
    "### Create and run the training job\n",
    "\n",
    "The built-in Amazon SageMaker algorithms are stored as docker containers in [Amazon Elastic Container Registry (Amazon ECR)](https://aws.amazon.com/ecr/). For model training, you first need to specify the location of the NTM container in Amazon ECR, closest to your region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b36bf761",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The method get_image_uri has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "Defaulting to the only supported framework/algorithm version: 1. Ignoring framework/algorithm version: 1.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "container = get_image_uri(boto3.Session().region_name, 'ntm')\n",
    "\n",
    "#container = sagemaker.image_uris.retrieve(\"ntm\", boto3.Session().region_name, \"latest\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b81606",
   "metadata": {},
   "source": [
    "The [Amazon SageMaker Python SDK](https://sagemaker.readthedocs.io/) includes the [sagemaker.estimator.Estimator estimator](https://sagemaker.readthedocs.io/en/stable/estimators.html#sagemaker.estimator.Estimator). This estimator allows you to specify the infrastructure (Amazon EC2 instance type, number of instances, hyperparameters, output path, and optionally, any security-related settings (virtual private cloud (VPC), security groups, etc.) that may be relevant if we are training our model in a custom VPC of our choice as opposed to an Amazon VPC. The NTM fully takes advantage of GPU hardware and, in general, trains roughly an order of magnitude faster on a GPU than on a CPU. Multi-GPU or multi-instance training further improves training speed roughly linearly if communication overhead is low compared to compute time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "708bff5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_instance_type has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    }
   ],
   "source": [
    "sess = sagemaker.Session()\n",
    "#train_instance_count=2\n",
    "ntm = sagemaker.estimator.Estimator(container,\n",
    "                                    role, \n",
    "                                    instance_count=2,                                     train_instance_type='ml.m4.xlarge',\n",
    "                                    output_path=output_path,\n",
    "                                    sagemaker_session=sess,\n",
    "                                    use_spot_instances=True,\n",
    "                                    max_run=300,\n",
    "                                    max_wait=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b294bc",
   "metadata": {},
   "source": [
    "Now, you can set the hyperparameters for the topic model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db548a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntm.set_hyperparameters(num_topics=NUM_TOPICS, feature_dim=vocab_size, mini_batch_size=128, \n",
    "                        epochs=100, num_patience_epochs=5, tolerance=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3987a0",
   "metadata": {},
   "source": [
    "SageMaker offers two modes for data channels:\n",
    "\n",
    "* **FullyReplicated:** All data files are copied to all workers.\n",
    "* **ShardedByS3Key:** Data files are sharded to different workers, that is, each worker receives a different portion of the full data set.\n",
    "At the time of writing, by default, the Amazon SageMaker Python SDK uses FullyReplicated mode for all data channels. This mode is desirable for validation (test) channel but not as efficient for the training channel, when you use multiple workers.\n",
    "\n",
    "In this case, you want to have each worker go through a different portion of the full dataset to provide different gradients within epochs. You specify distribution to be ShardedByS3Key for the training data channel as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "146f340c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vectors.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d836c678",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9051, 2000)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72e562c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The class sagemaker.session.s3_input has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-03 11:57:20 Starting - Starting the training job...\n",
      "2022-03-03 11:57:21 Starting - Launching requested ML instancesProfilerReport-1646308639: InProgress\n",
      ".........\n",
      "2022-03-03 11:59:04 Starting - Preparing the instances for training..................\n",
      "2022-03-03 12:02:05 Downloading - Downloading input data...\n",
      "2022-03-03 12:02:42 Training - Downloading the training image..\u001b[34mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[34mRunning default environment configuration script\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.7/site-packages/jsonref.py:8: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  from collections import Mapping, MutableMapping, Sequence\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:04 INFO 140501882455872] Reading default configuration from /opt/amazon/lib/python3.7/site-packages/algorithm/default-input.json: {'encoder_layers': 'auto', 'mini_batch_size': '256', 'epochs': '50', 'encoder_layers_activation': 'sigmoid', 'optimizer': 'adadelta', 'tolerance': '0.001', 'num_patience_epochs': '3', 'batch_norm': 'false', 'rescale_gradient': '1.0', 'clip_gradient': 'Inf', 'weight_decay': '0.0', 'learning_rate': '0.01', 'sub_sample': '1.0', '_tuning_objective_metric': '', '_data_format': 'record', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_kvstore': 'auto_gpu'}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:04 INFO 140501882455872] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {'feature_dim': '2000', 'num_topics': '30', 'num_patience_epochs': '5', 'epochs': '100', 'tolerance': '0.001', 'mini_batch_size': '128'}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:04 INFO 140501882455872] Final configuration: {'encoder_layers': 'auto', 'mini_batch_size': '128', 'epochs': '100', 'encoder_layers_activation': 'sigmoid', 'optimizer': 'adadelta', 'tolerance': '0.001', 'num_patience_epochs': '5', 'batch_norm': 'false', 'rescale_gradient': '1.0', 'clip_gradient': 'Inf', 'weight_decay': '0.0', 'learning_rate': '0.01', 'sub_sample': '1.0', '_tuning_objective_metric': '', '_data_format': 'record', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_kvstore': 'auto_gpu', 'feature_dim': '2000', 'num_topics': '30'}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:05 INFO 140501882455872] nvidia-smi: took 0.054 seconds to run.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:05 INFO 140501882455872] nvidia-smi identified 0 GPUs.\u001b[0m\n",
      "\n",
      "2022-03-03 12:03:10 Training - Training image download completed. Training in progress.\u001b[34mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[34mRunning default environment configuration script\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:10 INFO 140501882455872] Launching parameter server for role server\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:10 INFO 140501882455872] {'ENVROOT': '/opt/amazon', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'HOSTNAME': 'ip-10-0-107-49.ec2.internal', 'TRAINING_JOB_NAME': 'ntm-2022-03-03-11-57-19-868', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-east-1:731670586362:training-job/ntm-2022-03-03-11-57-19-868', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/ab790684-c1da-48aa-a900-a1e5c2da786e', 'CANONICAL_ENVROOT': '/opt/amazon', 'PYTHONUNBUFFERED': 'TRUE', 'NVIDIA_VISIBLE_DEVICES': 'void', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python3.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'PWD': '/', 'LANG': 'en_US.utf8', 'AWS_REGION': 'us-east-1', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'HOME': '/root', 'SHLVL': '1', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'OMP_NUM_THREADS': '2', 'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/4381ece2-399e-4fcc-964b-e5a509cb4a65', 'DMLC_INTERFACE': 'eth0', 'ECS_CONTAINER_METADATA_URI_V4': 'http://169.254.170.2/v4/4381ece2-399e-4fcc-964b-e5a509cb4a65', 'SAGEMAKER_HTTP_PORT': '8080', 'SAGEMAKER_DATA_PATH': '/opt/ml'}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:10 INFO 140501882455872] envs={'ENVROOT': '/opt/amazon', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'HOSTNAME': 'ip-10-0-107-49.ec2.internal', 'TRAINING_JOB_NAME': 'ntm-2022-03-03-11-57-19-868', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-east-1:731670586362:training-job/ntm-2022-03-03-11-57-19-868', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/ab790684-c1da-48aa-a900-a1e5c2da786e', 'CANONICAL_ENVROOT': '/opt/amazon', 'PYTHONUNBUFFERED': 'TRUE', 'NVIDIA_VISIBLE_DEVICES': 'void', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python3.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'PWD': '/', 'LANG': 'en_US.utf8', 'AWS_REGION': 'us-east-1', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'HOME': '/root', 'SHLVL': '1', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'OMP_NUM_THREADS': '2', 'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/4381ece2-399e-4fcc-964b-e5a509cb4a65', 'DMLC_INTERFACE': 'eth0', 'ECS_CONTAINER_METADATA_URI_V4': 'http://169.254.170.2/v4/4381ece2-399e-4fcc-964b-e5a509cb4a65', 'SAGEMAKER_HTTP_PORT': '8080', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'DMLC_ROLE': 'server', 'DMLC_PS_ROOT_URI': '10.0.68.171', 'DMLC_PS_ROOT_PORT': '9000', 'DMLC_NUM_SERVER': '2', 'DMLC_NUM_WORKER': '2'}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:10 INFO 140501882455872] Environment: {'ENVROOT': '/opt/amazon', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'HOSTNAME': 'ip-10-0-107-49.ec2.internal', 'TRAINING_JOB_NAME': 'ntm-2022-03-03-11-57-19-868', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-east-1:731670586362:training-job/ntm-2022-03-03-11-57-19-868', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/ab790684-c1da-48aa-a900-a1e5c2da786e', 'CANONICAL_ENVROOT': '/opt/amazon', 'PYTHONUNBUFFERED': 'TRUE', 'NVIDIA_VISIBLE_DEVICES': 'void', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python3.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'PWD': '/', 'LANG': 'en_US.utf8', 'AWS_REGION': 'us-east-1', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'HOME': '/root', 'SHLVL': '1', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'OMP_NUM_THREADS': '2', 'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/4381ece2-399e-4fcc-964b-e5a509cb4a65', 'DMLC_INTERFACE': 'eth0', 'ECS_CONTAINER_METADATA_URI_V4': 'http://169.254.170.2/v4/4381ece2-399e-4fcc-964b-e5a509cb4a65', 'SAGEMAKER_HTTP_PORT': '8080', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'DMLC_ROLE': 'worker', 'DMLC_PS_ROOT_URI': '10.0.68.171', 'DMLC_PS_ROOT_PORT': '9000', 'DMLC_NUM_SERVER': '2', 'DMLC_NUM_WORKER': '2'}\u001b[0m\n",
      "\u001b[35mProcess 35 is a shell:server.\u001b[0m\n",
      "\u001b[35mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:10 INFO 140501882455872] Using default worker.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:10 INFO 140501882455872] Checkpoint loading and saving are disabled.\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:03:10.605] [tensorio] [warning] TensorIO is already initialized; ignoring the initialization routine.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:10 INFO 140501882455872] Initializing\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:10 INFO 140501882455872] None\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:10 INFO 140501882455872] vocab.txt\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:10 INFO 140501882455872] Vocab file is not provided\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:10 INFO 140501882455872] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:10 INFO 140501882455872] Create Store: dist_async\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.7/site-packages/jsonref.py:8: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  from collections import Mapping, MutableMapping, Sequence\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:14 INFO 139667671275328] Reading default configuration from /opt/amazon/lib/python3.7/site-packages/algorithm/default-input.json: {'encoder_layers': 'auto', 'mini_batch_size': '256', 'epochs': '50', 'encoder_layers_activation': 'sigmoid', 'optimizer': 'adadelta', 'tolerance': '0.001', 'num_patience_epochs': '3', 'batch_norm': 'false', 'rescale_gradient': '1.0', 'clip_gradient': 'Inf', 'weight_decay': '0.0', 'learning_rate': '0.01', 'sub_sample': '1.0', '_tuning_objective_metric': '', '_data_format': 'record', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_kvstore': 'auto_gpu'}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:14 INFO 139667671275328] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {'feature_dim': '2000', 'num_topics': '30', 'num_patience_epochs': '5', 'epochs': '100', 'tolerance': '0.001', 'mini_batch_size': '128'}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:14 INFO 139667671275328] Final configuration: {'encoder_layers': 'auto', 'mini_batch_size': '128', 'epochs': '100', 'encoder_layers_activation': 'sigmoid', 'optimizer': 'adadelta', 'tolerance': '0.001', 'num_patience_epochs': '5', 'batch_norm': 'false', 'rescale_gradient': '1.0', 'clip_gradient': 'Inf', 'weight_decay': '0.0', 'learning_rate': '0.01', 'sub_sample': '1.0', '_tuning_objective_metric': '', '_data_format': 'record', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_kvstore': 'auto_gpu', 'feature_dim': '2000', 'num_topics': '30'}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:14 INFO 139667671275328] nvidia-smi: took 0.054 seconds to run.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:14 INFO 139667671275328] nvidia-smi identified 0 GPUs.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:14 INFO 139667671275328] Launching parameter server for role scheduler\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:14 INFO 139667671275328] {'ENVROOT': '/opt/amazon', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'HOSTNAME': 'ip-10-0-68-171.ec2.internal', 'TRAINING_JOB_NAME': 'ntm-2022-03-03-11-57-19-868', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-east-1:731670586362:training-job/ntm-2022-03-03-11-57-19-868', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/4299684e-d055-453d-916d-cc9c9962fc97', 'CANONICAL_ENVROOT': '/opt/amazon', 'PYTHONUNBUFFERED': 'TRUE', 'NVIDIA_VISIBLE_DEVICES': 'void', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python3.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'PWD': '/', 'LANG': 'en_US.utf8', 'AWS_REGION': 'us-east-1', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'HOME': '/root', 'SHLVL': '1', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'OMP_NUM_THREADS': '2', 'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/ba6bed0d-85c7-44a4-8253-a5c9080134dc', 'DMLC_INTERFACE': 'eth0', 'ECS_CONTAINER_METADATA_URI_V4': 'http://169.254.170.2/v4/ba6bed0d-85c7-44a4-8253-a5c9080134dc', 'SAGEMAKER_HTTP_PORT': '8080', 'SAGEMAKER_DATA_PATH': '/opt/ml'}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:14 INFO 139667671275328] envs={'ENVROOT': '/opt/amazon', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'HOSTNAME': 'ip-10-0-68-171.ec2.internal', 'TRAINING_JOB_NAME': 'ntm-2022-03-03-11-57-19-868', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-east-1:731670586362:training-job/ntm-2022-03-03-11-57-19-868', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/4299684e-d055-453d-916d-cc9c9962fc97', 'CANONICAL_ENVROOT': '/opt/amazon', 'PYTHONUNBUFFERED': 'TRUE', 'NVIDIA_VISIBLE_DEVICES': 'void', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python3.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'PWD': '/', 'LANG': 'en_US.utf8', 'AWS_REGION': 'us-east-1', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'HOME': '/root', 'SHLVL': '1', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'OMP_NUM_THREADS': '2', 'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/ba6bed0d-85c7-44a4-8253-a5c9080134dc', 'DMLC_INTERFACE': 'eth0', 'ECS_CONTAINER_METADATA_URI_V4': 'http://169.254.170.2/v4/ba6bed0d-85c7-44a4-8253-a5c9080134dc', 'SAGEMAKER_HTTP_PORT': '8080', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'DMLC_ROLE': 'scheduler', 'DMLC_PS_ROOT_URI': '10.0.68.171', 'DMLC_PS_ROOT_PORT': '9000', 'DMLC_NUM_SERVER': '2', 'DMLC_NUM_WORKER': '2'}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:14 INFO 139667671275328] Launching parameter server for role server\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:14 INFO 139667671275328] {'ENVROOT': '/opt/amazon', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'HOSTNAME': 'ip-10-0-68-171.ec2.internal', 'TRAINING_JOB_NAME': 'ntm-2022-03-03-11-57-19-868', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-east-1:731670586362:training-job/ntm-2022-03-03-11-57-19-868', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/4299684e-d055-453d-916d-cc9c9962fc97', 'CANONICAL_ENVROOT': '/opt/amazon', 'PYTHONUNBUFFERED': 'TRUE', 'NVIDIA_VISIBLE_DEVICES': 'void', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python3.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'PWD': '/', 'LANG': 'en_US.utf8', 'AWS_REGION': 'us-east-1', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'HOME': '/root', 'SHLVL': '1', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'OMP_NUM_THREADS': '2', 'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/ba6bed0d-85c7-44a4-8253-a5c9080134dc', 'DMLC_INTERFACE': 'eth0', 'ECS_CONTAINER_METADATA_URI_V4': 'http://169.254.170.2/v4/ba6bed0d-85c7-44a4-8253-a5c9080134dc', 'SAGEMAKER_HTTP_PORT': '8080', 'SAGEMAKER_DATA_PATH': '/opt/ml'}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:14 INFO 139667671275328] envs={'ENVROOT': '/opt/amazon', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'HOSTNAME': 'ip-10-0-68-171.ec2.internal', 'TRAINING_JOB_NAME': 'ntm-2022-03-03-11-57-19-868', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-east-1:731670586362:training-job/ntm-2022-03-03-11-57-19-868', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/4299684e-d055-453d-916d-cc9c9962fc97', 'CANONICAL_ENVROOT': '/opt/amazon', 'PYTHONUNBUFFERED': 'TRUE', 'NVIDIA_VISIBLE_DEVICES': 'void', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python3.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'PWD': '/', 'LANG': 'en_US.utf8', 'AWS_REGION': 'us-east-1', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'HOME': '/root', 'SHLVL': '1', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'OMP_NUM_THREADS': '2', 'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/ba6bed0d-85c7-44a4-8253-a5c9080134dc', 'DMLC_INTERFACE': 'eth0', 'ECS_CONTAINER_METADATA_URI_V4': 'http://169.254.170.2/v4/ba6bed0d-85c7-44a4-8253-a5c9080134dc', 'SAGEMAKER_HTTP_PORT': '8080', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'DMLC_ROLE': 'server', 'DMLC_PS_ROOT_URI': '10.0.68.171', 'DMLC_PS_ROOT_PORT': '9000', 'DMLC_NUM_SERVER': '2', 'DMLC_NUM_WORKER': '2'}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:14 INFO 139667671275328] Environment: {'ENVROOT': '/opt/amazon', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'HOSTNAME': 'ip-10-0-68-171.ec2.internal', 'TRAINING_JOB_NAME': 'ntm-2022-03-03-11-57-19-868', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-east-1:731670586362:training-job/ntm-2022-03-03-11-57-19-868', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/4299684e-d055-453d-916d-cc9c9962fc97', 'CANONICAL_ENVROOT': '/opt/amazon', 'PYTHONUNBUFFERED': 'TRUE', 'NVIDIA_VISIBLE_DEVICES': 'void', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python3.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'PWD': '/', 'LANG': 'en_US.utf8', 'AWS_REGION': 'us-east-1', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'HOME': '/root', 'SHLVL': '1', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'OMP_NUM_THREADS': '2', 'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/ba6bed0d-85c7-44a4-8253-a5c9080134dc', 'DMLC_INTERFACE': 'eth0', 'ECS_CONTAINER_METADATA_URI_V4': 'http://169.254.170.2/v4/ba6bed0d-85c7-44a4-8253-a5c9080134dc', 'SAGEMAKER_HTTP_PORT': '8080', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'DMLC_ROLE': 'worker', 'DMLC_PS_ROOT_URI': '10.0.68.171', 'DMLC_PS_ROOT_PORT': '9000', 'DMLC_NUM_SERVER': '2', 'DMLC_NUM_WORKER': '2'}\u001b[0m\n",
      "\u001b[34mProcess 34 is a shell:scheduler.\u001b[0m\n",
      "\u001b[34mProcess 35 is a shell:server.\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:14 INFO 139667671275328] Using default worker.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:14 INFO 139667671275328] Checkpoint loading and saving are disabled.\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:03:14.244] [tensorio] [warning] TensorIO is already initialized; ignoring the initialization routine.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:14 INFO 139667671275328] Initializing\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:14 INFO 139667671275328] None\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:14 INFO 139667671275328] vocab.txt\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:14 INFO 139667671275328] Vocab file is not provided\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:14 INFO 139667671275328] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:14 INFO 139667671275328] Create Store: dist_async\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646308995.1244345, \"EndTime\": 1646308995.124474, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"Meta\": \"init_train_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Total Batches Seen\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Max Records Seen Between Resets\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Max Batches Seen Between Resets\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Reset Count\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Number of Records Since Last Reset\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Number of Batches Since Last Reset\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}}}\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:03:15.125] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 0, \"duration\": 897, \"num_examples\": 1, \"num_bytes\": 29144}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:15 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:15 INFO 139667671275328] # Starting training for epoch 1\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:03:16.805] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 2, \"duration\": 1680, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:16 INFO 139667671275328] # Finished training epoch 1 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:16 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:16 INFO 139667671275328] Loss (name: value) total: 6.941364447275798\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:16 INFO 139667671275328] Loss (name: value) kld: 0.022103631223823566\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:16 INFO 139667671275328] Loss (name: value) recons: 6.9192608329984875\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:16 INFO 139667671275328] Loss (name: value) logppx: 6.941364447275798\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:16 INFO 139667671275328] #quality_metric: host=algo-1, epoch=1, train total_loss <loss>=6.941364447275798\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:16 INFO 139667671275328] Timing: train: 1.69s, val: 0.00s, epoch: 1.69s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:16 INFO 139667671275328] #progress_metric: host=algo-1, completed 1.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646308995.1251116, \"EndTime\": 1646308996.8152902, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 0, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Total Batches Seen\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:16 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=2676.135063150337 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:16 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:16 INFO 139667671275328] # Starting training for epoch 2\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:03:18.362] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 5, \"duration\": 1545, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:18 INFO 139667671275328] # Finished training epoch 2 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:18 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:18 INFO 139667671275328] Loss (name: value) total: 6.860765463776058\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:18 INFO 139667671275328] Loss (name: value) kld: 0.0018963330270101626\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:18 INFO 139667671275328] Loss (name: value) recons: 6.858869135379791\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:18 INFO 139667671275328] Loss (name: value) logppx: 6.860765463776058\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:18 INFO 139667671275328] #quality_metric: host=algo-1, epoch=2, train total_loss <loss>=6.860765463776058\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:18 INFO 139667671275328] Timing: train: 1.55s, val: 0.01s, epoch: 1.55s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:18 INFO 139667671275328] #progress_metric: host=algo-1, completed 2.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646308996.8161619, \"EndTime\": 1646308998.3725922, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 1, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 9048.0, \"count\": 1, \"min\": 9048, \"max\": 9048}, \"Total Batches Seen\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:18 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=2906.139624654499 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:18 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:18 INFO 139667671275328] # Starting training for epoch 3\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646308995.1195898, \"EndTime\": 1646308995.119624, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"Meta\": \"init_train_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Total Batches Seen\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Max Records Seen Between Resets\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Max Batches Seen Between Resets\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Reset Count\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Number of Records Since Last Reset\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Number of Batches Since Last Reset\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}}}\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:03:15.120] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 0, \"duration\": 4521, \"num_examples\": 1, \"num_bytes\": 28716}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:15 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:15 INFO 140501882455872] # Starting training for epoch 1\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:03:16.709] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 2, \"duration\": 1588, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:16 INFO 140501882455872] # Finished training epoch 1 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:16 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:16 INFO 140501882455872] Loss (name: value) total: 6.986181146568722\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:16 INFO 140501882455872] Loss (name: value) kld: 0.019022692749988183\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:16 INFO 140501882455872] Loss (name: value) recons: 6.967158476511638\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:16 INFO 140501882455872] Loss (name: value) logppx: 6.986181146568722\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:16 INFO 140501882455872] #quality_metric: host=algo-2, epoch=1, train total_loss <loss>=6.986181146568722\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:16 INFO 140501882455872] Timing: train: 1.60s, val: 0.00s, epoch: 1.60s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:16 INFO 140501882455872] #progress_metric: host=algo-2, completed 1.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646308995.1202629, \"EndTime\": 1646308996.7196443, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 0, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Total Batches Seen\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:16 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=2830.1730793022025 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:16 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:16 INFO 140501882455872] # Starting training for epoch 2\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:03:18.250] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 5, \"duration\": 1530, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:18 INFO 140501882455872] # Finished training epoch 2 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:18 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:18 INFO 140501882455872] Loss (name: value) total: 6.898008730676439\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:18 INFO 140501882455872] Loss (name: value) kld: 0.0017882193246300125\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:18 INFO 140501882455872] Loss (name: value) recons: 6.896220531728533\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:18 INFO 140501882455872] Loss (name: value) logppx: 6.898008730676439\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:18 INFO 140501882455872] #quality_metric: host=algo-2, epoch=2, train total_loss <loss>=6.898008730676439\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:18 INFO 140501882455872] Timing: train: 1.53s, val: 0.00s, epoch: 1.54s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:18 INFO 140501882455872] #progress_metric: host=algo-2, completed 2.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646308996.7200818, \"EndTime\": 1646308998.2572124, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 1, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 9054.0, \"count\": 1, \"min\": 9054, \"max\": 9054}, \"Total Batches Seen\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:18 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=2944.7922750207745 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:18 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:18 INFO 140501882455872] # Starting training for epoch 3\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:03:19.858] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 8, \"duration\": 1484, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:19 INFO 139667671275328] # Finished training epoch 3 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:19 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:19 INFO 139667671275328] Loss (name: value) total: 6.858906533983019\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:19 INFO 139667671275328] Loss (name: value) kld: 0.004575604514684528\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:19 INFO 139667671275328] Loss (name: value) recons: 6.854330917199452\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:19 INFO 139667671275328] Loss (name: value) logppx: 6.858906533983019\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:19 INFO 139667671275328] #quality_metric: host=algo-1, epoch=3, train total_loss <loss>=6.858906533983019\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:19 INFO 139667671275328] Timing: train: 1.49s, val: 0.00s, epoch: 1.49s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:19 INFO 139667671275328] #progress_metric: host=algo-1, completed 3.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646308998.3731022, \"EndTime\": 1646308999.865111, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 2, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 13572.0, \"count\": 1, \"min\": 13572, \"max\": 13572}, \"Total Batches Seen\": {\"sum\": 108.0, \"count\": 1, \"min\": 108, \"max\": 108}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 6.0, \"count\": 1, \"min\": 6, \"max\": 6}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:19 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=3031.0859186443504 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:19 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:19 INFO 139667671275328] # Starting training for epoch 4\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:03:19.911] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 8, \"duration\": 1653, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:19 INFO 140501882455872] # Finished training epoch 3 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:19 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:19 INFO 140501882455872] Loss (name: value) total: 6.891111102369097\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:19 INFO 140501882455872] Loss (name: value) kld: 0.0049736288169191945\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:19 INFO 140501882455872] Loss (name: value) recons: 6.88613748550415\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:19 INFO 140501882455872] Loss (name: value) logppx: 6.891111102369097\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:19 INFO 140501882455872] #quality_metric: host=algo-2, epoch=3, train total_loss <loss>=6.891111102369097\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:19 INFO 140501882455872] Timing: train: 1.66s, val: 0.00s, epoch: 1.66s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:19 INFO 140501882455872] #progress_metric: host=algo-2, completed 3.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646308998.2575486, \"EndTime\": 1646308999.9171221, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 2, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 13581.0, \"count\": 1, \"min\": 13581, \"max\": 13581}, \"Total Batches Seen\": {\"sum\": 108.0, \"count\": 1, \"min\": 108, \"max\": 108}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 6.0, \"count\": 1, \"min\": 6, \"max\": 6}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:19 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=2727.5047332247077 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:19 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:19 INFO 140501882455872] # Starting training for epoch 4\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:03:21.430] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 11, \"duration\": 1513, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:21 INFO 140501882455872] # Finished training epoch 4 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:21 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:21 INFO 140501882455872] Loss (name: value) total: 6.881037288241917\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:21 INFO 140501882455872] Loss (name: value) kld: 0.006480645103794005\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:21 INFO 140501882455872] Loss (name: value) recons: 6.87455674012502\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:21 INFO 140501882455872] Loss (name: value) logppx: 6.881037288241917\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:21 INFO 140501882455872] #quality_metric: host=algo-2, epoch=4, train total_loss <loss>=6.881037288241917\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:21 INFO 140501882455872] Timing: train: 1.51s, val: 0.00s, epoch: 1.52s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:21 INFO 140501882455872] #progress_metric: host=algo-2, completed 4.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646308999.91749, \"EndTime\": 1646309001.436735, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 3, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 18108.0, \"count\": 1, \"min\": 18108, \"max\": 18108}, \"Total Batches Seen\": {\"sum\": 144.0, \"count\": 1, \"min\": 144, \"max\": 144}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 8.0, \"count\": 1, \"min\": 8, \"max\": 8}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:21 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=2979.4181241994456 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:21 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:21 INFO 140501882455872] # Starting training for epoch 5\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:03:21.870] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 11, \"duration\": 2002, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:21 INFO 139667671275328] # Finished training epoch 4 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:21 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:21 INFO 139667671275328] Loss (name: value) total: 6.84793002737893\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:21 INFO 139667671275328] Loss (name: value) kld: 0.007507619616161618\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:21 INFO 139667671275328] Loss (name: value) recons: 6.840422418382433\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:21 INFO 139667671275328] Loss (name: value) logppx: 6.84793002737893\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:21 INFO 139667671275328] #quality_metric: host=algo-1, epoch=4, train total_loss <loss>=6.84793002737893\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:21 INFO 139667671275328] Timing: train: 2.01s, val: 0.01s, epoch: 2.01s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:21 INFO 139667671275328] #progress_metric: host=algo-1, completed 4.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646308999.8660347, \"EndTime\": 1646309001.878836, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 3, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 18096.0, \"count\": 1, \"min\": 18096, \"max\": 18096}, \"Total Batches Seen\": {\"sum\": 144.0, \"count\": 1, \"min\": 144, \"max\": 144}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 8.0, \"count\": 1, \"min\": 8, \"max\": 8}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:21 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=2246.929946346125 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:21 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:21 INFO 139667671275328] # Starting training for epoch 5\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:03:23.745] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 14, \"duration\": 1862, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:23 INFO 139667671275328] # Finished training epoch 5 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:23 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:23 INFO 139667671275328] Loss (name: value) total: 6.842657241556379\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:23 INFO 139667671275328] Loss (name: value) kld: 0.010105339001812454\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:23 INFO 139667671275328] Loss (name: value) recons: 6.832551936308543\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:23 INFO 139667671275328] Loss (name: value) logppx: 6.842657241556379\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:23 INFO 139667671275328] #quality_metric: host=algo-1, epoch=5, train total_loss <loss>=6.842657241556379\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:23 INFO 139667671275328] Timing: train: 1.86s, val: 0.00s, epoch: 1.87s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:23 INFO 139667671275328] #progress_metric: host=algo-1, completed 5.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309001.8811572, \"EndTime\": 1646309003.749788, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 4, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 22620.0, \"count\": 1, \"min\": 22620, \"max\": 22620}, \"Total Batches Seen\": {\"sum\": 180.0, \"count\": 1, \"min\": 180, \"max\": 180}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:23 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=2420.751772121329 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:23 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:23 INFO 139667671275328] # Starting training for epoch 6\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:03:23.025] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 14, \"duration\": 1587, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:23 INFO 140501882455872] # Finished training epoch 5 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:23 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:23 INFO 140501882455872] Loss (name: value) total: 6.876146694024404\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:23 INFO 140501882455872] Loss (name: value) kld: 0.009532667651203357\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:23 INFO 140501882455872] Loss (name: value) recons: 6.866614003976186\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:23 INFO 140501882455872] Loss (name: value) logppx: 6.876146694024404\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:23 INFO 140501882455872] #quality_metric: host=algo-2, epoch=5, train total_loss <loss>=6.876146694024404\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:23 INFO 140501882455872] Timing: train: 1.59s, val: 0.00s, epoch: 1.59s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:23 INFO 140501882455872] #progress_metric: host=algo-2, completed 5.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309001.4373465, \"EndTime\": 1646309003.0318525, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 4, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 22635.0, \"count\": 1, \"min\": 22635, \"max\": 22635}, \"Total Batches Seen\": {\"sum\": 180.0, \"count\": 1, \"min\": 180, \"max\": 180}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:23 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=2838.746023798528 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:23 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:23 INFO 140501882455872] # Starting training for epoch 6\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:03:24.470] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 17, \"duration\": 1437, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:24 INFO 140501882455872] # Finished training epoch 6 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:24 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:24 INFO 140501882455872] Loss (name: value) total: 6.869657430383894\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:24 INFO 140501882455872] Loss (name: value) kld: 0.010901373817533668\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:24 INFO 140501882455872] Loss (name: value) recons: 6.85875607199139\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:24 INFO 140501882455872] Loss (name: value) logppx: 6.869657430383894\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:24 INFO 140501882455872] #quality_metric: host=algo-2, epoch=6, train total_loss <loss>=6.869657430383894\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:24 INFO 140501882455872] patience losses:[6.986181146568722, 6.898008730676439, 6.891111102369097, 6.881037288241917, 6.876146694024404] min patience loss:6.876146694024404 current loss:6.869657430383894 absolute loss difference:0.006489263640509613\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:24 INFO 140501882455872] Timing: train: 1.44s, val: 0.01s, epoch: 1.44s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:24 INFO 140501882455872] #progress_metric: host=algo-2, completed 6.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309003.0322208, \"EndTime\": 1646309004.4772565, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 5, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 27162.0, \"count\": 1, \"min\": 27162, \"max\": 27162}, \"Total Batches Seen\": {\"sum\": 216.0, \"count\": 1, \"min\": 216, \"max\": 216}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 12.0, \"count\": 1, \"min\": 12, \"max\": 12}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:24 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=3132.3899142518057 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:24 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:24 INFO 140501882455872] # Starting training for epoch 7\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:03:25.288] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 17, \"duration\": 1530, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:25 INFO 139667671275328] # Finished training epoch 6 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:25 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:25 INFO 139667671275328] Loss (name: value) total: 6.835203720463647\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:25 INFO 139667671275328] Loss (name: value) kld: 0.010968341291623397\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:25 INFO 139667671275328] Loss (name: value) recons: 6.824235399564107\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:25 INFO 139667671275328] Loss (name: value) logppx: 6.835203720463647\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:25 INFO 139667671275328] #quality_metric: host=algo-1, epoch=6, train total_loss <loss>=6.835203720463647\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:25 INFO 139667671275328] patience losses:[6.941364447275798, 6.860765463776058, 6.858906533983019, 6.84793002737893, 6.842657241556379] min patience loss:6.842657241556379 current loss:6.835203720463647 absolute loss difference:0.007453521092732451\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:25 INFO 139667671275328] Timing: train: 1.54s, val: 0.00s, epoch: 1.54s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:25 INFO 139667671275328] #progress_metric: host=algo-1, completed 6.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309003.7501433, \"EndTime\": 1646309005.293814, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 5, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 27144.0, \"count\": 1, \"min\": 27144, \"max\": 27144}, \"Total Batches Seen\": {\"sum\": 216.0, \"count\": 1, \"min\": 216, \"max\": 216}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 12.0, \"count\": 1, \"min\": 12, \"max\": 12}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:25 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=2930.283202486173 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:25 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:25 INFO 139667671275328] # Starting training for epoch 7\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:03:26.019] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 20, \"duration\": 1541, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:26 INFO 140501882455872] # Finished training epoch 7 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:26 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:26 INFO 140501882455872] Loss (name: value) total: 6.871635569466485\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:26 INFO 140501882455872] Loss (name: value) kld: 0.0127042879919625\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:26 INFO 140501882455872] Loss (name: value) recons: 6.858931183815002\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:26 INFO 140501882455872] Loss (name: value) logppx: 6.871635569466485\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:26 INFO 140501882455872] #quality_metric: host=algo-2, epoch=7, train total_loss <loss>=6.871635569466485\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:26 INFO 140501882455872] patience losses:[6.898008730676439, 6.891111102369097, 6.881037288241917, 6.876146694024404, 6.869657430383894] min patience loss:6.869657430383894 current loss:6.871635569466485 absolute loss difference:0.001978139082591035\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:26 INFO 140501882455872] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:26 INFO 140501882455872] Timing: train: 1.54s, val: 0.00s, epoch: 1.54s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:26 INFO 140501882455872] #progress_metric: host=algo-2, completed 7.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309004.47763, \"EndTime\": 1646309006.021309, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 6, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 31689.0, \"count\": 1, \"min\": 31689, \"max\": 31689}, \"Total Batches Seen\": {\"sum\": 252.0, \"count\": 1, \"min\": 252, \"max\": 252}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 14.0, \"count\": 1, \"min\": 14, \"max\": 14}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:26 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=2932.2268139275266 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:26 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:26 INFO 140501882455872] # Starting training for epoch 8\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:03:26.927] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 20, \"duration\": 1632, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:26 INFO 139667671275328] # Finished training epoch 7 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:26 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:26 INFO 139667671275328] Loss (name: value) total: 6.8325409690539045\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:26 INFO 139667671275328] Loss (name: value) kld: 0.014345989025767066\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:26 INFO 139667671275328] Loss (name: value) recons: 6.818194978766972\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:26 INFO 139667671275328] Loss (name: value) logppx: 6.8325409690539045\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:26 INFO 139667671275328] #quality_metric: host=algo-1, epoch=7, train total_loss <loss>=6.8325409690539045\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:26 INFO 139667671275328] patience losses:[6.860765463776058, 6.858906533983019, 6.84793002737893, 6.842657241556379, 6.835203720463647] min patience loss:6.835203720463647 current loss:6.8325409690539045 absolute loss difference:0.00266275140974237\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:26 INFO 139667671275328] Timing: train: 1.63s, val: 0.01s, epoch: 1.64s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:26 INFO 139667671275328] #progress_metric: host=algo-1, completed 7.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309005.2942822, \"EndTime\": 1646309006.9339986, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 6, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 31668.0, \"count\": 1, \"min\": 31668, \"max\": 31668}, \"Total Batches Seen\": {\"sum\": 252.0, \"count\": 1, \"min\": 252, \"max\": 252}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 14.0, \"count\": 1, \"min\": 14, \"max\": 14}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:26 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=2758.659931137565 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:26 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:26 INFO 139667671275328] # Starting training for epoch 8\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:03:27.534] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 23, \"duration\": 1512, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:27 INFO 140501882455872] # Finished training epoch 8 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:27 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:27 INFO 140501882455872] Loss (name: value) total: 6.866094854142931\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:27 INFO 140501882455872] Loss (name: value) kld: 0.014847646570867963\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:27 INFO 140501882455872] Loss (name: value) recons: 6.851247211297353\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:27 INFO 140501882455872] Loss (name: value) logppx: 6.866094854142931\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:27 INFO 140501882455872] #quality_metric: host=algo-2, epoch=8, train total_loss <loss>=6.866094854142931\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:27 INFO 140501882455872] patience losses:[6.891111102369097, 6.881037288241917, 6.876146694024404, 6.869657430383894, 6.871635569466485] min patience loss:6.869657430383894 current loss:6.866094854142931 absolute loss difference:0.0035625762409630113\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:27 INFO 140501882455872] Timing: train: 1.51s, val: 0.00s, epoch: 1.52s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:27 INFO 140501882455872] #progress_metric: host=algo-2, completed 8.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309006.0218809, \"EndTime\": 1646309007.5398126, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 7, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 36216.0, \"count\": 1, \"min\": 36216, \"max\": 36216}, \"Total Batches Seen\": {\"sum\": 288.0, \"count\": 1, \"min\": 288, \"max\": 288}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 16.0, \"count\": 1, \"min\": 16, \"max\": 16}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:27 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=2981.9176670590186 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:27 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:27 INFO 140501882455872] # Starting training for epoch 9\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:03:28.508] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 23, \"duration\": 1573, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:28 INFO 139667671275328] # Finished training epoch 8 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:28 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:28 INFO 139667671275328] Loss (name: value) total: 6.826531973150042\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:28 INFO 139667671275328] Loss (name: value) kld: 0.016586058178088732\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:28 INFO 139667671275328] Loss (name: value) recons: 6.809945907857683\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:28 INFO 139667671275328] Loss (name: value) logppx: 6.826531973150042\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:28 INFO 139667671275328] #quality_metric: host=algo-1, epoch=8, train total_loss <loss>=6.826531973150042\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:28 INFO 139667671275328] patience losses:[6.858906533983019, 6.84793002737893, 6.842657241556379, 6.835203720463647, 6.8325409690539045] min patience loss:6.8325409690539045 current loss:6.826531973150042 absolute loss difference:0.006008995903862946\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:28 INFO 139667671275328] Timing: train: 1.58s, val: 0.00s, epoch: 1.58s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:28 INFO 139667671275328] #progress_metric: host=algo-1, completed 8.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309006.934395, \"EndTime\": 1646309008.5148442, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 7, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 36192.0, \"count\": 1, \"min\": 36192, \"max\": 36192}, \"Total Batches Seen\": {\"sum\": 288.0, \"count\": 1, \"min\": 288, \"max\": 288}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 16.0, \"count\": 1, \"min\": 16, \"max\": 16}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:28 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=2862.1070466995548 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:28 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:28 INFO 139667671275328] # Starting training for epoch 9\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:03:29.026] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 26, \"duration\": 1485, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:03:30.123] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 26, \"duration\": 1607, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:30 INFO 139667671275328] # Finished training epoch 9 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:30 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:30 INFO 139667671275328] Loss (name: value) total: 6.822794834772746\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:30 INFO 139667671275328] Loss (name: value) kld: 0.019447460330815777\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:30 INFO 139667671275328] Loss (name: value) recons: 6.803347461753422\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:30 INFO 139667671275328] Loss (name: value) logppx: 6.822794834772746\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:30 INFO 139667671275328] #quality_metric: host=algo-1, epoch=9, train total_loss <loss>=6.822794834772746\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:30 INFO 139667671275328] patience losses:[6.84793002737893, 6.842657241556379, 6.835203720463647, 6.8325409690539045, 6.826531973150042] min patience loss:6.826531973150042 current loss:6.822794834772746 absolute loss difference:0.0037371383772955014\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:30 INFO 139667671275328] Timing: train: 1.61s, val: 0.00s, epoch: 1.61s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:30 INFO 139667671275328] #progress_metric: host=algo-1, completed 9.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309008.5152445, \"EndTime\": 1646309010.130614, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 8, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 40716.0, \"count\": 1, \"min\": 40716, \"max\": 40716}, \"Total Batches Seen\": {\"sum\": 324.0, \"count\": 1, \"min\": 324, \"max\": 324}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 18.0, \"count\": 1, \"min\": 18, \"max\": 18}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:30 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=2800.11523558264 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:30 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:30 INFO 139667671275328] # Starting training for epoch 10\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:03:31.823] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 29, \"duration\": 1691, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:31 INFO 139667671275328] # Finished training epoch 10 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:31 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:31 INFO 139667671275328] Loss (name: value) total: 6.814841780397627\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:31 INFO 139667671275328] Loss (name: value) kld: 0.026083115410680573\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:31 INFO 139667671275328] Loss (name: value) recons: 6.788758609029982\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:31 INFO 139667671275328] Loss (name: value) logppx: 6.814841780397627\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:31 INFO 139667671275328] #quality_metric: host=algo-1, epoch=10, train total_loss <loss>=6.814841780397627\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:31 INFO 139667671275328] patience losses:[6.842657241556379, 6.835203720463647, 6.8325409690539045, 6.826531973150042, 6.822794834772746] min patience loss:6.822794834772746 current loss:6.814841780397627 absolute loss difference:0.007953054375119173\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:31 INFO 139667671275328] Timing: train: 1.69s, val: 0.00s, epoch: 1.70s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:31 INFO 139667671275328] #progress_metric: host=algo-1, completed 10.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309010.1313117, \"EndTime\": 1646309011.8286037, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 9, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 45240.0, \"count\": 1, \"min\": 45240, \"max\": 45240}, \"Total Batches Seen\": {\"sum\": 360.0, \"count\": 1, \"min\": 360, \"max\": 360}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:31 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=2664.9819555387644 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:31 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:31 INFO 139667671275328] # Starting training for epoch 11\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:03:33.324] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 32, \"duration\": 1493, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:33 INFO 139667671275328] # Finished training epoch 11 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:33 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:33 INFO 139667671275328] Loss (name: value) total: 6.792824374304877\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:33 INFO 139667671275328] Loss (name: value) kld: 0.036033589548120894\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:33 INFO 139667671275328] Loss (name: value) recons: 6.756790763801998\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:33 INFO 139667671275328] Loss (name: value) logppx: 6.792824374304877\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:33 INFO 139667671275328] #quality_metric: host=algo-1, epoch=11, train total_loss <loss>=6.792824374304877\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:33 INFO 139667671275328] patience losses:[6.835203720463647, 6.8325409690539045, 6.826531973150042, 6.822794834772746, 6.814841780397627] min patience loss:6.814841780397627 current loss:6.792824374304877 absolute loss difference:0.022017406092749603\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:33 INFO 139667671275328] Timing: train: 1.50s, val: 0.00s, epoch: 1.50s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:33 INFO 139667671275328] #progress_metric: host=algo-1, completed 11.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309011.829009, \"EndTime\": 1646309013.3296502, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 10, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 49764.0, \"count\": 1, \"min\": 49764, \"max\": 49764}, \"Total Batches Seen\": {\"sum\": 396.0, \"count\": 1, \"min\": 396, \"max\": 396}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 22.0, \"count\": 1, \"min\": 22, \"max\": 22}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:33 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=3014.277133215103 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:33 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:33 INFO 139667671275328] # Starting training for epoch 12\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:29 INFO 140501882455872] # Finished training epoch 9 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:29 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:29 INFO 140501882455872] Loss (name: value) total: 6.860461466842228\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:29 INFO 140501882455872] Loss (name: value) kld: 0.017022669134247635\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:29 INFO 140501882455872] Loss (name: value) recons: 6.843438777658674\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:29 INFO 140501882455872] Loss (name: value) logppx: 6.860461466842228\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:29 INFO 140501882455872] #quality_metric: host=algo-2, epoch=9, train total_loss <loss>=6.860461466842228\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:29 INFO 140501882455872] patience losses:[6.881037288241917, 6.876146694024404, 6.869657430383894, 6.871635569466485, 6.866094854142931] min patience loss:6.866094854142931 current loss:6.860461466842228 absolute loss difference:0.005633387300703063\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:29 INFO 140501882455872] Timing: train: 1.49s, val: 0.00s, epoch: 1.49s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:29 INFO 140501882455872] #progress_metric: host=algo-2, completed 9.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309007.5401986, \"EndTime\": 1646309009.0315623, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 8, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 40743.0, \"count\": 1, \"min\": 40743, \"max\": 40743}, \"Total Batches Seen\": {\"sum\": 324.0, \"count\": 1, \"min\": 324, \"max\": 324}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 18.0, \"count\": 1, \"min\": 18, \"max\": 18}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:29 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=3035.0803155345625 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:29 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:29 INFO 140501882455872] # Starting training for epoch 10\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:03:30.537] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 29, \"duration\": 1504, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:30 INFO 140501882455872] # Finished training epoch 10 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:30 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:30 INFO 140501882455872] Loss (name: value) total: 6.849921955002679\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:30 INFO 140501882455872] Loss (name: value) kld: 0.020876804237357445\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:30 INFO 140501882455872] Loss (name: value) recons: 6.82904511027866\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:30 INFO 140501882455872] Loss (name: value) logppx: 6.849921955002679\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:30 INFO 140501882455872] #quality_metric: host=algo-2, epoch=10, train total_loss <loss>=6.849921955002679\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:30 INFO 140501882455872] patience losses:[6.876146694024404, 6.869657430383894, 6.871635569466485, 6.866094854142931, 6.860461466842228] min patience loss:6.860461466842228 current loss:6.849921955002679 absolute loss difference:0.010539511839549043\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:30 INFO 140501882455872] Timing: train: 1.51s, val: 0.00s, epoch: 1.51s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:30 INFO 140501882455872] #progress_metric: host=algo-2, completed 10.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309009.0319407, \"EndTime\": 1646309010.543388, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 9, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 45270.0, \"count\": 1, \"min\": 45270, \"max\": 45270}, \"Total Batches Seen\": {\"sum\": 360.0, \"count\": 1, \"min\": 360, \"max\": 360}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:30 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=2994.599928271976 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:30 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:30 INFO 140501882455872] # Starting training for epoch 11\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:03:32.072] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 32, \"duration\": 1528, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:32 INFO 140501882455872] # Finished training epoch 11 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:32 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:32 INFO 140501882455872] Loss (name: value) total: 6.8467277023527355\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:32 INFO 140501882455872] Loss (name: value) kld: 0.027150133754023247\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:32 INFO 140501882455872] Loss (name: value) recons: 6.819577528370751\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:32 INFO 140501882455872] Loss (name: value) logppx: 6.8467277023527355\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:32 INFO 140501882455872] #quality_metric: host=algo-2, epoch=11, train total_loss <loss>=6.8467277023527355\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:32 INFO 140501882455872] patience losses:[6.869657430383894, 6.871635569466485, 6.866094854142931, 6.860461466842228, 6.849921955002679] min patience loss:6.849921955002679 current loss:6.8467277023527355 absolute loss difference:0.00319425264994333\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:32 INFO 140501882455872] Timing: train: 1.53s, val: 0.00s, epoch: 1.53s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:32 INFO 140501882455872] #progress_metric: host=algo-2, completed 11.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309010.544045, \"EndTime\": 1646309012.078748, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 10, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 49797.0, \"count\": 1, \"min\": 49797, \"max\": 49797}, \"Total Batches Seen\": {\"sum\": 396.0, \"count\": 1, \"min\": 396, \"max\": 396}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 22.0, \"count\": 1, \"min\": 22, \"max\": 22}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:32 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=2949.3934822174583 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:32 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:32 INFO 140501882455872] # Starting training for epoch 12\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:03:33.548] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 35, \"duration\": 1468, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:33 INFO 140501882455872] # Finished training epoch 12 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:33 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:33 INFO 140501882455872] Loss (name: value) total: 6.823913276195526\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:33 INFO 140501882455872] Loss (name: value) kld: 0.038065576874133616\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:33 INFO 140501882455872] Loss (name: value) recons: 6.785847736729516\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:33 INFO 140501882455872] Loss (name: value) logppx: 6.823913276195526\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:33 INFO 140501882455872] #quality_metric: host=algo-2, epoch=12, train total_loss <loss>=6.823913276195526\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:33 INFO 140501882455872] patience losses:[6.871635569466485, 6.866094854142931, 6.860461466842228, 6.849921955002679, 6.8467277023527355] min patience loss:6.8467277023527355 current loss:6.823913276195526 absolute loss difference:0.02281442615720941\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:33 INFO 140501882455872] Timing: train: 1.47s, val: 0.00s, epoch: 1.48s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:33 INFO 140501882455872] #progress_metric: host=algo-2, completed 12.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309012.0791094, \"EndTime\": 1646309013.5549457, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 11, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 54324.0, \"count\": 1, \"min\": 54324, \"max\": 54324}, \"Total Batches Seen\": {\"sum\": 432.0, \"count\": 1, \"min\": 432, \"max\": 432}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 24.0, \"count\": 1, \"min\": 24, \"max\": 24}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:33 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=3066.9353021472375 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:33 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:33 INFO 140501882455872] # Starting training for epoch 13\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:03:34.878] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 35, \"duration\": 1547, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:34 INFO 139667671275328] # Finished training epoch 12 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:34 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:34 INFO 139667671275328] Loss (name: value) total: 6.776878979470995\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:34 INFO 139667671275328] Loss (name: value) kld: 0.044381608410427965\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:34 INFO 139667671275328] Loss (name: value) recons: 6.732497347725762\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:34 INFO 139667671275328] Loss (name: value) logppx: 6.776878979470995\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:34 INFO 139667671275328] #quality_metric: host=algo-1, epoch=12, train total_loss <loss>=6.776878979470995\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:34 INFO 139667671275328] patience losses:[6.8325409690539045, 6.826531973150042, 6.822794834772746, 6.814841780397627, 6.792824374304877] min patience loss:6.792824374304877 current loss:6.776878979470995 absolute loss difference:0.015945394833882354\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:34 INFO 139667671275328] Timing: train: 1.55s, val: 0.01s, epoch: 1.56s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:34 INFO 139667671275328] #progress_metric: host=algo-1, completed 12.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309013.3300607, \"EndTime\": 1646309014.8868318, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 11, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 54288.0, \"count\": 1, \"min\": 54288, \"max\": 54288}, \"Total Batches Seen\": {\"sum\": 432.0, \"count\": 1, \"min\": 432, \"max\": 432}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 24.0, \"count\": 1, \"min\": 24, \"max\": 24}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:34 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=2905.631417702318 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:34 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:34 INFO 139667671275328] # Starting training for epoch 13\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:03:35.049] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 38, \"duration\": 1493, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:35 INFO 140501882455872] # Finished training epoch 13 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:35 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:35 INFO 140501882455872] Loss (name: value) total: 6.809986240333981\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:35 INFO 140501882455872] Loss (name: value) kld: 0.046737657135559454\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:35 INFO 140501882455872] Loss (name: value) recons: 6.763248615794712\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:35 INFO 140501882455872] Loss (name: value) logppx: 6.809986240333981\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:35 INFO 140501882455872] #quality_metric: host=algo-2, epoch=13, train total_loss <loss>=6.809986240333981\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:35 INFO 140501882455872] patience losses:[6.866094854142931, 6.860461466842228, 6.849921955002679, 6.8467277023527355, 6.823913276195526] min patience loss:6.823913276195526 current loss:6.809986240333981 absolute loss difference:0.013927035861545534\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:35 INFO 140501882455872] Timing: train: 1.50s, val: 0.00s, epoch: 1.50s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:35 INFO 140501882455872] #progress_metric: host=algo-2, completed 13.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309013.5554163, \"EndTime\": 1646309015.056868, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 12, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 58851.0, \"count\": 1, \"min\": 58851, \"max\": 58851}, \"Total Batches Seen\": {\"sum\": 468.0, \"count\": 1, \"min\": 468, \"max\": 468}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 26.0, \"count\": 1, \"min\": 26, \"max\": 26}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:35 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=3014.7142906362387 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:35 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:35 INFO 140501882455872] # Starting training for epoch 14\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:03:36.593] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 41, \"duration\": 1535, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:36 INFO 140501882455872] # Finished training epoch 14 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:36 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:36 INFO 140501882455872] Loss (name: value) total: 6.789065526591407\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:36 INFO 140501882455872] Loss (name: value) kld: 0.05027679012467464\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:36 INFO 140501882455872] Loss (name: value) recons: 6.738788743813832\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:36 INFO 140501882455872] Loss (name: value) logppx: 6.789065526591407\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:36 INFO 140501882455872] #quality_metric: host=algo-2, epoch=14, train total_loss <loss>=6.789065526591407\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:36 INFO 140501882455872] patience losses:[6.860461466842228, 6.849921955002679, 6.8467277023527355, 6.823913276195526, 6.809986240333981] min patience loss:6.809986240333981 current loss:6.789065526591407 absolute loss difference:0.02092071374257376\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:36 INFO 140501882455872] Timing: train: 1.54s, val: 0.00s, epoch: 1.54s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:36 INFO 140501882455872] #progress_metric: host=algo-2, completed 14.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309015.0574408, \"EndTime\": 1646309016.6007476, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 13, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 63378.0, \"count\": 1, \"min\": 63378, \"max\": 63378}, \"Total Batches Seen\": {\"sum\": 504.0, \"count\": 1, \"min\": 504, \"max\": 504}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 28.0, \"count\": 1, \"min\": 28, \"max\": 28}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:36 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=2932.715940116888 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:36 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:36 INFO 140501882455872] # Starting training for epoch 15\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:03:36.516] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 38, \"duration\": 1628, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:36 INFO 139667671275328] # Finished training epoch 13 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:36 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:36 INFO 139667671275328] Loss (name: value) total: 6.762210693624285\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:36 INFO 139667671275328] Loss (name: value) kld: 0.05071351594395108\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:36 INFO 139667671275328] Loss (name: value) recons: 6.711497187614441\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:36 INFO 139667671275328] Loss (name: value) logppx: 6.762210693624285\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:36 INFO 139667671275328] #quality_metric: host=algo-1, epoch=13, train total_loss <loss>=6.762210693624285\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:36 INFO 139667671275328] patience losses:[6.826531973150042, 6.822794834772746, 6.814841780397627, 6.792824374304877, 6.776878979470995] min patience loss:6.776878979470995 current loss:6.762210693624285 absolute loss difference:0.014668285846710205\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:36 INFO 139667671275328] Timing: train: 1.63s, val: 0.01s, epoch: 1.64s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:36 INFO 139667671275328] #progress_metric: host=algo-1, completed 13.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309014.8871963, \"EndTime\": 1646309016.523486, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 12, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 58812.0, \"count\": 1, \"min\": 58812, \"max\": 58812}, \"Total Batches Seen\": {\"sum\": 468.0, \"count\": 1, \"min\": 468, \"max\": 468}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 26.0, \"count\": 1, \"min\": 26, \"max\": 26}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:36 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=2764.4699250207063 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:36 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:36 INFO 139667671275328] # Starting training for epoch 14\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:03:38.051] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 41, \"duration\": 1527, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:38 INFO 139667671275328] # Finished training epoch 14 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:38 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:38 INFO 139667671275328] Loss (name: value) total: 6.754655930731031\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:38 INFO 139667671275328] Loss (name: value) kld: 0.054691213390065566\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:38 INFO 139667671275328] Loss (name: value) recons: 6.699964748488532\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:38 INFO 139667671275328] Loss (name: value) logppx: 6.754655930731031\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:38 INFO 139667671275328] #quality_metric: host=algo-1, epoch=14, train total_loss <loss>=6.754655930731031\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:38 INFO 139667671275328] patience losses:[6.822794834772746, 6.814841780397627, 6.792824374304877, 6.776878979470995, 6.762210693624285] min patience loss:6.762210693624285 current loss:6.754655930731031 absolute loss difference:0.007554762893253297\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:38 INFO 139667671275328] Timing: train: 1.53s, val: 0.01s, epoch: 1.53s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:38 INFO 139667671275328] #progress_metric: host=algo-1, completed 14.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309016.5238292, \"EndTime\": 1646309018.0586014, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 13, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 63336.0, \"count\": 1, \"min\": 63336, \"max\": 63336}, \"Total Batches Seen\": {\"sum\": 504.0, \"count\": 1, \"min\": 504, \"max\": 504}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 28.0, \"count\": 1, \"min\": 28, \"max\": 28}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:38 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=2946.9496048376177 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:38 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:38 INFO 139667671275328] # Starting training for epoch 15\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:03:38.066] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 44, \"duration\": 1464, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:38 INFO 140501882455872] # Finished training epoch 15 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:38 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:38 INFO 140501882455872] Loss (name: value) total: 6.782608972655402\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:38 INFO 140501882455872] Loss (name: value) kld: 0.05520643392163846\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:38 INFO 140501882455872] Loss (name: value) recons: 6.727402554617988\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:38 INFO 140501882455872] Loss (name: value) logppx: 6.782608972655402\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:38 INFO 140501882455872] #quality_metric: host=algo-2, epoch=15, train total_loss <loss>=6.782608972655402\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:38 INFO 140501882455872] patience losses:[6.849921955002679, 6.8467277023527355, 6.823913276195526, 6.809986240333981, 6.789065526591407] min patience loss:6.789065526591407 current loss:6.782608972655402 absolute loss difference:0.006456553936004639\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:38 INFO 140501882455872] Timing: train: 1.47s, val: 0.01s, epoch: 1.47s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:38 INFO 140501882455872] #progress_metric: host=algo-2, completed 15.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309016.6013575, \"EndTime\": 1646309018.0748641, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 14, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 67905.0, \"count\": 1, \"min\": 67905, \"max\": 67905}, \"Total Batches Seen\": {\"sum\": 540.0, \"count\": 1, \"min\": 540, \"max\": 540}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 30.0, \"count\": 1, \"min\": 30, \"max\": 30}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:38 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=3071.8424534081755 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:38 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:38 INFO 140501882455872] # Starting training for epoch 16\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:03:39.825] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 44, \"duration\": 1765, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:39 INFO 139667671275328] # Finished training epoch 15 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:39 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:39 INFO 139667671275328] Loss (name: value) total: 6.747453914748298\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:39 INFO 139667671275328] Loss (name: value) kld: 0.060155492120732866\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:39 INFO 139667671275328] Loss (name: value) recons: 6.687298430336846\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:39 INFO 139667671275328] Loss (name: value) logppx: 6.747453914748298\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:39 INFO 139667671275328] #quality_metric: host=algo-1, epoch=15, train total_loss <loss>=6.747453914748298\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:39 INFO 139667671275328] patience losses:[6.814841780397627, 6.792824374304877, 6.776878979470995, 6.762210693624285, 6.754655930731031] min patience loss:6.754655930731031 current loss:6.747453914748298 absolute loss difference:0.007202015982733734\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:39 INFO 139667671275328] Timing: train: 1.77s, val: 0.01s, epoch: 1.78s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:39 INFO 139667671275328] #progress_metric: host=algo-1, completed 15.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309018.0593317, \"EndTime\": 1646309019.8351066, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 14, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 67860.0, \"count\": 1, \"min\": 67860, \"max\": 67860}, \"Total Batches Seen\": {\"sum\": 540.0, \"count\": 1, \"min\": 540, \"max\": 540}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 30.0, \"count\": 1, \"min\": 30, \"max\": 30}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:39 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=2547.211435114678 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:39 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:39 INFO 139667671275328] # Starting training for epoch 16\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:03:39.795] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 47, \"duration\": 1719, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:39 INFO 140501882455872] # Finished training epoch 16 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:39 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:39 INFO 140501882455872] Loss (name: value) total: 6.778489073117574\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:39 INFO 140501882455872] Loss (name: value) kld: 0.05907039095958074\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:39 INFO 140501882455872] Loss (name: value) recons: 6.719418671396044\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:39 INFO 140501882455872] Loss (name: value) logppx: 6.778489073117574\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:39 INFO 140501882455872] #quality_metric: host=algo-2, epoch=16, train total_loss <loss>=6.778489073117574\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:39 INFO 140501882455872] patience losses:[6.8467277023527355, 6.823913276195526, 6.809986240333981, 6.789065526591407, 6.782608972655402] min patience loss:6.782608972655402 current loss:6.778489073117574 absolute loss difference:0.004119899537828431\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:39 INFO 140501882455872] Timing: train: 1.72s, val: 0.00s, epoch: 1.72s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:39 INFO 140501882455872] #progress_metric: host=algo-2, completed 16.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309018.0752418, \"EndTime\": 1646309019.8003526, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 15, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 72432.0, \"count\": 1, \"min\": 72432, \"max\": 72432}, \"Total Batches Seen\": {\"sum\": 576.0, \"count\": 1, \"min\": 576, \"max\": 576}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 32.0, \"count\": 1, \"min\": 32, \"max\": 32}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:39 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=2623.8656411991205 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:39 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:39 INFO 140501882455872] # Starting training for epoch 17\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:03:41.402] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 47, \"duration\": 1566, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:41 INFO 139667671275328] # Finished training epoch 16 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:41 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:41 INFO 139667671275328] Loss (name: value) total: 6.737155609660679\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:41 INFO 139667671275328] Loss (name: value) kld: 0.061225291755464345\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:41 INFO 139667671275328] Loss (name: value) recons: 6.675930380821228\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:41 INFO 139667671275328] Loss (name: value) logppx: 6.737155609660679\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:41 INFO 139667671275328] #quality_metric: host=algo-1, epoch=16, train total_loss <loss>=6.737155609660679\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:41 INFO 139667671275328] patience losses:[6.792824374304877, 6.776878979470995, 6.762210693624285, 6.754655930731031, 6.747453914748298] min patience loss:6.747453914748298 current loss:6.737155609660679 absolute loss difference:0.010298305087618864\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:41 INFO 139667671275328] Timing: train: 1.57s, val: 0.01s, epoch: 1.57s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:41 INFO 139667671275328] #progress_metric: host=algo-1, completed 16.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309019.835663, \"EndTime\": 1646309021.40989, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 15, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 72384.0, \"count\": 1, \"min\": 72384, \"max\": 72384}, \"Total Batches Seen\": {\"sum\": 576.0, \"count\": 1, \"min\": 576, \"max\": 576}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 32.0, \"count\": 1, \"min\": 32, \"max\": 32}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:41 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=2873.407388529013 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:41 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:41 INFO 139667671275328] # Starting training for epoch 17\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:03:41.336] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 50, \"duration\": 1534, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:41 INFO 140501882455872] # Finished training epoch 17 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:41 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:41 INFO 140501882455872] Loss (name: value) total: 6.772778974639045\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:41 INFO 140501882455872] Loss (name: value) kld: 0.06098830317043596\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:41 INFO 140501882455872] Loss (name: value) recons: 6.711790687508053\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:41 INFO 140501882455872] Loss (name: value) logppx: 6.772778974639045\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:41 INFO 140501882455872] #quality_metric: host=algo-2, epoch=17, train total_loss <loss>=6.772778974639045\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:41 INFO 140501882455872] patience losses:[6.823913276195526, 6.809986240333981, 6.789065526591407, 6.782608972655402, 6.778489073117574] min patience loss:6.778489073117574 current loss:6.772778974639045 absolute loss difference:0.005710098478528991\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:41 INFO 140501882455872] Timing: train: 1.54s, val: 0.00s, epoch: 1.54s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:41 INFO 140501882455872] #progress_metric: host=algo-2, completed 17.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309019.8007417, \"EndTime\": 1646309021.3424435, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 16, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 76959.0, \"count\": 1, \"min\": 76959, \"max\": 76959}, \"Total Batches Seen\": {\"sum\": 612.0, \"count\": 1, \"min\": 612, \"max\": 612}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 34.0, \"count\": 1, \"min\": 34, \"max\": 34}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:41 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=2936.0508707281006 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:41 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:41 INFO 140501882455872] # Starting training for epoch 18\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:03:43.047] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 50, \"duration\": 1636, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:43 INFO 139667671275328] # Finished training epoch 17 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:43 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:43 INFO 139667671275328] Loss (name: value) total: 6.7365393373701306\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:43 INFO 139667671275328] Loss (name: value) kld: 0.06378390101922883\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:43 INFO 139667671275328] Loss (name: value) recons: 6.672755499680837\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:43 INFO 139667671275328] Loss (name: value) logppx: 6.7365393373701306\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:43 INFO 139667671275328] #quality_metric: host=algo-1, epoch=17, train total_loss <loss>=6.7365393373701306\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:43 INFO 139667671275328] patience losses:[6.776878979470995, 6.762210693624285, 6.754655930731031, 6.747453914748298, 6.737155609660679] min patience loss:6.737155609660679 current loss:6.7365393373701306 absolute loss difference:0.0006162722905482809\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:43 INFO 139667671275328] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:43 INFO 139667671275328] Timing: train: 1.64s, val: 0.01s, epoch: 1.65s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:43 INFO 139667671275328] #progress_metric: host=algo-1, completed 17.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309021.4103105, \"EndTime\": 1646309023.0575418, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 16, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 76908.0, \"count\": 1, \"min\": 76908, \"max\": 76908}, \"Total Batches Seen\": {\"sum\": 612.0, \"count\": 1, \"min\": 612, \"max\": 612}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 34.0, \"count\": 1, \"min\": 34, \"max\": 34}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:43 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=2745.3545416625493 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:43 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:43 INFO 139667671275328] # Starting training for epoch 18\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:03:42.951] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 53, \"duration\": 1608, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:42 INFO 140501882455872] # Finished training epoch 18 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:42 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:42 INFO 140501882455872] Loss (name: value) total: 6.766437186135186\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:42 INFO 140501882455872] Loss (name: value) kld: 0.06405783651603593\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:42 INFO 140501882455872] Loss (name: value) recons: 6.70237934589386\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:42 INFO 140501882455872] Loss (name: value) logppx: 6.766437186135186\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:42 INFO 140501882455872] #quality_metric: host=algo-2, epoch=18, train total_loss <loss>=6.766437186135186\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:42 INFO 140501882455872] patience losses:[6.809986240333981, 6.789065526591407, 6.782608972655402, 6.778489073117574, 6.772778974639045] min patience loss:6.772778974639045 current loss:6.766437186135186 absolute loss difference:0.006341788503858581\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:42 INFO 140501882455872] Timing: train: 1.61s, val: 0.01s, epoch: 1.62s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:42 INFO 140501882455872] #progress_metric: host=algo-2, completed 18.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309021.3427799, \"EndTime\": 1646309022.9592464, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 17, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 81486.0, \"count\": 1, \"min\": 81486, \"max\": 81486}, \"Total Batches Seen\": {\"sum\": 648.0, \"count\": 1, \"min\": 648, \"max\": 648}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:42 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=2800.2290913552747 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:42 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:42 INFO 140501882455872] # Starting training for epoch 19\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:03:44.485] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 56, \"duration\": 1525, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:44 INFO 140501882455872] # Finished training epoch 19 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:44 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:44 INFO 140501882455872] Loss (name: value) total: 6.762062430381775\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:44 INFO 140501882455872] Loss (name: value) kld: 0.06652028579264879\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:44 INFO 140501882455872] Loss (name: value) recons: 6.695542209678226\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:44 INFO 140501882455872] Loss (name: value) logppx: 6.762062430381775\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:44 INFO 140501882455872] #quality_metric: host=algo-2, epoch=19, train total_loss <loss>=6.762062430381775\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:44 INFO 140501882455872] patience losses:[6.789065526591407, 6.782608972655402, 6.778489073117574, 6.772778974639045, 6.766437186135186] min patience loss:6.766437186135186 current loss:6.762062430381775 absolute loss difference:0.004374755753411286\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:44 INFO 140501882455872] Timing: train: 1.53s, val: 0.01s, epoch: 1.53s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:44 INFO 140501882455872] #progress_metric: host=algo-2, completed 19.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309022.9596322, \"EndTime\": 1646309024.4933836, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 18, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 86013.0, \"count\": 1, \"min\": 86013, \"max\": 86013}, \"Total Batches Seen\": {\"sum\": 684.0, \"count\": 1, \"min\": 684, \"max\": 684}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 38.0, \"count\": 1, \"min\": 38, \"max\": 38}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:44 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=2950.5704466115067 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:44 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:44 INFO 140501882455872] # Starting training for epoch 20\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:03:44.814] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 53, \"duration\": 1753, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:44 INFO 139667671275328] # Finished training epoch 18 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:44 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:44 INFO 139667671275328] Loss (name: value) total: 6.727557765112983\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:44 INFO 139667671275328] Loss (name: value) kld: 0.06807662820857432\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:44 INFO 139667671275328] Loss (name: value) recons: 6.659481181038751\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:44 INFO 139667671275328] Loss (name: value) logppx: 6.727557765112983\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:44 INFO 139667671275328] #quality_metric: host=algo-1, epoch=18, train total_loss <loss>=6.727557765112983\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:44 INFO 139667671275328] patience losses:[6.762210693624285, 6.754655930731031, 6.747453914748298, 6.737155609660679, 6.7365393373701306] min patience loss:6.7365393373701306 current loss:6.727557765112983 absolute loss difference:0.008981572257147796\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:44 INFO 139667671275328] Timing: train: 1.76s, val: 0.00s, epoch: 1.76s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:44 INFO 139667671275328] #progress_metric: host=algo-1, completed 18.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309023.0602033, \"EndTime\": 1646309024.8196762, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 17, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 81432.0, \"count\": 1, \"min\": 81432, \"max\": 81432}, \"Total Batches Seen\": {\"sum\": 648.0, \"count\": 1, \"min\": 648, \"max\": 648}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:44 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=2570.4556040039165 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:44 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:44 INFO 139667671275328] # Starting training for epoch 19\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:03:46.388] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 56, \"duration\": 1567, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:46 INFO 139667671275328] # Finished training epoch 19 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:46 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:46 INFO 139667671275328] Loss (name: value) total: 6.727606587939793\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:46 INFO 139667671275328] Loss (name: value) kld: 0.07002168014231655\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:46 INFO 139667671275328] Loss (name: value) recons: 6.657584839397007\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:46 INFO 139667671275328] Loss (name: value) logppx: 6.727606587939793\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:46 INFO 139667671275328] #quality_metric: host=algo-1, epoch=19, train total_loss <loss>=6.727606587939793\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:46 INFO 139667671275328] patience losses:[6.754655930731031, 6.747453914748298, 6.737155609660679, 6.7365393373701306, 6.727557765112983] min patience loss:6.727557765112983 current loss:6.727606587939793 absolute loss difference:4.882282680984673e-05\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:46 INFO 139667671275328] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:46 INFO 139667671275328] Timing: train: 1.57s, val: 0.00s, epoch: 1.57s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:46 INFO 139667671275328] #progress_metric: host=algo-1, completed 19.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309024.8204045, \"EndTime\": 1646309026.3919194, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 18, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 85956.0, \"count\": 1, \"min\": 85956, \"max\": 85956}, \"Total Batches Seen\": {\"sum\": 684.0, \"count\": 1, \"min\": 684, \"max\": 684}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 38.0, \"count\": 1, \"min\": 38, \"max\": 38}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:46 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=2877.8830609219917 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:46 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:46 INFO 139667671275328] # Starting training for epoch 20\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:03:46.055] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 59, \"duration\": 1560, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:46 INFO 140501882455872] # Finished training epoch 20 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:46 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:46 INFO 140501882455872] Loss (name: value) total: 6.760328372319539\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:46 INFO 140501882455872] Loss (name: value) kld: 0.06949828927301699\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:46 INFO 140501882455872] Loss (name: value) recons: 6.690830085012648\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:46 INFO 140501882455872] Loss (name: value) logppx: 6.760328372319539\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:46 INFO 140501882455872] #quality_metric: host=algo-2, epoch=20, train total_loss <loss>=6.760328372319539\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:46 INFO 140501882455872] patience losses:[6.782608972655402, 6.778489073117574, 6.772778974639045, 6.766437186135186, 6.762062430381775] min patience loss:6.762062430381775 current loss:6.760328372319539 absolute loss difference:0.0017340580622358104\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:46 INFO 140501882455872] Timing: train: 1.56s, val: 0.00s, epoch: 1.57s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:46 INFO 140501882455872] #progress_metric: host=algo-2, completed 20.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309024.4943676, \"EndTime\": 1646309026.0620258, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 19, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 90540.0, \"count\": 1, \"min\": 90540, \"max\": 90540}, \"Total Batches Seen\": {\"sum\": 720.0, \"count\": 1, \"min\": 720, \"max\": 720}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 40.0, \"count\": 1, \"min\": 40, \"max\": 40}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:46 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=2887.4043334755165 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:46 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:46 INFO 140501882455872] # Starting training for epoch 21\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:03:47.612] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 62, \"duration\": 1549, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:47 INFO 140501882455872] # Finished training epoch 21 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:47 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:47 INFO 140501882455872] Loss (name: value) total: 6.753943542639415\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:47 INFO 140501882455872] Loss (name: value) kld: 0.07038749195635319\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:47 INFO 140501882455872] Loss (name: value) recons: 6.683556046750811\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:47 INFO 140501882455872] Loss (name: value) logppx: 6.753943542639415\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:47 INFO 140501882455872] #quality_metric: host=algo-2, epoch=21, train total_loss <loss>=6.753943542639415\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:47 INFO 140501882455872] patience losses:[6.778489073117574, 6.772778974639045, 6.766437186135186, 6.762062430381775, 6.760328372319539] min patience loss:6.760328372319539 current loss:6.753943542639415 absolute loss difference:0.0063848296801243265\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:47 INFO 140501882455872] Timing: train: 1.55s, val: 0.00s, epoch: 1.55s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:47 INFO 140501882455872] #progress_metric: host=algo-2, completed 21.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309026.0623887, \"EndTime\": 1646309027.6174438, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 20, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 95067.0, \"count\": 1, \"min\": 95067, \"max\": 95067}, \"Total Batches Seen\": {\"sum\": 756.0, \"count\": 1, \"min\": 756, \"max\": 756}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 42.0, \"count\": 1, \"min\": 42, \"max\": 42}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:47 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=2910.7388806611134 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:47 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:47 INFO 140501882455872] # Starting training for epoch 22\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:03:47.985] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 59, \"duration\": 1592, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:47 INFO 139667671275328] # Finished training epoch 20 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:47 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:47 INFO 139667671275328] Loss (name: value) total: 6.718680845366584\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:47 INFO 139667671275328] Loss (name: value) kld: 0.07412022942056258\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:47 INFO 139667671275328] Loss (name: value) recons: 6.644560548994276\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:47 INFO 139667671275328] Loss (name: value) logppx: 6.718680845366584\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:47 INFO 139667671275328] #quality_metric: host=algo-1, epoch=20, train total_loss <loss>=6.718680845366584\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:47 INFO 139667671275328] patience losses:[6.747453914748298, 6.737155609660679, 6.7365393373701306, 6.727557765112983, 6.727606587939793] min patience loss:6.727557765112983 current loss:6.718680845366584 absolute loss difference:0.008876919746398926\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:47 INFO 139667671275328] Timing: train: 1.59s, val: 0.01s, epoch: 1.60s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:47 INFO 139667671275328] #progress_metric: host=algo-1, completed 20.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309026.3928404, \"EndTime\": 1646309027.9938228, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 19, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 90480.0, \"count\": 1, \"min\": 90480, \"max\": 90480}, \"Total Batches Seen\": {\"sum\": 720.0, \"count\": 1, \"min\": 720, \"max\": 720}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 40.0, \"count\": 1, \"min\": 40, \"max\": 40}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:47 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=2825.3521739467074 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:47 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:47 INFO 139667671275328] # Starting training for epoch 21\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:03:49.109] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 65, \"duration\": 1490, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:49 INFO 140501882455872] # Finished training epoch 22 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:49 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:49 INFO 140501882455872] Loss (name: value) total: 6.741333603858948\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:49 INFO 140501882455872] Loss (name: value) kld: 0.07516732263482279\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:49 INFO 140501882455872] Loss (name: value) recons: 6.666166272428301\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:49 INFO 140501882455872] Loss (name: value) logppx: 6.741333603858948\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:49 INFO 140501882455872] #quality_metric: host=algo-2, epoch=22, train total_loss <loss>=6.741333603858948\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:49 INFO 140501882455872] patience losses:[6.772778974639045, 6.766437186135186, 6.762062430381775, 6.760328372319539, 6.753943542639415] min patience loss:6.753943542639415 current loss:6.741333603858948 absolute loss difference:0.012609938780467012\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:49 INFO 140501882455872] Timing: train: 1.49s, val: 0.00s, epoch: 1.50s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:49 INFO 140501882455872] #progress_metric: host=algo-2, completed 22.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309027.617856, \"EndTime\": 1646309029.1155758, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 21, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 99594.0, \"count\": 1, \"min\": 99594, \"max\": 99594}, \"Total Batches Seen\": {\"sum\": 792.0, \"count\": 1, \"min\": 792, \"max\": 792}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 44.0, \"count\": 1, \"min\": 44, \"max\": 44}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:49 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=3022.212344495277 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:49 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:49 INFO 140501882455872] # Starting training for epoch 23\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:03:50.548] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 68, \"duration\": 1431, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:50 INFO 140501882455872] # Finished training epoch 23 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:50 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:50 INFO 140501882455872] Loss (name: value) total: 6.73884563975864\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:50 INFO 140501882455872] Loss (name: value) kld: 0.07823020147366656\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:50 INFO 140501882455872] Loss (name: value) recons: 6.660615457428826\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:50 INFO 140501882455872] Loss (name: value) logppx: 6.73884563975864\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:50 INFO 140501882455872] #quality_metric: host=algo-2, epoch=23, train total_loss <loss>=6.73884563975864\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:50 INFO 140501882455872] patience losses:[6.766437186135186, 6.762062430381775, 6.760328372319539, 6.753943542639415, 6.741333603858948] min patience loss:6.741333603858948 current loss:6.73884563975864 absolute loss difference:0.0024879641003074937\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:50 INFO 140501882455872] Timing: train: 1.43s, val: 0.00s, epoch: 1.44s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:50 INFO 140501882455872] #progress_metric: host=algo-2, completed 23.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309029.115985, \"EndTime\": 1646309030.5537918, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 22, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 104121.0, \"count\": 1, \"min\": 104121, \"max\": 104121}, \"Total Batches Seen\": {\"sum\": 828.0, \"count\": 1, \"min\": 828, \"max\": 828}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 46.0, \"count\": 1, \"min\": 46, \"max\": 46}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:50 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=3148.044180285727 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:50 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:50 INFO 140501882455872] # Starting training for epoch 24\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:03:49.575] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 62, \"duration\": 1578, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:49 INFO 139667671275328] # Finished training epoch 21 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:49 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:49 INFO 139667671275328] Loss (name: value) total: 6.714177833663093\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:49 INFO 139667671275328] Loss (name: value) kld: 0.07678448688238859\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:49 INFO 139667671275328] Loss (name: value) recons: 6.6373933222558765\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:49 INFO 139667671275328] Loss (name: value) logppx: 6.714177833663093\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:49 INFO 139667671275328] #quality_metric: host=algo-1, epoch=21, train total_loss <loss>=6.714177833663093\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:49 INFO 139667671275328] patience losses:[6.737155609660679, 6.7365393373701306, 6.727557765112983, 6.727606587939793, 6.718680845366584] min patience loss:6.718680845366584 current loss:6.714177833663093 absolute loss difference:0.004503011703491211\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:49 INFO 139667671275328] Timing: train: 1.58s, val: 0.00s, epoch: 1.59s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:49 INFO 139667671275328] #progress_metric: host=algo-1, completed 21.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309027.9942644, \"EndTime\": 1646309029.5817845, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 20, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 95004.0, \"count\": 1, \"min\": 95004, \"max\": 95004}, \"Total Batches Seen\": {\"sum\": 756.0, \"count\": 1, \"min\": 756, \"max\": 756}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 42.0, \"count\": 1, \"min\": 42, \"max\": 42}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:49 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=2849.339959708372 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:49 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:49 INFO 139667671275328] # Starting training for epoch 22\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:03:51.098] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 65, \"duration\": 1512, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:51 INFO 139667671275328] # Finished training epoch 22 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:51 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:51 INFO 139667671275328] Loss (name: value) total: 6.70602340830697\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:51 INFO 139667671275328] Loss (name: value) kld: 0.08083148735264938\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:51 INFO 139667671275328] Loss (name: value) recons: 6.625191999806298\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:51 INFO 139667671275328] Loss (name: value) logppx: 6.70602340830697\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:51 INFO 139667671275328] #quality_metric: host=algo-1, epoch=22, train total_loss <loss>=6.70602340830697\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:51 INFO 139667671275328] patience losses:[6.7365393373701306, 6.727557765112983, 6.727606587939793, 6.718680845366584, 6.714177833663093] min patience loss:6.714177833663093 current loss:6.70602340830697 absolute loss difference:0.008154425356122985\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:51 INFO 139667671275328] Timing: train: 1.52s, val: 0.01s, epoch: 1.53s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:51 INFO 139667671275328] #progress_metric: host=algo-1, completed 22.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309029.5827208, \"EndTime\": 1646309031.1090047, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 21, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 99528.0, \"count\": 1, \"min\": 99528, \"max\": 99528}, \"Total Batches Seen\": {\"sum\": 792.0, \"count\": 1, \"min\": 792, \"max\": 792}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 44.0, \"count\": 1, \"min\": 44, \"max\": 44}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:51 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=2962.6132557168976 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:51 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:51 INFO 139667671275328] # Starting training for epoch 23\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:03:52.091] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 71, \"duration\": 1536, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:52 INFO 140501882455872] # Finished training epoch 24 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:52 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:52 INFO 140501882455872] Loss (name: value) total: 6.737933390670353\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:52 INFO 140501882455872] Loss (name: value) kld: 0.08188984470648898\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:52 INFO 140501882455872] Loss (name: value) recons: 6.656043582492405\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:52 INFO 140501882455872] Loss (name: value) logppx: 6.737933390670353\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:52 INFO 140501882455872] #quality_metric: host=algo-2, epoch=24, train total_loss <loss>=6.737933390670353\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:52 INFO 140501882455872] patience losses:[6.762062430381775, 6.760328372319539, 6.753943542639415, 6.741333603858948, 6.73884563975864] min patience loss:6.73884563975864 current loss:6.737933390670353 absolute loss difference:0.0009122490882873535\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:52 INFO 140501882455872] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:52 INFO 140501882455872] Timing: train: 1.54s, val: 0.01s, epoch: 1.54s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:52 INFO 140501882455872] #progress_metric: host=algo-2, completed 24.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309030.5541668, \"EndTime\": 1646309032.0996964, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 23, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 108648.0, \"count\": 1, \"min\": 108648, \"max\": 108648}, \"Total Batches Seen\": {\"sum\": 864.0, \"count\": 1, \"min\": 864, \"max\": 864}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 48.0, \"count\": 1, \"min\": 48, \"max\": 48}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:52 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=2928.706753807797 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:52 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:52 INFO 140501882455872] # Starting training for epoch 25\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:03:52.709] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 68, \"duration\": 1598, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:52 INFO 139667671275328] # Finished training epoch 23 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:52 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:52 INFO 139667671275328] Loss (name: value) total: 6.700065321392483\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:52 INFO 139667671275328] Loss (name: value) kld: 0.0841486890696817\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:52 INFO 139667671275328] Loss (name: value) recons: 6.615916609764099\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:52 INFO 139667671275328] Loss (name: value) logppx: 6.700065321392483\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:52 INFO 139667671275328] #quality_metric: host=algo-1, epoch=23, train total_loss <loss>=6.700065321392483\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:52 INFO 139667671275328] patience losses:[6.727557765112983, 6.727606587939793, 6.718680845366584, 6.714177833663093, 6.70602340830697] min patience loss:6.70602340830697 current loss:6.700065321392483 absolute loss difference:0.005958086914486849\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:52 INFO 139667671275328] Timing: train: 1.60s, val: 0.01s, epoch: 1.61s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:52 INFO 139667671275328] #progress_metric: host=algo-1, completed 23.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309031.1096683, \"EndTime\": 1646309032.717803, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 22, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 104052.0, \"count\": 1, \"min\": 104052, \"max\": 104052}, \"Total Batches Seen\": {\"sum\": 828.0, \"count\": 1, \"min\": 828, \"max\": 828}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 46.0, \"count\": 1, \"min\": 46, \"max\": 46}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:52 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=2811.9730869056075 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:52 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:52 INFO 139667671275328] # Starting training for epoch 24\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:03:53.534] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 74, \"duration\": 1433, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:53 INFO 140501882455872] # Finished training epoch 25 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:53 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:53 INFO 140501882455872] Loss (name: value) total: 6.731082757314046\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:53 INFO 140501882455872] Loss (name: value) kld: 0.0840614021031393\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:53 INFO 140501882455872] Loss (name: value) recons: 6.647021359867519\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:53 INFO 140501882455872] Loss (name: value) logppx: 6.731082757314046\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:53 INFO 140501882455872] #quality_metric: host=algo-2, epoch=25, train total_loss <loss>=6.731082757314046\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:53 INFO 140501882455872] patience losses:[6.760328372319539, 6.753943542639415, 6.741333603858948, 6.73884563975864, 6.737933390670353] min patience loss:6.737933390670353 current loss:6.731082757314046 absolute loss difference:0.006850633356306979\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:53 INFO 140501882455872] Timing: train: 1.44s, val: 0.00s, epoch: 1.44s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:53 INFO 140501882455872] #progress_metric: host=algo-2, completed 25.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309032.1004508, \"EndTime\": 1646309033.5428982, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 24, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 113175.0, \"count\": 1, \"min\": 113175, \"max\": 113175}, \"Total Batches Seen\": {\"sum\": 900.0, \"count\": 1, \"min\": 900, \"max\": 900}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 50.0, \"count\": 1, \"min\": 50, \"max\": 50}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:53 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=3138.038350357829 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:53 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:53 INFO 140501882455872] # Starting training for epoch 26\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:03:54.273] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 71, \"duration\": 1554, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:54 INFO 139667671275328] # Finished training epoch 24 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:54 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:54 INFO 139667671275328] Loss (name: value) total: 6.690782606601715\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:54 INFO 139667671275328] Loss (name: value) kld: 0.08660156466066837\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:54 INFO 139667671275328] Loss (name: value) recons: 6.60418105787701\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:54 INFO 139667671275328] Loss (name: value) logppx: 6.690782606601715\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:54 INFO 139667671275328] #quality_metric: host=algo-1, epoch=24, train total_loss <loss>=6.690782606601715\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:54 INFO 139667671275328] patience losses:[6.727606587939793, 6.718680845366584, 6.714177833663093, 6.70602340830697, 6.700065321392483] min patience loss:6.700065321392483 current loss:6.690782606601715 absolute loss difference:0.009282714790767699\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:54 INFO 139667671275328] Timing: train: 1.56s, val: 0.00s, epoch: 1.56s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:54 INFO 139667671275328] #progress_metric: host=algo-1, completed 24.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309032.7188663, \"EndTime\": 1646309034.2794695, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 23, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 108576.0, \"count\": 1, \"min\": 108576, \"max\": 108576}, \"Total Batches Seen\": {\"sum\": 864.0, \"count\": 1, \"min\": 864, \"max\": 864}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 48.0, \"count\": 1, \"min\": 48, \"max\": 48}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:54 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=2897.941067201119 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:54 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:54 INFO 139667671275328] # Starting training for epoch 25\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:03:55.020] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 77, \"duration\": 1476, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:55 INFO 140501882455872] # Finished training epoch 26 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:55 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:55 INFO 140501882455872] Loss (name: value) total: 6.7241807447539435\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:55 INFO 140501882455872] Loss (name: value) kld: 0.08593239624881083\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:55 INFO 140501882455872] Loss (name: value) recons: 6.638248311148749\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:55 INFO 140501882455872] Loss (name: value) logppx: 6.7241807447539435\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:55 INFO 140501882455872] #quality_metric: host=algo-2, epoch=26, train total_loss <loss>=6.7241807447539435\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:55 INFO 140501882455872] patience losses:[6.753943542639415, 6.741333603858948, 6.73884563975864, 6.737933390670353, 6.731082757314046] min patience loss:6.731082757314046 current loss:6.7241807447539435 absolute loss difference:0.006902012560102477\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:55 INFO 140501882455872] Timing: train: 1.48s, val: 0.00s, epoch: 1.48s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:55 INFO 140501882455872] #progress_metric: host=algo-2, completed 26.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309033.5432408, \"EndTime\": 1646309035.025683, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 25, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 117702.0, \"count\": 1, \"min\": 117702, \"max\": 117702}, \"Total Batches Seen\": {\"sum\": 936.0, \"count\": 1, \"min\": 936, \"max\": 936}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 52.0, \"count\": 1, \"min\": 52, \"max\": 52}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:55 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=3053.3794589323534 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:55 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:55 INFO 140501882455872] # Starting training for epoch 27\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:03:55.836] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 74, \"duration\": 1554, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:55 INFO 139667671275328] # Finished training epoch 25 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:55 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:55 INFO 139667671275328] Loss (name: value) total: 6.687228759129842\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:55 INFO 139667671275328] Loss (name: value) kld: 0.08945511529843013\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:55 INFO 139667671275328] Loss (name: value) recons: 6.597773598300086\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:55 INFO 139667671275328] Loss (name: value) logppx: 6.687228759129842\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:55 INFO 139667671275328] #quality_metric: host=algo-1, epoch=25, train total_loss <loss>=6.687228759129842\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:55 INFO 139667671275328] patience losses:[6.718680845366584, 6.714177833663093, 6.70602340830697, 6.700065321392483, 6.690782606601715] min patience loss:6.690782606601715 current loss:6.687228759129842 absolute loss difference:0.0035538474718732616\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:55 INFO 139667671275328] Timing: train: 1.56s, val: 0.00s, epoch: 1.56s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:55 INFO 139667671275328] #progress_metric: host=algo-1, completed 25.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309034.2803712, \"EndTime\": 1646309035.8423586, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 24, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 113100.0, \"count\": 1, \"min\": 113100, \"max\": 113100}, \"Total Batches Seen\": {\"sum\": 900.0, \"count\": 1, \"min\": 900, \"max\": 900}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 50.0, \"count\": 1, \"min\": 50, \"max\": 50}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:55 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=2895.254414397424 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:55 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:55 INFO 139667671275328] # Starting training for epoch 26\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:03:57.407] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 77, \"duration\": 1563, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:57 INFO 139667671275328] # Finished training epoch 26 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:57 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:57 INFO 139667671275328] Loss (name: value) total: 6.6872404019037885\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:57 INFO 139667671275328] Loss (name: value) kld: 0.09177494400905238\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:57 INFO 139667671275328] Loss (name: value) recons: 6.595465454790327\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:57 INFO 139667671275328] Loss (name: value) logppx: 6.6872404019037885\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:57 INFO 139667671275328] #quality_metric: host=algo-1, epoch=26, train total_loss <loss>=6.6872404019037885\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:57 INFO 139667671275328] patience losses:[6.714177833663093, 6.70602340830697, 6.700065321392483, 6.690782606601715, 6.687228759129842] min patience loss:6.687228759129842 current loss:6.6872404019037885 absolute loss difference:1.1642773946718421e-05\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:57 INFO 139667671275328] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:57 INFO 139667671275328] Timing: train: 1.57s, val: 0.00s, epoch: 1.57s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:57 INFO 139667671275328] #progress_metric: host=algo-1, completed 26.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309035.8433936, \"EndTime\": 1646309037.411055, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 25, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 117624.0, \"count\": 1, \"min\": 117624, \"max\": 117624}, \"Total Batches Seen\": {\"sum\": 936.0, \"count\": 1, \"min\": 936, \"max\": 936}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 52.0, \"count\": 1, \"min\": 52, \"max\": 52}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:57 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=2884.8876676006744 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:57 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:57 INFO 139667671275328] # Starting training for epoch 27\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:03:59.067] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 80, \"duration\": 1652, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:59 INFO 139667671275328] # Finished training epoch 27 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:59 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:59 INFO 139667671275328] Loss (name: value) total: 6.684045473734538\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:59 INFO 139667671275328] Loss (name: value) kld: 0.09625571686774492\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:59 INFO 139667671275328] Loss (name: value) recons: 6.587789754072825\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:59 INFO 139667671275328] Loss (name: value) logppx: 6.684045473734538\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:59 INFO 139667671275328] #quality_metric: host=algo-1, epoch=27, train total_loss <loss>=6.684045473734538\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:59 INFO 139667671275328] patience losses:[6.70602340830697, 6.700065321392483, 6.690782606601715, 6.687228759129842, 6.6872404019037885] min patience loss:6.687228759129842 current loss:6.684045473734538 absolute loss difference:0.00318328539530377\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:59 INFO 139667671275328] Timing: train: 1.66s, val: 0.01s, epoch: 1.66s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:59 INFO 139667671275328] #progress_metric: host=algo-1, completed 27.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309037.411967, \"EndTime\": 1646309039.0736694, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 26, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 122148.0, \"count\": 1, \"min\": 122148, \"max\": 122148}, \"Total Batches Seen\": {\"sum\": 972.0, \"count\": 1, \"min\": 972, \"max\": 972}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 54.0, \"count\": 1, \"min\": 54, \"max\": 54}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:59 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=2722.2123258988486 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:59 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:03:59 INFO 139667671275328] # Starting training for epoch 28\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:03:56.472] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 80, \"duration\": 1446, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:56 INFO 140501882455872] # Finished training epoch 27 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:56 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:56 INFO 140501882455872] Loss (name: value) total: 6.721780704127418\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:56 INFO 140501882455872] Loss (name: value) kld: 0.08858633734699753\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:56 INFO 140501882455872] Loss (name: value) recons: 6.633194393581814\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:56 INFO 140501882455872] Loss (name: value) logppx: 6.721780704127418\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:56 INFO 140501882455872] #quality_metric: host=algo-2, epoch=27, train total_loss <loss>=6.721780704127418\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:56 INFO 140501882455872] patience losses:[6.741333603858948, 6.73884563975864, 6.737933390670353, 6.731082757314046, 6.7241807447539435] min patience loss:6.7241807447539435 current loss:6.721780704127418 absolute loss difference:0.002400040626525879\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:56 INFO 140501882455872] Timing: train: 1.45s, val: 0.01s, epoch: 1.45s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:56 INFO 140501882455872] #progress_metric: host=algo-2, completed 27.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309035.0260537, \"EndTime\": 1646309036.4804246, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 26, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 122229.0, \"count\": 1, \"min\": 122229, \"max\": 122229}, \"Total Batches Seen\": {\"sum\": 972.0, \"count\": 1, \"min\": 972, \"max\": 972}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 54.0, \"count\": 1, \"min\": 54, \"max\": 54}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:56 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=3112.2986333177782 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:56 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:56 INFO 140501882455872] # Starting training for epoch 28\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:03:57.968] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 83, \"duration\": 1484, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:57 INFO 140501882455872] # Finished training epoch 28 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:57 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:57 INFO 140501882455872] Loss (name: value) total: 6.713635173108843\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:57 INFO 140501882455872] Loss (name: value) kld: 0.09177744968069924\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:57 INFO 140501882455872] Loss (name: value) recons: 6.6218576894866095\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:57 INFO 140501882455872] Loss (name: value) logppx: 6.713635173108843\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:57 INFO 140501882455872] #quality_metric: host=algo-2, epoch=28, train total_loss <loss>=6.713635173108843\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:57 INFO 140501882455872] patience losses:[6.73884563975864, 6.737933390670353, 6.731082757314046, 6.7241807447539435, 6.721780704127418] min patience loss:6.721780704127418 current loss:6.713635173108843 absolute loss difference:0.008145531018574736\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:57 INFO 140501882455872] Timing: train: 1.49s, val: 0.01s, epoch: 1.49s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:57 INFO 140501882455872] #progress_metric: host=algo-2, completed 28.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309036.480946, \"EndTime\": 1646309037.9755316, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 27, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 126756.0, \"count\": 1, \"min\": 126756, \"max\": 126756}, \"Total Batches Seen\": {\"sum\": 1008.0, \"count\": 1, \"min\": 1008, \"max\": 1008}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 56.0, \"count\": 1, \"min\": 56, \"max\": 56}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:57 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=3028.508755786829 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:57 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:57 INFO 140501882455872] # Starting training for epoch 29\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:03:59.559] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 86, \"duration\": 1582, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:59 INFO 140501882455872] # Finished training epoch 29 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:59 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:59 INFO 140501882455872] Loss (name: value) total: 6.708157956600189\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:59 INFO 140501882455872] Loss (name: value) kld: 0.09357451751000351\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:59 INFO 140501882455872] Loss (name: value) recons: 6.614583479033576\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:59 INFO 140501882455872] Loss (name: value) logppx: 6.708157956600189\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:59 INFO 140501882455872] #quality_metric: host=algo-2, epoch=29, train total_loss <loss>=6.708157956600189\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:59 INFO 140501882455872] patience losses:[6.737933390670353, 6.731082757314046, 6.7241807447539435, 6.721780704127418, 6.713635173108843] min patience loss:6.713635173108843 current loss:6.708157956600189 absolute loss difference:0.005477216508653626\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:59 INFO 140501882455872] Timing: train: 1.58s, val: 0.01s, epoch: 1.59s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:59 INFO 140501882455872] #progress_metric: host=algo-2, completed 29.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309037.9758964, \"EndTime\": 1646309039.5665941, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 28, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 131283.0, \"count\": 1, \"min\": 131283, \"max\": 131283}, \"Total Batches Seen\": {\"sum\": 1044.0, \"count\": 1, \"min\": 1044, \"max\": 1044}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 58.0, \"count\": 1, \"min\": 58, \"max\": 58}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:59 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=2845.569878308503 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:59 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:03:59 INFO 140501882455872] # Starting training for epoch 30\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:04:00.655] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 83, \"duration\": 1580, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:00 INFO 139667671275328] # Finished training epoch 28 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:00 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:00 INFO 139667671275328] Loss (name: value) total: 6.670919597148895\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:00 INFO 139667671275328] Loss (name: value) kld: 0.0970518256848057\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:00 INFO 139667671275328] Loss (name: value) recons: 6.573867725001441\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:00 INFO 139667671275328] Loss (name: value) logppx: 6.670919597148895\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:00 INFO 139667671275328] #quality_metric: host=algo-1, epoch=28, train total_loss <loss>=6.670919597148895\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:00 INFO 139667671275328] patience losses:[6.700065321392483, 6.690782606601715, 6.687228759129842, 6.6872404019037885, 6.684045473734538] min patience loss:6.684045473734538 current loss:6.670919597148895 absolute loss difference:0.013125876585642793\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:00 INFO 139667671275328] Timing: train: 1.58s, val: 0.00s, epoch: 1.59s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:00 INFO 139667671275328] #progress_metric: host=algo-1, completed 28.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309039.0739896, \"EndTime\": 1646309040.6616263, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 27, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 126672.0, \"count\": 1, \"min\": 126672, \"max\": 126672}, \"Total Batches Seen\": {\"sum\": 1008.0, \"count\": 1, \"min\": 1008, \"max\": 1008}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 56.0, \"count\": 1, \"min\": 56, \"max\": 56}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:00 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=2849.1179154491306 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:00 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:00 INFO 139667671275328] # Starting training for epoch 29\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:04:01.065] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 89, \"duration\": 1497, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:01 INFO 140501882455872] # Finished training epoch 30 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:01 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:01 INFO 140501882455872] Loss (name: value) total: 6.701709979110294\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:01 INFO 140501882455872] Loss (name: value) kld: 0.09812712193363243\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:01 INFO 140501882455872] Loss (name: value) recons: 6.603582852416569\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:01 INFO 140501882455872] Loss (name: value) logppx: 6.701709979110294\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:01 INFO 140501882455872] #quality_metric: host=algo-2, epoch=30, train total_loss <loss>=6.701709979110294\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:01 INFO 140501882455872] patience losses:[6.731082757314046, 6.7241807447539435, 6.721780704127418, 6.713635173108843, 6.708157956600189] min patience loss:6.708157956600189 current loss:6.701709979110294 absolute loss difference:0.006447977489894896\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:01 INFO 140501882455872] Timing: train: 1.50s, val: 0.01s, epoch: 1.51s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:01 INFO 140501882455872] #progress_metric: host=algo-2, completed 30.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309039.5672417, \"EndTime\": 1646309041.073285, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 29, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 135810.0, \"count\": 1, \"min\": 135810, \"max\": 135810}, \"Total Batches Seen\": {\"sum\": 1080.0, \"count\": 1, \"min\": 1080, \"max\": 1080}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 60.0, \"count\": 1, \"min\": 60, \"max\": 60}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:01 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=3005.114433911066 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:01 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:01 INFO 140501882455872] # Starting training for epoch 31\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:04:02.477] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 86, \"duration\": 1815, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:02 INFO 139667671275328] # Finished training epoch 29 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:02 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:02 INFO 139667671275328] Loss (name: value) total: 6.673023535145654\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:02 INFO 139667671275328] Loss (name: value) kld: 0.10345107679151827\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:02 INFO 139667671275328] Loss (name: value) recons: 6.569572501712376\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:02 INFO 139667671275328] Loss (name: value) logppx: 6.673023535145654\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:02 INFO 139667671275328] #quality_metric: host=algo-1, epoch=29, train total_loss <loss>=6.673023535145654\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:02 INFO 139667671275328] patience losses:[6.690782606601715, 6.687228759129842, 6.6872404019037885, 6.684045473734538, 6.670919597148895] min patience loss:6.670919597148895 current loss:6.673023535145654 absolute loss difference:0.0021039379967584537\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:02 INFO 139667671275328] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:02 INFO 139667671275328] Timing: train: 1.82s, val: 0.00s, epoch: 1.82s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:02 INFO 139667671275328] #progress_metric: host=algo-1, completed 29.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309040.6622741, \"EndTime\": 1646309042.479476, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 28, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 131196.0, \"count\": 1, \"min\": 131196, \"max\": 131196}, \"Total Batches Seen\": {\"sum\": 1044.0, \"count\": 1, \"min\": 1044, \"max\": 1044}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 58.0, \"count\": 1, \"min\": 58, \"max\": 58}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:02 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=2488.9585907493793 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:02 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:02 INFO 139667671275328] # Starting training for epoch 30\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:04:02.840] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 92, \"duration\": 1764, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:04:04.395] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 89, \"duration\": 1914, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:04 INFO 139667671275328] # Finished training epoch 30 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:04 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:04 INFO 139667671275328] Loss (name: value) total: 6.6631129251586065\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:04 INFO 139667671275328] Loss (name: value) kld: 0.10617857902414268\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:04 INFO 139667671275328] Loss (name: value) recons: 6.556934316953023\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:04 INFO 139667671275328] Loss (name: value) logppx: 6.6631129251586065\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:04 INFO 139667671275328] #quality_metric: host=algo-1, epoch=30, train total_loss <loss>=6.6631129251586065\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:04 INFO 139667671275328] patience losses:[6.687228759129842, 6.6872404019037885, 6.684045473734538, 6.670919597148895, 6.673023535145654] min patience loss:6.670919597148895 current loss:6.6631129251586065 absolute loss difference:0.007806671990288727\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:04 INFO 139667671275328] Timing: train: 1.92s, val: 0.01s, epoch: 1.92s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:04 INFO 139667671275328] #progress_metric: host=algo-1, completed 30.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309042.4798553, \"EndTime\": 1646309044.4036589, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 29, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 135720.0, \"count\": 1, \"min\": 135720, \"max\": 135720}, \"Total Batches Seen\": {\"sum\": 1080.0, \"count\": 1, \"min\": 1080, \"max\": 1080}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 60.0, \"count\": 1, \"min\": 60, \"max\": 60}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:04 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=2351.268282056563 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:04 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:04 INFO 139667671275328] # Starting training for epoch 31\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:04:06.572] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 92, \"duration\": 2167, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:06 INFO 139667671275328] # Finished training epoch 31 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:06 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:06 INFO 139667671275328] Loss (name: value) total: 6.662654863463508\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:06 INFO 139667671275328] Loss (name: value) kld: 0.109400302482148\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:06 INFO 139667671275328] Loss (name: value) recons: 6.553254584471385\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:06 INFO 139667671275328] Loss (name: value) logppx: 6.662654863463508\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:06 INFO 139667671275328] #quality_metric: host=algo-1, epoch=31, train total_loss <loss>=6.662654863463508\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:06 INFO 139667671275328] patience losses:[6.6872404019037885, 6.684045473734538, 6.670919597148895, 6.673023535145654, 6.6631129251586065] min patience loss:6.6631129251586065 current loss:6.662654863463508 absolute loss difference:0.00045806169509887695\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:06 INFO 139667671275328] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:06 INFO 139667671275328] Timing: train: 2.17s, val: 0.01s, epoch: 2.17s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:06 INFO 139667671275328] #progress_metric: host=algo-1, completed 31.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309044.4044163, \"EndTime\": 1646309046.5794132, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 30, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 140244.0, \"count\": 1, \"min\": 140244, \"max\": 140244}, \"Total Batches Seen\": {\"sum\": 1116.0, \"count\": 1, \"min\": 1116, \"max\": 1116}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 62.0, \"count\": 1, \"min\": 62, \"max\": 62}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:06 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=2079.7622636719893 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:06 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:06 INFO 139667671275328] # Starting training for epoch 32\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:02 INFO 140501882455872] # Finished training epoch 31 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:02 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:02 INFO 140501882455872] Loss (name: value) total: 6.707730220423804\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:02 INFO 140501882455872] Loss (name: value) kld: 0.10036334488540888\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:02 INFO 140501882455872] Loss (name: value) recons: 6.60736686653561\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:02 INFO 140501882455872] Loss (name: value) logppx: 6.707730220423804\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:02 INFO 140501882455872] #quality_metric: host=algo-2, epoch=31, train total_loss <loss>=6.707730220423804\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:02 INFO 140501882455872] patience losses:[6.7241807447539435, 6.721780704127418, 6.713635173108843, 6.708157956600189, 6.701709979110294] min patience loss:6.701709979110294 current loss:6.707730220423804 absolute loss difference:0.0060202413135099775\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:02 INFO 140501882455872] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:02 INFO 140501882455872] Timing: train: 1.77s, val: 0.00s, epoch: 1.77s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:02 INFO 140501882455872] #progress_metric: host=algo-2, completed 31.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309041.0738795, \"EndTime\": 1646309042.8424644, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 30, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 140337.0, \"count\": 1, \"min\": 140337, \"max\": 140337}, \"Total Batches Seen\": {\"sum\": 1116.0, \"count\": 1, \"min\": 1116, \"max\": 1116}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 62.0, \"count\": 1, \"min\": 62, \"max\": 62}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:02 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=2559.4070983608435 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:02 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:02 INFO 140501882455872] # Starting training for epoch 32\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:04:04.310] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 95, \"duration\": 1466, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:04 INFO 140501882455872] # Finished training epoch 32 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:04 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:04 INFO 140501882455872] Loss (name: value) total: 6.695896751350826\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:04 INFO 140501882455872] Loss (name: value) kld: 0.10332844013141261\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:04 INFO 140501882455872] Loss (name: value) recons: 6.5925683048036365\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:04 INFO 140501882455872] Loss (name: value) logppx: 6.695896751350826\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:04 INFO 140501882455872] #quality_metric: host=algo-2, epoch=32, train total_loss <loss>=6.695896751350826\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:04 INFO 140501882455872] patience losses:[6.721780704127418, 6.713635173108843, 6.708157956600189, 6.701709979110294, 6.707730220423804] min patience loss:6.701709979110294 current loss:6.695896751350826 absolute loss difference:0.00581322775946802\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:04 INFO 140501882455872] Timing: train: 1.47s, val: 0.00s, epoch: 1.47s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:04 INFO 140501882455872] #progress_metric: host=algo-2, completed 32.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309042.842866, \"EndTime\": 1646309044.3152902, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 31, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 144864.0, \"count\": 1, \"min\": 144864, \"max\": 144864}, \"Total Batches Seen\": {\"sum\": 1152.0, \"count\": 1, \"min\": 1152, \"max\": 1152}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 64.0, \"count\": 1, \"min\": 64, \"max\": 64}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:04 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=3074.176491235796 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:04 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:04 INFO 140501882455872] # Starting training for epoch 33\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:04:06.249] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 98, \"duration\": 1933, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:06 INFO 140501882455872] # Finished training epoch 33 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:06 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:06 INFO 140501882455872] Loss (name: value) total: 6.690500848823124\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:06 INFO 140501882455872] Loss (name: value) kld: 0.1065123671044906\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:06 INFO 140501882455872] Loss (name: value) recons: 6.583988441361321\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:06 INFO 140501882455872] Loss (name: value) logppx: 6.690500848823124\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:06 INFO 140501882455872] #quality_metric: host=algo-2, epoch=33, train total_loss <loss>=6.690500848823124\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:06 INFO 140501882455872] patience losses:[6.713635173108843, 6.708157956600189, 6.701709979110294, 6.707730220423804, 6.695896751350826] min patience loss:6.695896751350826 current loss:6.690500848823124 absolute loss difference:0.00539590252770239\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:06 INFO 140501882455872] Timing: train: 1.94s, val: 0.01s, epoch: 1.94s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:06 INFO 140501882455872] #progress_metric: host=algo-2, completed 33.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309044.315592, \"EndTime\": 1646309046.2590022, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 32, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 149391.0, \"count\": 1, \"min\": 149391, \"max\": 149391}, \"Total Batches Seen\": {\"sum\": 1188.0, \"count\": 1, \"min\": 1188, \"max\": 1188}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 66.0, \"count\": 1, \"min\": 66, \"max\": 66}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:06 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=2329.1836035783404 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:06 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:06 INFO 140501882455872] # Starting training for epoch 34\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:04:08.169] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 95, \"duration\": 1584, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:08 INFO 139667671275328] # Finished training epoch 32 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:08 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:08 INFO 139667671275328] Loss (name: value) total: 6.647783597310384\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:08 INFO 139667671275328] Loss (name: value) kld: 0.11145265400409698\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:08 INFO 139667671275328] Loss (name: value) recons: 6.53633095158471\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:08 INFO 139667671275328] Loss (name: value) logppx: 6.647783597310384\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:08 INFO 139667671275328] #quality_metric: host=algo-1, epoch=32, train total_loss <loss>=6.647783597310384\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:08 INFO 139667671275328] patience losses:[6.684045473734538, 6.670919597148895, 6.673023535145654, 6.6631129251586065, 6.662654863463508] min patience loss:6.662654863463508 current loss:6.647783597310384 absolute loss difference:0.014871266153123841\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:08 INFO 139667671275328] Timing: train: 1.59s, val: 0.00s, epoch: 1.60s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:08 INFO 139667671275328] #progress_metric: host=algo-1, completed 32.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309046.5798554, \"EndTime\": 1646309048.1764364, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 31, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 144768.0, \"count\": 1, \"min\": 144768, \"max\": 144768}, \"Total Batches Seen\": {\"sum\": 1152.0, \"count\": 1, \"min\": 1152, \"max\": 1152}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 64.0, \"count\": 1, \"min\": 64, \"max\": 64}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:08 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=2833.1644828815183 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:08 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:08 INFO 139667671275328] # Starting training for epoch 33\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:04:07.946] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 101, \"duration\": 1682, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:07 INFO 140501882455872] # Finished training epoch 34 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:07 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:07 INFO 140501882455872] Loss (name: value) total: 6.685550702942742\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:07 INFO 140501882455872] Loss (name: value) kld: 0.1081424183729622\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:07 INFO 140501882455872] Loss (name: value) recons: 6.577408260769314\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:07 INFO 140501882455872] Loss (name: value) logppx: 6.685550702942742\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:07 INFO 140501882455872] #quality_metric: host=algo-2, epoch=34, train total_loss <loss>=6.685550702942742\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:07 INFO 140501882455872] patience losses:[6.708157956600189, 6.701709979110294, 6.707730220423804, 6.695896751350826, 6.690500848823124] min patience loss:6.690500848823124 current loss:6.685550702942742 absolute loss difference:0.004950145880381562\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:07 INFO 140501882455872] Timing: train: 1.69s, val: 0.01s, epoch: 1.69s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:07 INFO 140501882455872] #progress_metric: host=algo-2, completed 34.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309046.2593684, \"EndTime\": 1646309047.9539483, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 33, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 153918.0, \"count\": 1, \"min\": 153918, \"max\": 153918}, \"Total Batches Seen\": {\"sum\": 1224.0, \"count\": 1, \"min\": 1224, \"max\": 1224}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 68.0, \"count\": 1, \"min\": 68, \"max\": 68}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:07 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=2671.15575807683 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:07 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:07 INFO 140501882455872] # Starting training for epoch 35\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:04:09.456] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 104, \"duration\": 1501, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:09 INFO 140501882455872] # Finished training epoch 35 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:09 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:09 INFO 140501882455872] Loss (name: value) total: 6.6848527855343285\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:09 INFO 140501882455872] Loss (name: value) kld: 0.11208252008590433\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:09 INFO 140501882455872] Loss (name: value) recons: 6.572770277659099\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:09 INFO 140501882455872] Loss (name: value) logppx: 6.6848527855343285\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:09 INFO 140501882455872] #quality_metric: host=algo-2, epoch=35, train total_loss <loss>=6.6848527855343285\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:09 INFO 140501882455872] patience losses:[6.701709979110294, 6.707730220423804, 6.695896751350826, 6.690500848823124, 6.685550702942742] min patience loss:6.685550702942742 current loss:6.6848527855343285 absolute loss difference:0.0006979174084138506\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:09 INFO 140501882455872] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:09 INFO 140501882455872] Timing: train: 1.51s, val: 0.01s, epoch: 1.51s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:09 INFO 140501882455872] #progress_metric: host=algo-2, completed 35.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309047.9543161, \"EndTime\": 1646309049.4659183, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 34, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 158445.0, \"count\": 1, \"min\": 158445, \"max\": 158445}, \"Total Batches Seen\": {\"sum\": 1260.0, \"count\": 1, \"min\": 1260, \"max\": 1260}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 70.0, \"count\": 1, \"min\": 70, \"max\": 70}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:09 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=2994.4974452102915 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:09 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:09 INFO 140501882455872] # Starting training for epoch 36\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:04:09.724] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 98, \"duration\": 1546, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:09 INFO 139667671275328] # Finished training epoch 33 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:09 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:09 INFO 139667671275328] Loss (name: value) total: 6.652277463012272\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:09 INFO 139667671275328] Loss (name: value) kld: 0.11507311277091503\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:09 INFO 139667671275328] Loss (name: value) recons: 6.537204345067342\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:09 INFO 139667671275328] Loss (name: value) logppx: 6.652277463012272\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:09 INFO 139667671275328] #quality_metric: host=algo-1, epoch=33, train total_loss <loss>=6.652277463012272\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:09 INFO 139667671275328] patience losses:[6.670919597148895, 6.673023535145654, 6.6631129251586065, 6.662654863463508, 6.647783597310384] min patience loss:6.647783597310384 current loss:6.652277463012272 absolute loss difference:0.0044938657018880335\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:09 INFO 139667671275328] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:09 INFO 139667671275328] Timing: train: 1.55s, val: 0.00s, epoch: 1.55s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:09 INFO 139667671275328] #progress_metric: host=algo-1, completed 33.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309048.177055, \"EndTime\": 1646309049.7265103, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 32, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 149292.0, \"count\": 1, \"min\": 149292, \"max\": 149292}, \"Total Batches Seen\": {\"sum\": 1188.0, \"count\": 1, \"min\": 1188, \"max\": 1188}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 66.0, \"count\": 1, \"min\": 66, \"max\": 66}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:09 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=2919.340231119054 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:09 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:09 INFO 139667671275328] # Starting training for epoch 34\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:04:11.319] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 101, \"duration\": 1591, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:11 INFO 139667671275328] # Finished training epoch 34 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:11 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:11 INFO 139667671275328] Loss (name: value) total: 6.647996955447727\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:11 INFO 139667671275328] Loss (name: value) kld: 0.11630256411929925\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:11 INFO 139667671275328] Loss (name: value) recons: 6.531694412231445\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:11 INFO 139667671275328] Loss (name: value) logppx: 6.647996955447727\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:11 INFO 139667671275328] #quality_metric: host=algo-1, epoch=34, train total_loss <loss>=6.647996955447727\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:11 INFO 139667671275328] patience losses:[6.673023535145654, 6.6631129251586065, 6.662654863463508, 6.647783597310384, 6.652277463012272] min patience loss:6.647783597310384 current loss:6.647996955447727 absolute loss difference:0.00021335813734335574\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:11 INFO 139667671275328] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:11 INFO 139667671275328] Timing: train: 1.59s, val: 0.00s, epoch: 1.59s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:11 INFO 139667671275328] #progress_metric: host=algo-1, completed 34.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309049.7269785, \"EndTime\": 1646309051.3209682, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 33, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 153816.0, \"count\": 1, \"min\": 153816, \"max\": 153816}, \"Total Batches Seen\": {\"sum\": 1224.0, \"count\": 1, \"min\": 1224, \"max\": 1224}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 68.0, \"count\": 1, \"min\": 68, \"max\": 68}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:11 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=2837.7858899685666 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:11 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:11 INFO 139667671275328] # Starting training for epoch 35\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:04:10.975] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 107, \"duration\": 1505, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:10 INFO 140501882455872] # Finished training epoch 36 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:10 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:10 INFO 140501882455872] Loss (name: value) total: 6.677068279849158\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:10 INFO 140501882455872] Loss (name: value) kld: 0.11560121882292959\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:10 INFO 140501882455872] Loss (name: value) recons: 6.561467064751519\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:10 INFO 140501882455872] Loss (name: value) logppx: 6.677068279849158\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:10 INFO 140501882455872] #quality_metric: host=algo-2, epoch=36, train total_loss <loss>=6.677068279849158\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:10 INFO 140501882455872] patience losses:[6.707730220423804, 6.695896751350826, 6.690500848823124, 6.685550702942742, 6.6848527855343285] min patience loss:6.6848527855343285 current loss:6.677068279849158 absolute loss difference:0.0077845056851701955\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:10 INFO 140501882455872] Timing: train: 1.51s, val: 0.01s, epoch: 1.52s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:10 INFO 140501882455872] #progress_metric: host=algo-2, completed 36.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309049.4662366, \"EndTime\": 1646309050.9835005, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 35, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 162972.0, \"count\": 1, \"min\": 162972, \"max\": 162972}, \"Total Batches Seen\": {\"sum\": 1296.0, \"count\": 1, \"min\": 1296, \"max\": 1296}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:10 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=2983.30681297548 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:10 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:10 INFO 140501882455872] # Starting training for epoch 37\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:04:12.522] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 110, \"duration\": 1537, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:12 INFO 140501882455872] # Finished training epoch 37 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:12 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:12 INFO 140501882455872] Loss (name: value) total: 6.677471611234877\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:12 INFO 140501882455872] Loss (name: value) kld: 0.1169307062195407\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:12 INFO 140501882455872] Loss (name: value) recons: 6.560540868176354\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:12 INFO 140501882455872] Loss (name: value) logppx: 6.677471611234877\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:12 INFO 140501882455872] #quality_metric: host=algo-2, epoch=37, train total_loss <loss>=6.677471611234877\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:12 INFO 140501882455872] patience losses:[6.695896751350826, 6.690500848823124, 6.685550702942742, 6.6848527855343285, 6.677068279849158] min patience loss:6.677068279849158 current loss:6.677471611234877 absolute loss difference:0.0004033313857183529\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:12 INFO 140501882455872] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:12 INFO 140501882455872] Timing: train: 1.54s, val: 0.00s, epoch: 1.54s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:12 INFO 140501882455872] #progress_metric: host=algo-2, completed 37.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309050.9840457, \"EndTime\": 1646309052.524562, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 36, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 167499.0, \"count\": 1, \"min\": 167499, \"max\": 167499}, \"Total Batches Seen\": {\"sum\": 1332.0, \"count\": 1, \"min\": 1332, \"max\": 1332}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 74.0, \"count\": 1, \"min\": 74, \"max\": 74}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:12 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=2938.22669247803 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:12 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:12 INFO 140501882455872] # Starting training for epoch 38\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:04:12.956] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 104, \"duration\": 1634, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:12 INFO 139667671275328] # Finished training epoch 35 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:12 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:12 INFO 139667671275328] Loss (name: value) total: 6.640991608301799\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:12 INFO 139667671275328] Loss (name: value) kld: 0.12123446818441153\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:12 INFO 139667671275328] Loss (name: value) recons: 6.519757098621792\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:12 INFO 139667671275328] Loss (name: value) logppx: 6.640991608301799\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:12 INFO 139667671275328] #quality_metric: host=algo-1, epoch=35, train total_loss <loss>=6.640991608301799\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:12 INFO 139667671275328] patience losses:[6.6631129251586065, 6.662654863463508, 6.647783597310384, 6.652277463012272, 6.647996955447727] min patience loss:6.647783597310384 current loss:6.640991608301799 absolute loss difference:0.00679198900858502\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:12 INFO 139667671275328] Timing: train: 1.64s, val: 0.00s, epoch: 1.64s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:12 INFO 139667671275328] #progress_metric: host=algo-1, completed 35.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309051.321377, \"EndTime\": 1646309052.9618661, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 34, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 158340.0, \"count\": 1, \"min\": 158340, \"max\": 158340}, \"Total Batches Seen\": {\"sum\": 1260.0, \"count\": 1, \"min\": 1260, \"max\": 1260}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 70.0, \"count\": 1, \"min\": 70, \"max\": 70}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:12 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=2757.3434650341087 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:12 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:12 INFO 139667671275328] # Starting training for epoch 36\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:04:14.468] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 107, \"duration\": 1505, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:14 INFO 139667671275328] # Finished training epoch 36 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:14 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:14 INFO 139667671275328] Loss (name: value) total: 6.630528748035431\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:14 INFO 139667671275328] Loss (name: value) kld: 0.1215728216080202\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:14 INFO 139667671275328] Loss (name: value) recons: 6.508955929014418\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:14 INFO 139667671275328] Loss (name: value) logppx: 6.630528748035431\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:14 INFO 139667671275328] #quality_metric: host=algo-1, epoch=36, train total_loss <loss>=6.630528748035431\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:14 INFO 139667671275328] patience losses:[6.662654863463508, 6.647783597310384, 6.652277463012272, 6.647996955447727, 6.640991608301799] min patience loss:6.640991608301799 current loss:6.630528748035431 absolute loss difference:0.01046286026636789\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:14 INFO 139667671275328] Timing: train: 1.51s, val: 0.00s, epoch: 1.51s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:14 INFO 139667671275328] #progress_metric: host=algo-1, completed 36.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309052.9622877, \"EndTime\": 1646309054.4752307, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 35, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 162864.0, \"count\": 1, \"min\": 162864, \"max\": 162864}, \"Total Batches Seen\": {\"sum\": 1296.0, \"count\": 1, \"min\": 1296, \"max\": 1296}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:14 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=2989.802748345054 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:14 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:14 INFO 139667671275328] # Starting training for epoch 37\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:04:13.964] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 113, \"duration\": 1439, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:13 INFO 140501882455872] # Finished training epoch 38 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:13 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:13 INFO 140501882455872] Loss (name: value) total: 6.671072555912866\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:13 INFO 140501882455872] Loss (name: value) kld: 0.11883660167869595\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:13 INFO 140501882455872] Loss (name: value) recons: 6.552235987451342\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:13 INFO 140501882455872] Loss (name: value) logppx: 6.671072555912866\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:13 INFO 140501882455872] #quality_metric: host=algo-2, epoch=38, train total_loss <loss>=6.671072555912866\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:13 INFO 140501882455872] patience losses:[6.690500848823124, 6.685550702942742, 6.6848527855343285, 6.677068279849158, 6.677471611234877] min patience loss:6.677068279849158 current loss:6.671072555912866 absolute loss difference:0.005995723936292663\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:13 INFO 140501882455872] Timing: train: 1.44s, val: 0.01s, epoch: 1.45s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:13 INFO 140501882455872] #progress_metric: host=algo-2, completed 38.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309052.5249808, \"EndTime\": 1646309053.972117, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 37, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 172026.0, \"count\": 1, \"min\": 172026, \"max\": 172026}, \"Total Batches Seen\": {\"sum\": 1368.0, \"count\": 1, \"min\": 1368, \"max\": 1368}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 76.0, \"count\": 1, \"min\": 76, \"max\": 76}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:13 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=3127.9202051145535 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:13 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:13 INFO 140501882455872] # Starting training for epoch 39\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:04:15.520] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 116, \"duration\": 1547, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:15 INFO 140501882455872] # Finished training epoch 39 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:15 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:15 INFO 140501882455872] Loss (name: value) total: 6.668180333243476\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:15 INFO 140501882455872] Loss (name: value) kld: 0.1214848557073209\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:15 INFO 140501882455872] Loss (name: value) recons: 6.546695503923628\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:15 INFO 140501882455872] Loss (name: value) logppx: 6.668180333243476\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:15 INFO 140501882455872] #quality_metric: host=algo-2, epoch=39, train total_loss <loss>=6.668180333243476\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:15 INFO 140501882455872] patience losses:[6.685550702942742, 6.6848527855343285, 6.677068279849158, 6.677471611234877, 6.671072555912866] min patience loss:6.671072555912866 current loss:6.668180333243476 absolute loss difference:0.00289222266938971\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:15 INFO 140501882455872] Timing: train: 1.55s, val: 0.00s, epoch: 1.55s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:15 INFO 140501882455872] #progress_metric: host=algo-2, completed 39.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309053.9724386, \"EndTime\": 1646309055.525746, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 38, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 176553.0, \"count\": 1, \"min\": 176553, \"max\": 176553}, \"Total Batches Seen\": {\"sum\": 1404.0, \"count\": 1, \"min\": 1404, \"max\": 1404}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 78.0, \"count\": 1, \"min\": 78, \"max\": 78}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:15 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=2914.132665408168 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:15 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:15 INFO 140501882455872] # Starting training for epoch 40\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:04:16.184] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 110, \"duration\": 1708, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:16 INFO 139667671275328] # Finished training epoch 37 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:16 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:16 INFO 139667671275328] Loss (name: value) total: 6.636443383163876\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:16 INFO 139667671275328] Loss (name: value) kld: 0.126739671971235\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:16 INFO 139667671275328] Loss (name: value) recons: 6.509703709019555\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:16 INFO 139667671275328] Loss (name: value) logppx: 6.636443383163876\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:16 INFO 139667671275328] #quality_metric: host=algo-1, epoch=37, train total_loss <loss>=6.636443383163876\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:16 INFO 139667671275328] patience losses:[6.647783597310384, 6.652277463012272, 6.647996955447727, 6.640991608301799, 6.630528748035431] min patience loss:6.630528748035431 current loss:6.636443383163876 absolute loss difference:0.005914635128444701\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:16 INFO 139667671275328] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:16 INFO 139667671275328] Timing: train: 1.71s, val: 0.00s, epoch: 1.71s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:16 INFO 139667671275328] #progress_metric: host=algo-1, completed 37.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309054.4755871, \"EndTime\": 1646309056.18619, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 36, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 167388.0, \"count\": 1, \"min\": 167388, \"max\": 167388}, \"Total Batches Seen\": {\"sum\": 1332.0, \"count\": 1, \"min\": 1332, \"max\": 1332}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 74.0, \"count\": 1, \"min\": 74, \"max\": 74}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:16 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=2644.330202507281 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:16 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:16 INFO 139667671275328] # Starting training for epoch 38\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:04:16.961] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 119, \"duration\": 1435, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:16 INFO 140501882455872] # Finished training epoch 40 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:16 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:16 INFO 140501882455872] Loss (name: value) total: 6.662067565653059\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:16 INFO 140501882455872] Loss (name: value) kld: 0.12548433844414023\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:16 INFO 140501882455872] Loss (name: value) recons: 6.536583191818661\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:16 INFO 140501882455872] Loss (name: value) logppx: 6.662067565653059\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:16 INFO 140501882455872] #quality_metric: host=algo-2, epoch=40, train total_loss <loss>=6.662067565653059\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:16 INFO 140501882455872] patience losses:[6.6848527855343285, 6.677068279849158, 6.677471611234877, 6.671072555912866, 6.668180333243476] min patience loss:6.668180333243476 current loss:6.662067565653059 absolute loss difference:0.006112767590416901\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:16 INFO 140501882455872] Timing: train: 1.44s, val: 0.01s, epoch: 1.44s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:16 INFO 140501882455872] #progress_metric: host=algo-2, completed 40.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309055.5260398, \"EndTime\": 1646309056.9698622, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 39, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 181080.0, \"count\": 1, \"min\": 181080, \"max\": 181080}, \"Total Batches Seen\": {\"sum\": 1440.0, \"count\": 1, \"min\": 1440, \"max\": 1440}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:16 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=3134.9695904412856 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:16 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:16 INFO 140501882455872] # Starting training for epoch 41\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:04:17.800] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 113, \"duration\": 1613, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:17 INFO 139667671275328] # Finished training epoch 38 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:17 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:17 INFO 139667671275328] Loss (name: value) total: 6.621777693430583\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:17 INFO 139667671275328] Loss (name: value) kld: 0.12755713953326145\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:17 INFO 139667671275328] Loss (name: value) recons: 6.494220548205906\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:17 INFO 139667671275328] Loss (name: value) logppx: 6.621777693430583\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:17 INFO 139667671275328] #quality_metric: host=algo-1, epoch=38, train total_loss <loss>=6.621777693430583\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:17 INFO 139667671275328] patience losses:[6.652277463012272, 6.647996955447727, 6.640991608301799, 6.630528748035431, 6.636443383163876] min patience loss:6.630528748035431 current loss:6.621777693430583 absolute loss difference:0.00875105460484793\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:17 INFO 139667671275328] Timing: train: 1.62s, val: 0.00s, epoch: 1.62s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:17 INFO 139667671275328] #progress_metric: host=algo-1, completed 38.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309056.1866338, \"EndTime\": 1646309057.8068328, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 37, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 171912.0, \"count\": 1, \"min\": 171912, \"max\": 171912}, \"Total Batches Seen\": {\"sum\": 1368.0, \"count\": 1, \"min\": 1368, \"max\": 1368}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 76.0, \"count\": 1, \"min\": 76, \"max\": 76}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:17 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=2791.843342755697 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:17 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:17 INFO 139667671275328] # Starting training for epoch 39\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:04:19.318] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 116, \"duration\": 1511, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:19 INFO 139667671275328] # Finished training epoch 39 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:19 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:19 INFO 139667671275328] Loss (name: value) total: 6.621735466851129\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:19 INFO 139667671275328] Loss (name: value) kld: 0.13108207233664063\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:19 INFO 139667671275328] Loss (name: value) recons: 6.490653428766462\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:19 INFO 139667671275328] Loss (name: value) logppx: 6.621735466851129\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:19 INFO 139667671275328] #quality_metric: host=algo-1, epoch=39, train total_loss <loss>=6.621735466851129\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:19 INFO 139667671275328] patience losses:[6.647996955447727, 6.640991608301799, 6.630528748035431, 6.636443383163876, 6.621777693430583] min patience loss:6.621777693430583 current loss:6.621735466851129 absolute loss difference:4.222657945440744e-05\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:19 INFO 139667671275328] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:19 INFO 139667671275328] Timing: train: 1.52s, val: 0.01s, epoch: 1.52s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:19 INFO 139667671275328] #progress_metric: host=algo-1, completed 39.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309057.8072495, \"EndTime\": 1646309059.328321, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 38, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 176436.0, \"count\": 1, \"min\": 176436, \"max\": 176436}, \"Total Batches Seen\": {\"sum\": 1404.0, \"count\": 1, \"min\": 1404, \"max\": 1404}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 78.0, \"count\": 1, \"min\": 78, \"max\": 78}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:19 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=2973.8207501350557 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:19 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:19 INFO 139667671275328] # Starting training for epoch 40\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:04:20.853] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 119, \"duration\": 1523, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:20 INFO 139667671275328] # Finished training epoch 40 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:20 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:20 INFO 139667671275328] Loss (name: value) total: 6.613672607474857\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:20 INFO 139667671275328] Loss (name: value) kld: 0.13269087692929638\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:20 INFO 139667671275328] Loss (name: value) recons: 6.480981740686628\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:20 INFO 139667671275328] Loss (name: value) logppx: 6.613672607474857\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:20 INFO 139667671275328] #quality_metric: host=algo-1, epoch=40, train total_loss <loss>=6.613672607474857\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:20 INFO 139667671275328] patience losses:[6.640991608301799, 6.630528748035431, 6.636443383163876, 6.621777693430583, 6.621735466851129] min patience loss:6.621735466851129 current loss:6.613672607474857 absolute loss difference:0.00806285937627127\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:20 INFO 139667671275328] Timing: train: 1.53s, val: 0.01s, epoch: 1.53s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:20 INFO 139667671275328] #progress_metric: host=algo-1, completed 40.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309059.329067, \"EndTime\": 1646309060.8638349, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 39, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 180960.0, \"count\": 1, \"min\": 180960, \"max\": 180960}, \"Total Batches Seen\": {\"sum\": 1440.0, \"count\": 1, \"min\": 1440, \"max\": 1440}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:20 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=2946.139274603483 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:20 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:20 INFO 139667671275328] # Starting training for epoch 41\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:04:22.341] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 122, \"duration\": 1475, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:22 INFO 139667671275328] # Finished training epoch 41 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:22 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:22 INFO 139667671275328] Loss (name: value) total: 6.606203695138295\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:22 INFO 139667671275328] Loss (name: value) kld: 0.13649041371213066\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:22 INFO 139667671275328] Loss (name: value) recons: 6.469713264041477\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:22 INFO 139667671275328] Loss (name: value) logppx: 6.606203695138295\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:22 INFO 139667671275328] #quality_metric: host=algo-1, epoch=41, train total_loss <loss>=6.606203695138295\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:22 INFO 139667671275328] patience losses:[6.630528748035431, 6.636443383163876, 6.621777693430583, 6.621735466851129, 6.613672607474857] min patience loss:6.613672607474857 current loss:6.606203695138295 absolute loss difference:0.007468912336562106\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:22 INFO 139667671275328] Timing: train: 1.48s, val: 0.00s, epoch: 1.48s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:22 INFO 139667671275328] #progress_metric: host=algo-1, completed 41.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309060.8651545, \"EndTime\": 1646309062.348143, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 40, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 185484.0, \"count\": 1, \"min\": 185484, \"max\": 185484}, \"Total Batches Seen\": {\"sum\": 1476.0, \"count\": 1, \"min\": 1476, \"max\": 1476}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 82.0, \"count\": 1, \"min\": 82, \"max\": 82}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:22 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=3050.2170667521423 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:22 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:22 INFO 139667671275328] # Starting training for epoch 42\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:04:23.981] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 125, \"duration\": 1632, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:23 INFO 139667671275328] # Finished training epoch 42 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:23 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:23 INFO 139667671275328] Loss (name: value) total: 6.601896729734209\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:23 INFO 139667671275328] Loss (name: value) kld: 0.13992755404776996\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:23 INFO 139667671275328] Loss (name: value) recons: 6.461969176928203\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:23 INFO 139667671275328] Loss (name: value) logppx: 6.601896729734209\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:23 INFO 139667671275328] #quality_metric: host=algo-1, epoch=42, train total_loss <loss>=6.601896729734209\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:23 INFO 139667671275328] patience losses:[6.636443383163876, 6.621777693430583, 6.621735466851129, 6.613672607474857, 6.606203695138295] min patience loss:6.606203695138295 current loss:6.601896729734209 absolute loss difference:0.004306965404086149\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:23 INFO 139667671275328] Timing: train: 1.63s, val: 0.00s, epoch: 1.64s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:23 INFO 139667671275328] #progress_metric: host=algo-1, completed 42.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309062.3484483, \"EndTime\": 1646309063.9877772, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 41, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 190008.0, \"count\": 1, \"min\": 190008, \"max\": 190008}, \"Total Batches Seen\": {\"sum\": 1512.0, \"count\": 1, \"min\": 1512, \"max\": 1512}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 84.0, \"count\": 1, \"min\": 84, \"max\": 84}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:23 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=2758.874517143233 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:23 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:23 INFO 139667671275328] # Starting training for epoch 43\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:04:25.472] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 128, \"duration\": 1482, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:25 INFO 139667671275328] # Finished training epoch 43 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:25 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:25 INFO 139667671275328] Loss (name: value) total: 6.600873251756032\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:25 INFO 139667671275328] Loss (name: value) kld: 0.14202828974359566\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:25 INFO 139667671275328] Loss (name: value) recons: 6.458844893508488\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:25 INFO 139667671275328] Loss (name: value) logppx: 6.600873251756032\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:25 INFO 139667671275328] #quality_metric: host=algo-1, epoch=43, train total_loss <loss>=6.600873251756032\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:25 INFO 139667671275328] patience losses:[6.621777693430583, 6.621735466851129, 6.613672607474857, 6.606203695138295, 6.601896729734209] min patience loss:6.601896729734209 current loss:6.600873251756032 absolute loss difference:0.0010234779781770342\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:25 INFO 139667671275328] Timing: train: 1.49s, val: 0.01s, epoch: 1.49s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:25 INFO 139667671275328] #progress_metric: host=algo-1, completed 43.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309063.988711, \"EndTime\": 1646309065.480704, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 42, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 194532.0, \"count\": 1, \"min\": 194532, \"max\": 194532}, \"Total Batches Seen\": {\"sum\": 1548.0, \"count\": 1, \"min\": 1548, \"max\": 1548}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 86.0, \"count\": 1, \"min\": 86, \"max\": 86}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:25 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=3031.540155019687 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:25 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:25 INFO 139667671275328] # Starting training for epoch 44\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:04:27.013] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 131, \"duration\": 1531, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:27 INFO 139667671275328] # Finished training epoch 44 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:27 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:27 INFO 139667671275328] Loss (name: value) total: 6.594906508922577\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:27 INFO 139667671275328] Loss (name: value) kld: 0.1470849666123589\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:27 INFO 139667671275328] Loss (name: value) recons: 6.447821570767297\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:27 INFO 139667671275328] Loss (name: value) logppx: 6.594906508922577\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:27 INFO 139667671275328] #quality_metric: host=algo-1, epoch=44, train total_loss <loss>=6.594906508922577\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:27 INFO 139667671275328] patience losses:[6.621735466851129, 6.613672607474857, 6.606203695138295, 6.601896729734209, 6.600873251756032] min patience loss:6.600873251756032 current loss:6.594906508922577 absolute loss difference:0.005966742833455108\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:27 INFO 139667671275328] Timing: train: 1.53s, val: 0.00s, epoch: 1.54s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:27 INFO 139667671275328] #progress_metric: host=algo-1, completed 44.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309065.4812305, \"EndTime\": 1646309067.0190732, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 43, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 199056.0, \"count\": 1, \"min\": 199056, \"max\": 199056}, \"Total Batches Seen\": {\"sum\": 1584.0, \"count\": 1, \"min\": 1584, \"max\": 1584}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 88.0, \"count\": 1, \"min\": 88, \"max\": 88}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:27 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=2941.3718963733527 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:27 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:27 INFO 139667671275328] # Starting training for epoch 45\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:04:28.569] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 134, \"duration\": 1549, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:28 INFO 139667671275328] # Finished training epoch 45 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:28 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:28 INFO 139667671275328] Loss (name: value) total: 6.5951483779483375\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:28 INFO 139667671275328] Loss (name: value) kld: 0.1480762434916364\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:28 INFO 139667671275328] Loss (name: value) recons: 6.447072108586629\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:28 INFO 139667671275328] Loss (name: value) logppx: 6.5951483779483375\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:28 INFO 139667671275328] #quality_metric: host=algo-1, epoch=45, train total_loss <loss>=6.5951483779483375\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:28 INFO 139667671275328] patience losses:[6.613672607474857, 6.606203695138295, 6.601896729734209, 6.600873251756032, 6.594906508922577] min patience loss:6.594906508922577 current loss:6.5951483779483375 absolute loss difference:0.00024186902576062153\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:28 INFO 139667671275328] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:28 INFO 139667671275328] Timing: train: 1.55s, val: 0.00s, epoch: 1.55s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:28 INFO 139667671275328] #progress_metric: host=algo-1, completed 45.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309067.0194871, \"EndTime\": 1646309068.5724545, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 44, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 203580.0, \"count\": 1, \"min\": 203580, \"max\": 203580}, \"Total Batches Seen\": {\"sum\": 1620.0, \"count\": 1, \"min\": 1620, \"max\": 1620}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 90.0, \"count\": 1, \"min\": 90, \"max\": 90}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:28 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=2912.2993829619027 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:28 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:28 INFO 139667671275328] # Starting training for epoch 46\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:04:30.155] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 137, \"duration\": 1581, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:30 INFO 139667671275328] # Finished training epoch 46 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:30 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:30 INFO 139667671275328] Loss (name: value) total: 6.584956924120585\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:30 INFO 139667671275328] Loss (name: value) kld: 0.14877813744048277\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:30 INFO 139667671275328] Loss (name: value) recons: 6.436178730593787\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:30 INFO 139667671275328] Loss (name: value) logppx: 6.584956924120585\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:30 INFO 139667671275328] #quality_metric: host=algo-1, epoch=46, train total_loss <loss>=6.584956924120585\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:30 INFO 139667671275328] patience losses:[6.606203695138295, 6.601896729734209, 6.600873251756032, 6.594906508922577, 6.5951483779483375] min patience loss:6.594906508922577 current loss:6.584956924120585 absolute loss difference:0.009949584801991485\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:30 INFO 139667671275328] Timing: train: 1.58s, val: 0.00s, epoch: 1.59s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:30 INFO 139667671275328] #progress_metric: host=algo-1, completed 46.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309068.573178, \"EndTime\": 1646309070.162673, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 45, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 208104.0, \"count\": 1, \"min\": 208104, \"max\": 208104}, \"Total Batches Seen\": {\"sum\": 1656.0, \"count\": 1, \"min\": 1656, \"max\": 1656}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 92.0, \"count\": 1, \"min\": 92, \"max\": 92}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:30 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=2845.502485817874 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:30 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:30 INFO 139667671275328] # Starting training for epoch 47\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:04:31.691] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 140, \"duration\": 1528, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:31 INFO 139667671275328] # Finished training epoch 47 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:31 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:31 INFO 139667671275328] Loss (name: value) total: 6.582765519618988\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:31 INFO 139667671275328] Loss (name: value) kld: 0.15278709452185366\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:31 INFO 139667671275328] Loss (name: value) recons: 6.429978470007579\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:31 INFO 139667671275328] Loss (name: value) logppx: 6.582765519618988\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:31 INFO 139667671275328] #quality_metric: host=algo-1, epoch=47, train total_loss <loss>=6.582765519618988\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:31 INFO 139667671275328] patience losses:[6.601896729734209, 6.600873251756032, 6.594906508922577, 6.5951483779483375, 6.584956924120585] min patience loss:6.584956924120585 current loss:6.582765519618988 absolute loss difference:0.0021914045015973826\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:31 INFO 139667671275328] Timing: train: 1.53s, val: 0.00s, epoch: 1.53s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:31 INFO 139667671275328] #progress_metric: host=algo-1, completed 47.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309070.163462, \"EndTime\": 1646309071.6968937, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 46, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 212628.0, \"count\": 1, \"min\": 212628, \"max\": 212628}, \"Total Batches Seen\": {\"sum\": 1692.0, \"count\": 1, \"min\": 1692, \"max\": 1692}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 94.0, \"count\": 1, \"min\": 94, \"max\": 94}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:31 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=2949.857831233898 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:31 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:31 INFO 139667671275328] # Starting training for epoch 48\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:04:33.276] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 143, \"duration\": 1575, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:33 INFO 139667671275328] # Finished training epoch 48 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:33 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:33 INFO 139667671275328] Loss (name: value) total: 6.579203135437435\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:33 INFO 139667671275328] Loss (name: value) kld: 0.1552004942463504\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:33 INFO 139667671275328] Loss (name: value) recons: 6.42400265402264\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:33 INFO 139667671275328] Loss (name: value) logppx: 6.579203135437435\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:33 INFO 139667671275328] #quality_metric: host=algo-1, epoch=48, train total_loss <loss>=6.579203135437435\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:33 INFO 139667671275328] patience losses:[6.600873251756032, 6.594906508922577, 6.5951483779483375, 6.584956924120585, 6.582765519618988] min patience loss:6.582765519618988 current loss:6.579203135437435 absolute loss difference:0.003562384181552858\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:33 INFO 139667671275328] Timing: train: 1.58s, val: 0.00s, epoch: 1.58s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:33 INFO 139667671275328] #progress_metric: host=algo-1, completed 48.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309071.6973138, \"EndTime\": 1646309073.2819927, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 47, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 217152.0, \"count\": 1, \"min\": 217152, \"max\": 217152}, \"Total Batches Seen\": {\"sum\": 1728.0, \"count\": 1, \"min\": 1728, \"max\": 1728}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 96.0, \"count\": 1, \"min\": 96, \"max\": 96}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:33 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=2854.307122939882 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:33 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:33 INFO 139667671275328] # Starting training for epoch 49\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:04:18.580] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 122, \"duration\": 1609, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:18 INFO 140501882455872] # Finished training epoch 41 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:18 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:18 INFO 140501882455872] Loss (name: value) total: 6.653964625464545\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:18 INFO 140501882455872] Loss (name: value) kld: 0.12671753950417042\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:18 INFO 140501882455872] Loss (name: value) recons: 6.527247137493557\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:18 INFO 140501882455872] Loss (name: value) logppx: 6.653964625464545\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:18 INFO 140501882455872] #quality_metric: host=algo-2, epoch=41, train total_loss <loss>=6.653964625464545\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:18 INFO 140501882455872] patience losses:[6.677068279849158, 6.677471611234877, 6.671072555912866, 6.668180333243476, 6.662067565653059] min patience loss:6.662067565653059 current loss:6.653964625464545 absolute loss difference:0.008102940188513763\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:18 INFO 140501882455872] Timing: train: 1.61s, val: 0.01s, epoch: 1.62s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:18 INFO 140501882455872] #progress_metric: host=algo-2, completed 41.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309056.970255, \"EndTime\": 1646309058.5874786, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 40, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 185607.0, \"count\": 1, \"min\": 185607, \"max\": 185607}, \"Total Batches Seen\": {\"sum\": 1476.0, \"count\": 1, \"min\": 1476, \"max\": 1476}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 82.0, \"count\": 1, \"min\": 82, \"max\": 82}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:18 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=2798.9432857531806 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:18 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:18 INFO 140501882455872] # Starting training for epoch 42\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:04:20.119] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 125, \"duration\": 1531, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:20 INFO 140501882455872] # Finished training epoch 42 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:20 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:20 INFO 140501882455872] Loss (name: value) total: 6.651634408368005\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:20 INFO 140501882455872] Loss (name: value) kld: 0.12930012121796608\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:20 INFO 140501882455872] Loss (name: value) recons: 6.522334264384376\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:20 INFO 140501882455872] Loss (name: value) logppx: 6.651634408368005\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:20 INFO 140501882455872] #quality_metric: host=algo-2, epoch=42, train total_loss <loss>=6.651634408368005\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:20 INFO 140501882455872] patience losses:[6.677471611234877, 6.671072555912866, 6.668180333243476, 6.662067565653059, 6.653964625464545] min patience loss:6.653964625464545 current loss:6.651634408368005 absolute loss difference:0.0023302170965404656\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:20 INFO 140501882455872] Timing: train: 1.53s, val: 0.01s, epoch: 1.54s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:20 INFO 140501882455872] #progress_metric: host=algo-2, completed 42.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309058.587832, \"EndTime\": 1646309060.1284254, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 41, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 190134.0, \"count\": 1, \"min\": 190134, \"max\": 190134}, \"Total Batches Seen\": {\"sum\": 1512.0, \"count\": 1, \"min\": 1512, \"max\": 1512}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 84.0, \"count\": 1, \"min\": 84, \"max\": 84}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:20 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=2938.0902965847954 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:20 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:20 INFO 140501882455872] # Starting training for epoch 43\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:04:21.565] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 128, \"duration\": 1436, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:21 INFO 140501882455872] # Finished training epoch 43 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:21 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:21 INFO 140501882455872] Loss (name: value) total: 6.651493900352055\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:21 INFO 140501882455872] Loss (name: value) kld: 0.13147454439765877\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:21 INFO 140501882455872] Loss (name: value) recons: 6.520019325945112\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:21 INFO 140501882455872] Loss (name: value) logppx: 6.651493900352055\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:21 INFO 140501882455872] #quality_metric: host=algo-2, epoch=43, train total_loss <loss>=6.651493900352055\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:21 INFO 140501882455872] patience losses:[6.671072555912866, 6.668180333243476, 6.662067565653059, 6.653964625464545, 6.651634408368005] min patience loss:6.651634408368005 current loss:6.651493900352055 absolute loss difference:0.00014050801595022477\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:21 INFO 140501882455872] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:21 INFO 140501882455872] Timing: train: 1.44s, val: 0.01s, epoch: 1.45s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:21 INFO 140501882455872] #progress_metric: host=algo-2, completed 43.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309060.1288364, \"EndTime\": 1646309061.5761664, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 42, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 194661.0, \"count\": 1, \"min\": 194661, \"max\": 194661}, \"Total Batches Seen\": {\"sum\": 1548.0, \"count\": 1, \"min\": 1548, \"max\": 1548}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 86.0, \"count\": 1, \"min\": 86, \"max\": 86}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:21 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=3127.3921351440335 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:21 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:21 INFO 140501882455872] # Starting training for epoch 44\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:04:23.138] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 131, \"duration\": 1561, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:23 INFO 140501882455872] # Finished training epoch 44 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:23 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:23 INFO 140501882455872] Loss (name: value) total: 6.6438835395707025\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:23 INFO 140501882455872] Loss (name: value) kld: 0.13660867626054418\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:23 INFO 140501882455872] Loss (name: value) recons: 6.507274885972341\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:23 INFO 140501882455872] Loss (name: value) logppx: 6.6438835395707025\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:23 INFO 140501882455872] #quality_metric: host=algo-2, epoch=44, train total_loss <loss>=6.6438835395707025\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:23 INFO 140501882455872] patience losses:[6.668180333243476, 6.662067565653059, 6.653964625464545, 6.651634408368005, 6.651493900352055] min patience loss:6.651493900352055 current loss:6.6438835395707025 absolute loss difference:0.007610360781352021\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:23 INFO 140501882455872] Timing: train: 1.56s, val: 0.01s, epoch: 1.57s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:23 INFO 140501882455872] #progress_metric: host=algo-2, completed 44.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309061.5765896, \"EndTime\": 1646309063.1458986, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 43, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 199188.0, \"count\": 1, \"min\": 199188, \"max\": 199188}, \"Total Batches Seen\": {\"sum\": 1584.0, \"count\": 1, \"min\": 1584, \"max\": 1584}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 88.0, \"count\": 1, \"min\": 88, \"max\": 88}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:23 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=2884.3809161543804 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:23 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:23 INFO 140501882455872] # Starting training for epoch 45\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:04:24.569] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 134, \"duration\": 1422, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:24 INFO 140501882455872] # Finished training epoch 45 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:24 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:24 INFO 140501882455872] Loss (name: value) total: 6.638270722495185\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:24 INFO 140501882455872] Loss (name: value) kld: 0.13695011898461315\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:24 INFO 140501882455872] Loss (name: value) recons: 6.501320593886906\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:24 INFO 140501882455872] Loss (name: value) logppx: 6.638270722495185\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:24 INFO 140501882455872] #quality_metric: host=algo-2, epoch=45, train total_loss <loss>=6.638270722495185\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:24 INFO 140501882455872] patience losses:[6.662067565653059, 6.653964625464545, 6.651634408368005, 6.651493900352055, 6.6438835395707025] min patience loss:6.6438835395707025 current loss:6.638270722495185 absolute loss difference:0.00561281707551764\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:24 INFO 140501882455872] Timing: train: 1.43s, val: 0.01s, epoch: 1.43s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:24 INFO 140501882455872] #progress_metric: host=algo-2, completed 45.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309063.1463332, \"EndTime\": 1646309064.5782437, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 44, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 203715.0, \"count\": 1, \"min\": 203715, \"max\": 203715}, \"Total Batches Seen\": {\"sum\": 1620.0, \"count\": 1, \"min\": 1620, \"max\": 1620}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 90.0, \"count\": 1, \"min\": 90, \"max\": 90}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:24 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=3161.08092385083 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:24 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:24 INFO 140501882455872] # Starting training for epoch 46\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:04:26.067] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 137, \"duration\": 1488, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:26 INFO 140501882455872] # Finished training epoch 46 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:26 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:26 INFO 140501882455872] Loss (name: value) total: 6.633586612012651\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:26 INFO 140501882455872] Loss (name: value) kld: 0.14308573264214727\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:26 INFO 140501882455872] Loss (name: value) recons: 6.4905008806122675\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:26 INFO 140501882455872] Loss (name: value) logppx: 6.633586612012651\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:26 INFO 140501882455872] #quality_metric: host=algo-2, epoch=46, train total_loss <loss>=6.633586612012651\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:26 INFO 140501882455872] patience losses:[6.653964625464545, 6.651634408368005, 6.651493900352055, 6.6438835395707025, 6.638270722495185] min patience loss:6.638270722495185 current loss:6.633586612012651 absolute loss difference:0.004684110482533477\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:26 INFO 140501882455872] Timing: train: 1.49s, val: 0.00s, epoch: 1.49s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:26 INFO 140501882455872] #progress_metric: host=algo-2, completed 46.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309064.5786135, \"EndTime\": 1646309066.0733242, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 45, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 208242.0, \"count\": 1, \"min\": 208242, \"max\": 208242}, \"Total Batches Seen\": {\"sum\": 1656.0, \"count\": 1, \"min\": 1656, \"max\": 1656}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 92.0, \"count\": 1, \"min\": 92, \"max\": 92}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:26 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=3028.279809936015 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:26 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:26 INFO 140501882455872] # Starting training for epoch 47\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:04:27.514] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 140, \"duration\": 1439, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:27 INFO 140501882455872] # Finished training epoch 47 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:27 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:27 INFO 140501882455872] Loss (name: value) total: 6.625221947828929\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:27 INFO 140501882455872] Loss (name: value) kld: 0.1455383367008633\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:27 INFO 140501882455872] Loss (name: value) recons: 6.479683571391636\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:27 INFO 140501882455872] Loss (name: value) logppx: 6.625221947828929\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:27 INFO 140501882455872] #quality_metric: host=algo-2, epoch=47, train total_loss <loss>=6.625221947828929\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:27 INFO 140501882455872] patience losses:[6.651634408368005, 6.651493900352055, 6.6438835395707025, 6.638270722495185, 6.633586612012651] min patience loss:6.633586612012651 current loss:6.625221947828929 absolute loss difference:0.008364664183722503\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:27 INFO 140501882455872] Timing: train: 1.44s, val: 0.01s, epoch: 1.45s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:27 INFO 140501882455872] #progress_metric: host=algo-2, completed 47.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309066.0737245, \"EndTime\": 1646309067.5216744, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 46, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 212769.0, \"count\": 1, \"min\": 212769, \"max\": 212769}, \"Total Batches Seen\": {\"sum\": 1692.0, \"count\": 1, \"min\": 1692, \"max\": 1692}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 94.0, \"count\": 1, \"min\": 94, \"max\": 94}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:27 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=3126.058586636536 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:27 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:27 INFO 140501882455872] # Starting training for epoch 48\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:04:29.049] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 143, \"duration\": 1526, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:29 INFO 140501882455872] # Finished training epoch 48 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:29 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:29 INFO 140501882455872] Loss (name: value) total: 6.627104189660814\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:29 INFO 140501882455872] Loss (name: value) kld: 0.14656563041110834\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:29 INFO 140501882455872] Loss (name: value) recons: 6.480538540416294\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:29 INFO 140501882455872] Loss (name: value) logppx: 6.627104189660814\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:29 INFO 140501882455872] #quality_metric: host=algo-2, epoch=48, train total_loss <loss>=6.627104189660814\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:29 INFO 140501882455872] patience losses:[6.651493900352055, 6.6438835395707025, 6.638270722495185, 6.633586612012651, 6.625221947828929] min patience loss:6.625221947828929 current loss:6.627104189660814 absolute loss difference:0.001882241831885345\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:29 INFO 140501882455872] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:29 INFO 140501882455872] Timing: train: 1.53s, val: 0.00s, epoch: 1.53s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:29 INFO 140501882455872] #progress_metric: host=algo-2, completed 48.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309067.5221066, \"EndTime\": 1646309069.0514023, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 47, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 217296.0, \"count\": 1, \"min\": 217296, \"max\": 217296}, \"Total Batches Seen\": {\"sum\": 1728.0, \"count\": 1, \"min\": 1728, \"max\": 1728}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 96.0, \"count\": 1, \"min\": 96, \"max\": 96}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:29 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=2959.8262926393636 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:29 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:29 INFO 140501882455872] # Starting training for epoch 49\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:04:30.502] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 146, \"duration\": 1449, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:30 INFO 140501882455872] # Finished training epoch 49 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:30 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:30 INFO 140501882455872] Loss (name: value) total: 6.62364317311181\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:30 INFO 140501882455872] Loss (name: value) kld: 0.14830420207646158\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:30 INFO 140501882455872] Loss (name: value) recons: 6.475338922606574\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:30 INFO 140501882455872] Loss (name: value) logppx: 6.62364317311181\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:30 INFO 140501882455872] #quality_metric: host=algo-2, epoch=49, train total_loss <loss>=6.62364317311181\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:30 INFO 140501882455872] patience losses:[6.6438835395707025, 6.638270722495185, 6.633586612012651, 6.625221947828929, 6.627104189660814] min patience loss:6.625221947828929 current loss:6.62364317311181 absolute loss difference:0.0015787747171192024\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:30 INFO 140501882455872] Timing: train: 1.45s, val: 0.00s, epoch: 1.46s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:30 INFO 140501882455872] #progress_metric: host=algo-2, completed 49.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309069.0517626, \"EndTime\": 1646309070.508251, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 48, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 221823.0, \"count\": 1, \"min\": 221823, \"max\": 221823}, \"Total Batches Seen\": {\"sum\": 1764.0, \"count\": 1, \"min\": 1764, \"max\": 1764}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 98.0, \"count\": 1, \"min\": 98, \"max\": 98}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:30 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=3107.7451348430022 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:30 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:30 INFO 140501882455872] # Starting training for epoch 50\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:04:31.975] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 149, \"duration\": 1466, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:31 INFO 140501882455872] # Finished training epoch 50 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:31 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:31 INFO 140501882455872] Loss (name: value) total: 6.622564123736487\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:31 INFO 140501882455872] Loss (name: value) kld: 0.15191342619558176\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:31 INFO 140501882455872] Loss (name: value) recons: 6.470650706026289\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:31 INFO 140501882455872] Loss (name: value) logppx: 6.622564123736487\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:31 INFO 140501882455872] #quality_metric: host=algo-2, epoch=50, train total_loss <loss>=6.622564123736487\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:31 INFO 140501882455872] patience losses:[6.638270722495185, 6.633586612012651, 6.625221947828929, 6.627104189660814, 6.62364317311181] min patience loss:6.62364317311181 current loss:6.622564123736487 absolute loss difference:0.0010790493753223274\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:31 INFO 140501882455872] Timing: train: 1.47s, val: 0.01s, epoch: 1.47s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:31 INFO 140501882455872] #progress_metric: host=algo-2, completed 50.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309070.508621, \"EndTime\": 1646309071.9825592, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 49, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 226350.0, \"count\": 1, \"min\": 226350, \"max\": 226350}, \"Total Batches Seen\": {\"sum\": 1800.0, \"count\": 1, \"min\": 1800, \"max\": 1800}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 100.0, \"count\": 1, \"min\": 100, \"max\": 100}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:31 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=3070.339863840241 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:31 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:31 INFO 140501882455872] # Starting training for epoch 51\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:04:33.479] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 152, \"duration\": 1495, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:33 INFO 140501882455872] # Finished training epoch 51 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:33 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:33 INFO 140501882455872] Loss (name: value) total: 6.611502236790127\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:33 INFO 140501882455872] Loss (name: value) kld: 0.15280787812338936\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:33 INFO 140501882455872] Loss (name: value) recons: 6.458694252702925\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:33 INFO 140501882455872] Loss (name: value) logppx: 6.611502236790127\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:33 INFO 140501882455872] #quality_metric: host=algo-2, epoch=51, train total_loss <loss>=6.611502236790127\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:33 INFO 140501882455872] patience losses:[6.633586612012651, 6.625221947828929, 6.627104189660814, 6.62364317311181, 6.622564123736487] min patience loss:6.622564123736487 current loss:6.611502236790127 absolute loss difference:0.011061886946360566\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:33 INFO 140501882455872] Timing: train: 1.50s, val: 0.00s, epoch: 1.50s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:33 INFO 140501882455872] #progress_metric: host=algo-2, completed 51.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309071.9833329, \"EndTime\": 1646309073.486097, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 50, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 230877.0, \"count\": 1, \"min\": 230877, \"max\": 230877}, \"Total Batches Seen\": {\"sum\": 1836.0, \"count\": 1, \"min\": 1836, \"max\": 1836}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 102.0, \"count\": 1, \"min\": 102, \"max\": 102}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:33 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=3011.930139186423 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:33 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:33 INFO 140501882455872] # Starting training for epoch 52\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:04:34.794] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 146, \"duration\": 1511, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:34 INFO 139667671275328] # Finished training epoch 49 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:34 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:34 INFO 139667671275328] Loss (name: value) total: 6.575775934590234\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:34 INFO 139667671275328] Loss (name: value) kld: 0.15926945747600663\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:34 INFO 139667671275328] Loss (name: value) recons: 6.416506462626987\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:34 INFO 139667671275328] Loss (name: value) logppx: 6.575775934590234\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:34 INFO 139667671275328] #quality_metric: host=algo-1, epoch=49, train total_loss <loss>=6.575775934590234\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:34 INFO 139667671275328] patience losses:[6.594906508922577, 6.5951483779483375, 6.584956924120585, 6.582765519618988, 6.579203135437435] min patience loss:6.579203135437435 current loss:6.575775934590234 absolute loss difference:0.0034272008472013837\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:34 INFO 139667671275328] Timing: train: 1.51s, val: 0.00s, epoch: 1.52s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:34 INFO 139667671275328] #progress_metric: host=algo-1, completed 49.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309073.282496, \"EndTime\": 1646309074.7996213, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 48, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 221676.0, \"count\": 1, \"min\": 221676, \"max\": 221676}, \"Total Batches Seen\": {\"sum\": 1764.0, \"count\": 1, \"min\": 1764, \"max\": 1764}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 98.0, \"count\": 1, \"min\": 98, \"max\": 98}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:34 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=2981.5185899537996 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:34 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:34 INFO 139667671275328] # Starting training for epoch 50\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:04:36.376] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 149, \"duration\": 1576, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:36 INFO 139667671275328] # Finished training epoch 50 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:36 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:36 INFO 139667671275328] Loss (name: value) total: 6.572551793522305\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:36 INFO 139667671275328] Loss (name: value) kld: 0.16049539566867882\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:36 INFO 139667671275328] Loss (name: value) recons: 6.412056353357103\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:36 INFO 139667671275328] Loss (name: value) logppx: 6.572551793522305\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:36 INFO 139667671275328] #quality_metric: host=algo-1, epoch=50, train total_loss <loss>=6.572551793522305\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:36 INFO 139667671275328] patience losses:[6.5951483779483375, 6.584956924120585, 6.582765519618988, 6.579203135437435, 6.575775934590234] min patience loss:6.575775934590234 current loss:6.572551793522305 absolute loss difference:0.0032241410679292315\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:36 INFO 139667671275328] Timing: train: 1.58s, val: 0.01s, epoch: 1.59s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:36 INFO 139667671275328] #progress_metric: host=algo-1, completed 50.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309074.8000927, \"EndTime\": 1646309076.3858337, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 49, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 226200.0, \"count\": 1, \"min\": 226200, \"max\": 226200}, \"Total Batches Seen\": {\"sum\": 1800.0, \"count\": 1, \"min\": 1800, \"max\": 1800}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 100.0, \"count\": 1, \"min\": 100, \"max\": 100}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:36 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=2852.5131081593013 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:36 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:36 INFO 139667671275328] # Starting training for epoch 51\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:04:37.890] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 152, \"duration\": 1503, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:37 INFO 139667671275328] # Finished training epoch 51 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:37 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:37 INFO 139667671275328] Loss (name: value) total: 6.571708261966705\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:37 INFO 139667671275328] Loss (name: value) kld: 0.1621695159830981\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:37 INFO 139667671275328] Loss (name: value) recons: 6.409538778993818\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:37 INFO 139667671275328] Loss (name: value) logppx: 6.571708261966705\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:37 INFO 139667671275328] #quality_metric: host=algo-1, epoch=51, train total_loss <loss>=6.571708261966705\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:37 INFO 139667671275328] patience losses:[6.584956924120585, 6.582765519618988, 6.579203135437435, 6.575775934590234, 6.572551793522305] min patience loss:6.572551793522305 current loss:6.571708261966705 absolute loss difference:0.0008435315555992418\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:37 INFO 139667671275328] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:37 INFO 139667671275328] Timing: train: 1.51s, val: 0.01s, epoch: 1.51s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:37 INFO 139667671275328] #progress_metric: host=algo-1, completed 51.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309076.3862703, \"EndTime\": 1646309077.8985012, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 50, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 230724.0, \"count\": 1, \"min\": 230724, \"max\": 230724}, \"Total Batches Seen\": {\"sum\": 1836.0, \"count\": 1, \"min\": 1836, \"max\": 1836}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 102.0, \"count\": 1, \"min\": 102, \"max\": 102}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:37 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=2991.236015462655 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:37 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:37 INFO 139667671275328] # Starting training for epoch 52\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:04:34.893] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 155, \"duration\": 1406, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:34 INFO 140501882455872] # Finished training epoch 52 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:34 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:34 INFO 140501882455872] Loss (name: value) total: 6.612351271841261\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:34 INFO 140501882455872] Loss (name: value) kld: 0.15555540120436084\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:34 INFO 140501882455872] Loss (name: value) recons: 6.456795811653137\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:34 INFO 140501882455872] Loss (name: value) logppx: 6.612351271841261\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:34 INFO 140501882455872] #quality_metric: host=algo-2, epoch=52, train total_loss <loss>=6.612351271841261\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:34 INFO 140501882455872] patience losses:[6.625221947828929, 6.627104189660814, 6.62364317311181, 6.622564123736487, 6.611502236790127] min patience loss:6.611502236790127 current loss:6.612351271841261 absolute loss difference:0.0008490350511340949\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:34 INFO 140501882455872] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:34 INFO 140501882455872] Timing: train: 1.41s, val: 0.00s, epoch: 1.41s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:34 INFO 140501882455872] #progress_metric: host=algo-2, completed 52.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309073.4865618, \"EndTime\": 1646309074.8962054, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 51, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 235404.0, \"count\": 1, \"min\": 235404, \"max\": 235404}, \"Total Batches Seen\": {\"sum\": 1872.0, \"count\": 1, \"min\": 1872, \"max\": 1872}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 104.0, \"count\": 1, \"min\": 104, \"max\": 104}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:34 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=3210.9796169339156 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:34 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:34 INFO 140501882455872] # Starting training for epoch 53\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:04:36.456] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 158, \"duration\": 1558, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:36 INFO 140501882455872] # Finished training epoch 53 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:36 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:36 INFO 140501882455872] Loss (name: value) total: 6.608671585718791\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:36 INFO 140501882455872] Loss (name: value) kld: 0.15729105306996238\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:36 INFO 140501882455872] Loss (name: value) recons: 6.451380517747667\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:36 INFO 140501882455872] Loss (name: value) logppx: 6.608671585718791\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:36 INFO 140501882455872] #quality_metric: host=algo-2, epoch=53, train total_loss <loss>=6.608671585718791\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:36 INFO 140501882455872] patience losses:[6.627104189660814, 6.62364317311181, 6.622564123736487, 6.611502236790127, 6.612351271841261] min patience loss:6.611502236790127 current loss:6.608671585718791 absolute loss difference:0.0028306510713358435\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:36 INFO 140501882455872] Timing: train: 1.56s, val: 0.00s, epoch: 1.57s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:36 INFO 140501882455872] #progress_metric: host=algo-2, completed 53.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309074.8965895, \"EndTime\": 1646309076.4622543, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 52, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 239931.0, \"count\": 1, \"min\": 239931, \"max\": 239931}, \"Total Batches Seen\": {\"sum\": 1908.0, \"count\": 1, \"min\": 1908, \"max\": 1908}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 106.0, \"count\": 1, \"min\": 106, \"max\": 106}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:36 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=2891.1052563060734 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:36 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:36 INFO 140501882455872] # Starting training for epoch 54\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:04:37.966] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 161, \"duration\": 1502, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:37 INFO 140501882455872] # Finished training epoch 54 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:37 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:37 INFO 140501882455872] Loss (name: value) total: 6.602026634746128\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:37 INFO 140501882455872] Loss (name: value) kld: 0.15979856066405773\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:37 INFO 140501882455872] Loss (name: value) recons: 6.442228085464901\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:37 INFO 140501882455872] Loss (name: value) logppx: 6.602026634746128\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:37 INFO 140501882455872] #quality_metric: host=algo-2, epoch=54, train total_loss <loss>=6.602026634746128\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:37 INFO 140501882455872] patience losses:[6.62364317311181, 6.622564123736487, 6.611502236790127, 6.612351271841261, 6.608671585718791] min patience loss:6.608671585718791 current loss:6.602026634746128 absolute loss difference:0.006644950972662933\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:37 INFO 140501882455872] Timing: train: 1.51s, val: 0.00s, epoch: 1.51s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:37 INFO 140501882455872] #progress_metric: host=algo-2, completed 54.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309076.462566, \"EndTime\": 1646309077.9711025, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 53, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 244458.0, \"count\": 1, \"min\": 244458, \"max\": 244458}, \"Total Batches Seen\": {\"sum\": 1944.0, \"count\": 1, \"min\": 1944, \"max\": 1944}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 108.0, \"count\": 1, \"min\": 108, \"max\": 108}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:37 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=3000.553601061435 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:37 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:37 INFO 140501882455872] # Starting training for epoch 55\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:04:39.507] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 155, \"duration\": 1607, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:39 INFO 139667671275328] # Finished training epoch 52 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:39 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:39 INFO 139667671275328] Loss (name: value) total: 6.564740081628163\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:39 INFO 139667671275328] Loss (name: value) kld: 0.1646947433344192\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:39 INFO 139667671275328] Loss (name: value) recons: 6.400045308801863\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:39 INFO 139667671275328] Loss (name: value) logppx: 6.564740081628163\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:39 INFO 139667671275328] #quality_metric: host=algo-1, epoch=52, train total_loss <loss>=6.564740081628163\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:39 INFO 139667671275328] patience losses:[6.582765519618988, 6.579203135437435, 6.575775934590234, 6.572551793522305, 6.571708261966705] min patience loss:6.571708261966705 current loss:6.564740081628163 absolute loss difference:0.006968180338541963\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:39 INFO 139667671275328] Timing: train: 1.61s, val: 0.00s, epoch: 1.61s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:39 INFO 139667671275328] #progress_metric: host=algo-1, completed 52.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309077.8992229, \"EndTime\": 1646309079.5137482, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 51, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 235248.0, \"count\": 1, \"min\": 235248, \"max\": 235248}, \"Total Batches Seen\": {\"sum\": 1872.0, \"count\": 1, \"min\": 1872, \"max\": 1872}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 104.0, \"count\": 1, \"min\": 104, \"max\": 104}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:39 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=2801.66066164854 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:39 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:39 INFO 139667671275328] # Starting training for epoch 53\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:04:39.486] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 164, \"duration\": 1514, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:39 INFO 140501882455872] # Finished training epoch 55 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:39 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:39 INFO 140501882455872] Loss (name: value) total: 6.599981884161632\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:39 INFO 140501882455872] Loss (name: value) kld: 0.1619293379286925\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:39 INFO 140501882455872] Loss (name: value) recons: 6.4380525549252825\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:39 INFO 140501882455872] Loss (name: value) logppx: 6.599981884161632\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:39 INFO 140501882455872] #quality_metric: host=algo-2, epoch=55, train total_loss <loss>=6.599981884161632\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:39 INFO 140501882455872] patience losses:[6.622564123736487, 6.611502236790127, 6.612351271841261, 6.608671585718791, 6.602026634746128] min patience loss:6.602026634746128 current loss:6.599981884161632 absolute loss difference:0.002044750584496491\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:39 INFO 140501882455872] Timing: train: 1.52s, val: 0.00s, epoch: 1.52s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:39 INFO 140501882455872] #progress_metric: host=algo-2, completed 55.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309077.971438, \"EndTime\": 1646309079.492112, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 54, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 248985.0, \"count\": 1, \"min\": 248985, \"max\": 248985}, \"Total Batches Seen\": {\"sum\": 1980.0, \"count\": 1, \"min\": 1980, \"max\": 1980}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 110.0, \"count\": 1, \"min\": 110, \"max\": 110}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:39 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=2976.5867613190744 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:39 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:39 INFO 140501882455872] # Starting training for epoch 56\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:04:41.007] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 158, \"duration\": 1492, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:41 INFO 139667671275328] # Finished training epoch 53 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:41 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:41 INFO 139667671275328] Loss (name: value) total: 6.564485430717468\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:41 INFO 139667671275328] Loss (name: value) kld: 0.16517717660301262\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:41 INFO 139667671275328] Loss (name: value) recons: 6.399308337105645\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:41 INFO 139667671275328] Loss (name: value) logppx: 6.564485430717468\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:41 INFO 139667671275328] #quality_metric: host=algo-1, epoch=53, train total_loss <loss>=6.564485430717468\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:41 INFO 139667671275328] patience losses:[6.579203135437435, 6.575775934590234, 6.572551793522305, 6.571708261966705, 6.564740081628163] min patience loss:6.564740081628163 current loss:6.564485430717468 absolute loss difference:0.0002546509106950978\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:41 INFO 139667671275328] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:41 INFO 139667671275328] Timing: train: 1.50s, val: 0.01s, epoch: 1.50s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:41 INFO 139667671275328] #progress_metric: host=algo-1, completed 53.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309079.5142138, \"EndTime\": 1646309081.0159755, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 52, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 239772.0, \"count\": 1, \"min\": 239772, \"max\": 239772}, \"Total Batches Seen\": {\"sum\": 1908.0, \"count\": 1, \"min\": 1908, \"max\": 1908}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 106.0, \"count\": 1, \"min\": 106, \"max\": 106}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:41 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=3012.0349919314103 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:41 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:41 INFO 139667671275328] # Starting training for epoch 54\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:04:42.585] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 161, \"duration\": 1568, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:42 INFO 139667671275328] # Finished training epoch 54 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:42 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:42 INFO 139667671275328] Loss (name: value) total: 6.566192156738705\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:42 INFO 139667671275328] Loss (name: value) kld: 0.16900008658154142\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:42 INFO 139667671275328] Loss (name: value) recons: 6.39719201458825\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:42 INFO 139667671275328] Loss (name: value) logppx: 6.566192156738705\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:42 INFO 139667671275328] #quality_metric: host=algo-1, epoch=54, train total_loss <loss>=6.566192156738705\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:42 INFO 139667671275328] patience losses:[6.575775934590234, 6.572551793522305, 6.571708261966705, 6.564740081628163, 6.564485430717468] min patience loss:6.564485430717468 current loss:6.566192156738705 absolute loss difference:0.0017067260212364488\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:42 INFO 139667671275328] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:42 INFO 139667671275328] Timing: train: 1.57s, val: 0.00s, epoch: 1.57s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:42 INFO 139667671275328] #progress_metric: host=algo-1, completed 54.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309081.0163152, \"EndTime\": 1646309082.58738, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 53, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 244296.0, \"count\": 1, \"min\": 244296, \"max\": 244296}, \"Total Batches Seen\": {\"sum\": 1944.0, \"count\": 1, \"min\": 1944, \"max\": 1944}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 108.0, \"count\": 1, \"min\": 108, \"max\": 108}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:42 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=2879.109217367608 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:42 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:42 INFO 139667671275328] # Starting training for epoch 55\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:04:40.957] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 167, \"duration\": 1464, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:40 INFO 140501882455872] # Finished training epoch 56 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:40 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:40 INFO 140501882455872] Loss (name: value) total: 6.597409632470873\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:40 INFO 140501882455872] Loss (name: value) kld: 0.16280052666034964\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:40 INFO 140501882455872] Loss (name: value) recons: 6.434609088632795\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:40 INFO 140501882455872] Loss (name: value) logppx: 6.597409632470873\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:40 INFO 140501882455872] #quality_metric: host=algo-2, epoch=56, train total_loss <loss>=6.597409632470873\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:40 INFO 140501882455872] patience losses:[6.611502236790127, 6.612351271841261, 6.608671585718791, 6.602026634746128, 6.599981884161632] min patience loss:6.599981884161632 current loss:6.597409632470873 absolute loss difference:0.002572251690758698\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:40 INFO 140501882455872] Timing: train: 1.47s, val: 0.00s, epoch: 1.47s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:40 INFO 140501882455872] #progress_metric: host=algo-2, completed 56.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309079.4924939, \"EndTime\": 1646309080.9635782, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 55, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 253512.0, \"count\": 1, \"min\": 253512, \"max\": 253512}, \"Total Batches Seen\": {\"sum\": 2016.0, \"count\": 1, \"min\": 2016, \"max\": 2016}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 112.0, \"count\": 1, \"min\": 112, \"max\": 112}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:40 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=3076.924880703539 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:40 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:40 INFO 140501882455872] # Starting training for epoch 57\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:04:42.460] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 170, \"duration\": 1496, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:42 INFO 140501882455872] # Finished training epoch 57 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:42 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:42 INFO 140501882455872] Loss (name: value) total: 6.594583445125156\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:42 INFO 140501882455872] Loss (name: value) kld: 0.16631712267796198\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:42 INFO 140501882455872] Loss (name: value) recons: 6.428266313340929\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:42 INFO 140501882455872] Loss (name: value) logppx: 6.594583445125156\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:42 INFO 140501882455872] #quality_metric: host=algo-2, epoch=57, train total_loss <loss>=6.594583445125156\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:42 INFO 140501882455872] patience losses:[6.612351271841261, 6.608671585718791, 6.602026634746128, 6.599981884161632, 6.597409632470873] min patience loss:6.597409632470873 current loss:6.594583445125156 absolute loss difference:0.002826187345716491\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:42 INFO 140501882455872] Timing: train: 1.50s, val: 0.00s, epoch: 1.50s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:42 INFO 140501882455872] #progress_metric: host=algo-2, completed 57.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309080.9640226, \"EndTime\": 1646309082.4660585, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 56, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 258039.0, \"count\": 1, \"min\": 258039, \"max\": 258039}, \"Total Batches Seen\": {\"sum\": 2052.0, \"count\": 1, \"min\": 2052, \"max\": 2052}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 114.0, \"count\": 1, \"min\": 114, \"max\": 114}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:42 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=3013.3225282334124 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:42 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:42 INFO 140501882455872] # Starting training for epoch 58\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:04:44.161] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 164, \"duration\": 1573, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:44 INFO 139667671275328] # Finished training epoch 55 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:44 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:44 INFO 139667671275328] Loss (name: value) total: 6.56342265341017\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:44 INFO 139667671275328] Loss (name: value) kld: 0.16940739026500118\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:44 INFO 139667671275328] Loss (name: value) recons: 6.394015259212917\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:44 INFO 139667671275328] Loss (name: value) logppx: 6.56342265341017\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:44 INFO 139667671275328] #quality_metric: host=algo-1, epoch=55, train total_loss <loss>=6.56342265341017\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:44 INFO 139667671275328] patience losses:[6.572551793522305, 6.571708261966705, 6.564740081628163, 6.564485430717468, 6.566192156738705] min patience loss:6.564485430717468 current loss:6.56342265341017 absolute loss difference:0.0010627773072986457\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:44 INFO 139667671275328] Timing: train: 1.58s, val: 0.01s, epoch: 1.58s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:44 INFO 139667671275328] #progress_metric: host=algo-1, completed 55.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309082.5879412, \"EndTime\": 1646309084.1707819, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 54, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 248820.0, \"count\": 1, \"min\": 248820, \"max\": 248820}, \"Total Batches Seen\": {\"sum\": 1980.0, \"count\": 1, \"min\": 1980, \"max\": 1980}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 110.0, \"count\": 1, \"min\": 110, \"max\": 110}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:44 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=2856.8275273961403 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:44 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:44 INFO 139667671275328] # Starting training for epoch 56\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:04:44.070] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 173, \"duration\": 1603, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:44 INFO 140501882455872] # Finished training epoch 58 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:44 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:44 INFO 140501882455872] Loss (name: value) total: 6.599851568539937\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:44 INFO 140501882455872] Loss (name: value) kld: 0.1682033116618792\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:44 INFO 140501882455872] Loss (name: value) recons: 6.431648234526317\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:44 INFO 140501882455872] Loss (name: value) logppx: 6.599851568539937\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:44 INFO 140501882455872] #quality_metric: host=algo-2, epoch=58, train total_loss <loss>=6.599851568539937\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:44 INFO 140501882455872] patience losses:[6.608671585718791, 6.602026634746128, 6.599981884161632, 6.597409632470873, 6.594583445125156] min patience loss:6.594583445125156 current loss:6.599851568539937 absolute loss difference:0.005268123414780668\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:44 INFO 140501882455872] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:44 INFO 140501882455872] Timing: train: 1.60s, val: 0.00s, epoch: 1.61s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:44 INFO 140501882455872] #progress_metric: host=algo-2, completed 58.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309082.4665475, \"EndTime\": 1646309084.072018, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 57, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 262566.0, \"count\": 1, \"min\": 262566, \"max\": 262566}, \"Total Batches Seen\": {\"sum\": 2088.0, \"count\": 1, \"min\": 2088, \"max\": 2088}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 116.0, \"count\": 1, \"min\": 116, \"max\": 116}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:44 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=2819.3830147579615 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:44 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:44 INFO 140501882455872] # Starting training for epoch 59\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:04:45.599] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 176, \"duration\": 1526, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:45 INFO 140501882455872] # Finished training epoch 59 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:45 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:45 INFO 140501882455872] Loss (name: value) total: 6.589462849828932\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:45 INFO 140501882455872] Loss (name: value) kld: 0.16659990201393762\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:45 INFO 140501882455872] Loss (name: value) recons: 6.422862960232629\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:45 INFO 140501882455872] Loss (name: value) logppx: 6.589462849828932\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:45 INFO 140501882455872] #quality_metric: host=algo-2, epoch=59, train total_loss <loss>=6.589462849828932\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:45 INFO 140501882455872] patience losses:[6.602026634746128, 6.599981884161632, 6.597409632470873, 6.594583445125156, 6.599851568539937] min patience loss:6.594583445125156 current loss:6.589462849828932 absolute loss difference:0.0051205952962245505\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:45 INFO 140501882455872] Timing: train: 1.53s, val: 0.00s, epoch: 1.53s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:45 INFO 140501882455872] #progress_metric: host=algo-2, completed 59.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309084.0723987, \"EndTime\": 1646309085.6062784, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 58, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 267093.0, \"count\": 1, \"min\": 267093, \"max\": 267093}, \"Total Batches Seen\": {\"sum\": 2124.0, \"count\": 1, \"min\": 2124, \"max\": 2124}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 118.0, \"count\": 1, \"min\": 118, \"max\": 118}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:45 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=2951.002877705892 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:45 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:45 INFO 140501882455872] # Starting training for epoch 60\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:04:45.860] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 167, \"duration\": 1687, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:45 INFO 139667671275328] # Finished training epoch 56 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:45 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:45 INFO 139667671275328] Loss (name: value) total: 6.554678188429938\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:45 INFO 139667671275328] Loss (name: value) kld: 0.17021709297680193\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:45 INFO 139667671275328] Loss (name: value) recons: 6.384461144606273\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:45 INFO 139667671275328] Loss (name: value) logppx: 6.554678188429938\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:45 INFO 139667671275328] #quality_metric: host=algo-1, epoch=56, train total_loss <loss>=6.554678188429938\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:45 INFO 139667671275328] patience losses:[6.571708261966705, 6.564740081628163, 6.564485430717468, 6.566192156738705, 6.56342265341017] min patience loss:6.56342265341017 current loss:6.554678188429938 absolute loss difference:0.008744464980231292\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:45 INFO 139667671275328] Timing: train: 1.69s, val: 0.01s, epoch: 1.70s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:45 INFO 139667671275328] #progress_metric: host=algo-1, completed 56.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309084.1719482, \"EndTime\": 1646309085.8716528, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 55, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 253344.0, \"count\": 1, \"min\": 253344, \"max\": 253344}, \"Total Batches Seen\": {\"sum\": 2016.0, \"count\": 1, \"min\": 2016, \"max\": 2016}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 112.0, \"count\": 1, \"min\": 112, \"max\": 112}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:45 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=2660.7528587446145 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:45 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:45 INFO 139667671275328] # Starting training for epoch 57\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:04:47.345] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 170, \"duration\": 1471, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:47 INFO 139667671275328] # Finished training epoch 57 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:47 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:47 INFO 139667671275328] Loss (name: value) total: 6.551777713828617\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:47 INFO 139667671275328] Loss (name: value) kld: 0.1722815271673931\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:47 INFO 139667671275328] Loss (name: value) recons: 6.37949616379208\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:47 INFO 139667671275328] Loss (name: value) logppx: 6.551777713828617\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:47 INFO 139667671275328] #quality_metric: host=algo-1, epoch=57, train total_loss <loss>=6.551777713828617\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:47 INFO 139667671275328] patience losses:[6.564740081628163, 6.564485430717468, 6.566192156738705, 6.56342265341017, 6.554678188429938] min patience loss:6.554678188429938 current loss:6.551777713828617 absolute loss difference:0.002900474601321257\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:47 INFO 139667671275328] Timing: train: 1.48s, val: 0.01s, epoch: 1.48s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:47 INFO 139667671275328] #progress_metric: host=algo-1, completed 57.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309085.872663, \"EndTime\": 1646309087.3549855, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 56, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 257868.0, \"count\": 1, \"min\": 257868, \"max\": 257868}, \"Total Batches Seen\": {\"sum\": 2052.0, \"count\": 1, \"min\": 2052, \"max\": 2052}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 114.0, \"count\": 1, \"min\": 114, \"max\": 114}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:47 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=3051.5267814253266 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:47 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:47 INFO 139667671275328] # Starting training for epoch 58\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:04:47.136] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 179, \"duration\": 1529, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:47 INFO 140501882455872] # Finished training epoch 60 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:47 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:47 INFO 140501882455872] Loss (name: value) total: 6.588746560944451\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:47 INFO 140501882455872] Loss (name: value) kld: 0.16698474871615568\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:47 INFO 140501882455872] Loss (name: value) recons: 6.4217618107795715\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:47 INFO 140501882455872] Loss (name: value) logppx: 6.588746560944451\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:47 INFO 140501882455872] #quality_metric: host=algo-2, epoch=60, train total_loss <loss>=6.588746560944451\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:47 INFO 140501882455872] patience losses:[6.599981884161632, 6.597409632470873, 6.594583445125156, 6.599851568539937, 6.589462849828932] min patience loss:6.589462849828932 current loss:6.588746560944451 absolute loss difference:0.0007162888844804982\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:47 INFO 140501882455872] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:47 INFO 140501882455872] Timing: train: 1.53s, val: 0.01s, epoch: 1.54s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:47 INFO 140501882455872] #progress_metric: host=algo-2, completed 60.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309085.6066413, \"EndTime\": 1646309087.1432097, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 59, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 271620.0, \"count\": 1, \"min\": 271620, \"max\": 271620}, \"Total Batches Seen\": {\"sum\": 2160.0, \"count\": 1, \"min\": 2160, \"max\": 2160}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 120.0, \"count\": 1, \"min\": 120, \"max\": 120}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:47 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=2945.620063139442 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:47 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:47 INFO 140501882455872] # Starting training for epoch 61\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:04:48.947] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 173, \"duration\": 1591, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:48 INFO 139667671275328] # Finished training epoch 58 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:48 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:48 INFO 139667671275328] Loss (name: value) total: 6.553048763010237\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:48 INFO 139667671275328] Loss (name: value) kld: 0.17260699574318197\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:48 INFO 139667671275328] Loss (name: value) recons: 6.380441831217872\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:48 INFO 139667671275328] Loss (name: value) logppx: 6.553048763010237\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:48 INFO 139667671275328] #quality_metric: host=algo-1, epoch=58, train total_loss <loss>=6.553048763010237\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:48 INFO 139667671275328] patience losses:[6.564485430717468, 6.566192156738705, 6.56342265341017, 6.554678188429938, 6.551777713828617] min patience loss:6.551777713828617 current loss:6.553048763010237 absolute loss difference:0.0012710491816196878\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:48 INFO 139667671275328] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:48 INFO 139667671275328] Timing: train: 1.59s, val: 0.00s, epoch: 1.60s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:48 INFO 139667671275328] #progress_metric: host=algo-1, completed 58.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309087.3556898, \"EndTime\": 1646309088.951577, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 57, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 262392.0, \"count\": 1, \"min\": 262392, \"max\": 262392}, \"Total Batches Seen\": {\"sum\": 2088.0, \"count\": 1, \"min\": 2088, \"max\": 2088}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 116.0, \"count\": 1, \"min\": 116, \"max\": 116}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:48 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=2834.3396971809907 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:48 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:48 INFO 139667671275328] # Starting training for epoch 59\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:04:48.728] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 182, \"duration\": 1584, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:48 INFO 140501882455872] # Finished training epoch 61 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:48 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:48 INFO 140501882455872] Loss (name: value) total: 6.590031378799015\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:48 INFO 140501882455872] Loss (name: value) kld: 0.17336822549502054\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:48 INFO 140501882455872] Loss (name: value) recons: 6.416663143369886\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:48 INFO 140501882455872] Loss (name: value) logppx: 6.590031378799015\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:48 INFO 140501882455872] #quality_metric: host=algo-2, epoch=61, train total_loss <loss>=6.590031378799015\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:48 INFO 140501882455872] patience losses:[6.597409632470873, 6.594583445125156, 6.599851568539937, 6.589462849828932, 6.588746560944451] min patience loss:6.588746560944451 current loss:6.590031378799015 absolute loss difference:0.0012848178545636912\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:48 INFO 140501882455872] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:48 INFO 140501882455872] Timing: train: 1.59s, val: 0.00s, epoch: 1.59s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:48 INFO 140501882455872] #progress_metric: host=algo-2, completed 61.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309087.1437485, \"EndTime\": 1646309088.7310185, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 60, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 276147.0, \"count\": 1, \"min\": 276147, \"max\": 276147}, \"Total Batches Seen\": {\"sum\": 2196.0, \"count\": 1, \"min\": 2196, \"max\": 2196}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 122.0, \"count\": 1, \"min\": 122, \"max\": 122}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:48 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=2851.697913328129 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:48 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:48 INFO 140501882455872] # Starting training for epoch 62\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:04:50.504] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 176, \"duration\": 1551, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:50 INFO 139667671275328] # Finished training epoch 59 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:50 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:50 INFO 139667671275328] Loss (name: value) total: 6.550578044520484\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:50 INFO 139667671275328] Loss (name: value) kld: 0.1758921934912602\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:50 INFO 139667671275328] Loss (name: value) recons: 6.374685804049174\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:50 INFO 139667671275328] Loss (name: value) logppx: 6.550578044520484\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:50 INFO 139667671275328] #quality_metric: host=algo-1, epoch=59, train total_loss <loss>=6.550578044520484\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:50 INFO 139667671275328] patience losses:[6.566192156738705, 6.56342265341017, 6.554678188429938, 6.551777713828617, 6.553048763010237] min patience loss:6.551777713828617 current loss:6.550578044520484 absolute loss difference:0.001199669308133089\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:50 INFO 139667671275328] Timing: train: 1.55s, val: 0.00s, epoch: 1.56s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:50 INFO 139667671275328] #progress_metric: host=algo-1, completed 59.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309088.952098, \"EndTime\": 1646309090.509628, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 58, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 266916.0, \"count\": 1, \"min\": 266916, \"max\": 266916}, \"Total Batches Seen\": {\"sum\": 2124.0, \"count\": 1, \"min\": 2124, \"max\": 2124}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 118.0, \"count\": 1, \"min\": 118, \"max\": 118}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:50 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=2904.2078720662957 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:50 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:50 INFO 139667671275328] # Starting training for epoch 60\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:04:50.293] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 185, \"duration\": 1561, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:50 INFO 140501882455872] # Finished training epoch 62 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:50 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:50 INFO 140501882455872] Loss (name: value) total: 6.58939786752065\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:50 INFO 140501882455872] Loss (name: value) kld: 0.17304566957884365\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:50 INFO 140501882455872] Loss (name: value) recons: 6.4163521726926165\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:50 INFO 140501882455872] Loss (name: value) logppx: 6.58939786752065\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:50 INFO 140501882455872] #quality_metric: host=algo-2, epoch=62, train total_loss <loss>=6.58939786752065\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:50 INFO 140501882455872] patience losses:[6.594583445125156, 6.599851568539937, 6.589462849828932, 6.588746560944451, 6.590031378799015] min patience loss:6.588746560944451 current loss:6.58939786752065 absolute loss difference:0.000651306576198607\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:50 INFO 140501882455872] Bad epoch: loss has not improved (enough). Bad count:3\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:50 INFO 140501882455872] Timing: train: 1.56s, val: 0.00s, epoch: 1.56s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:50 INFO 140501882455872] #progress_metric: host=algo-2, completed 62.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309088.7314038, \"EndTime\": 1646309090.2950387, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 61, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 280674.0, \"count\": 1, \"min\": 280674, \"max\": 280674}, \"Total Batches Seen\": {\"sum\": 2232.0, \"count\": 1, \"min\": 2232, \"max\": 2232}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 124.0, \"count\": 1, \"min\": 124, \"max\": 124}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:50 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=2894.8156786085574 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:50 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:50 INFO 140501882455872] # Starting training for epoch 63\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:04:52.030] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 179, \"duration\": 1519, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:52 INFO 139667671275328] # Finished training epoch 60 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:52 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:52 INFO 139667671275328] Loss (name: value) total: 6.54608420530955\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:52 INFO 139667671275328] Loss (name: value) kld: 0.17803116297970215\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:52 INFO 139667671275328] Loss (name: value) recons: 6.368053052160475\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:52 INFO 139667671275328] Loss (name: value) logppx: 6.54608420530955\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:52 INFO 139667671275328] #quality_metric: host=algo-1, epoch=60, train total_loss <loss>=6.54608420530955\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:52 INFO 139667671275328] patience losses:[6.56342265341017, 6.554678188429938, 6.551777713828617, 6.553048763010237, 6.550578044520484] min patience loss:6.550578044520484 current loss:6.54608420530955 absolute loss difference:0.004493839210933714\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:52 INFO 139667671275328] Timing: train: 1.52s, val: 0.00s, epoch: 1.53s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:52 INFO 139667671275328] #progress_metric: host=algo-1, completed 60.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309090.509987, \"EndTime\": 1646309092.0356085, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 59, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 271440.0, \"count\": 1, \"min\": 271440, \"max\": 271440}, \"Total Batches Seen\": {\"sum\": 2160.0, \"count\": 1, \"min\": 2160, \"max\": 2160}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 120.0, \"count\": 1, \"min\": 120, \"max\": 120}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:52 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=2964.999669670674 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:52 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:52 INFO 139667671275328] # Starting training for epoch 61\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:04:51.773] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 188, \"duration\": 1475, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:51 INFO 140501882455872] # Finished training epoch 63 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:51 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:51 INFO 140501882455872] Loss (name: value) total: 6.586519996325175\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:51 INFO 140501882455872] Loss (name: value) kld: 0.1737794489082363\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:51 INFO 140501882455872] Loss (name: value) recons: 6.412740521960789\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:51 INFO 140501882455872] Loss (name: value) logppx: 6.586519996325175\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:51 INFO 140501882455872] #quality_metric: host=algo-2, epoch=63, train total_loss <loss>=6.586519996325175\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:51 INFO 140501882455872] patience losses:[6.599851568539937, 6.589462849828932, 6.588746560944451, 6.590031378799015, 6.58939786752065] min patience loss:6.588746560944451 current loss:6.586519996325175 absolute loss difference:0.0022265646192760613\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:51 INFO 140501882455872] Timing: train: 1.48s, val: 0.00s, epoch: 1.48s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:51 INFO 140501882455872] #progress_metric: host=algo-2, completed 63.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309090.2954085, \"EndTime\": 1646309091.7788794, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 62, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 285201.0, \"count\": 1, \"min\": 285201, \"max\": 285201}, \"Total Batches Seen\": {\"sum\": 2268.0, \"count\": 1, \"min\": 2268, \"max\": 2268}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 126.0, \"count\": 1, \"min\": 126, \"max\": 126}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:51 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=3051.2278931733576 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:51 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:51 INFO 140501882455872] # Starting training for epoch 64\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:04:53.563] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 182, \"duration\": 1526, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:53 INFO 139667671275328] # Finished training epoch 61 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:53 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:53 INFO 139667671275328] Loss (name: value) total: 6.54098516702652\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:53 INFO 139667671275328] Loss (name: value) kld: 0.17846782112287152\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:53 INFO 139667671275328] Loss (name: value) recons: 6.362517396608989\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:53 INFO 139667671275328] Loss (name: value) logppx: 6.54098516702652\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:53 INFO 139667671275328] #quality_metric: host=algo-1, epoch=61, train total_loss <loss>=6.54098516702652\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:53 INFO 139667671275328] patience losses:[6.554678188429938, 6.551777713828617, 6.553048763010237, 6.550578044520484, 6.54608420530955] min patience loss:6.54608420530955 current loss:6.54098516702652 absolute loss difference:0.005099038283030488\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:53 INFO 139667671275328] Timing: train: 1.53s, val: 0.00s, epoch: 1.53s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:53 INFO 139667671275328] #progress_metric: host=algo-1, completed 61.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309092.0359015, \"EndTime\": 1646309093.5706728, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 60, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 275964.0, \"count\": 1, \"min\": 275964, \"max\": 275964}, \"Total Batches Seen\": {\"sum\": 2196.0, \"count\": 1, \"min\": 2196, \"max\": 2196}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 122.0, \"count\": 1, \"min\": 122, \"max\": 122}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:53 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=2947.315795163521 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:53 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:53 INFO 139667671275328] # Starting training for epoch 62\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:04:55.138] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 185, \"duration\": 1567, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:55 INFO 139667671275328] # Finished training epoch 62 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:55 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:55 INFO 139667671275328] Loss (name: value) total: 6.5459147691726685\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:55 INFO 139667671275328] Loss (name: value) kld: 0.18109914474189281\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:55 INFO 139667671275328] Loss (name: value) recons: 6.3648156258794994\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:55 INFO 139667671275328] Loss (name: value) logppx: 6.5459147691726685\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:55 INFO 139667671275328] #quality_metric: host=algo-1, epoch=62, train total_loss <loss>=6.5459147691726685\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:55 INFO 139667671275328] patience losses:[6.551777713828617, 6.553048763010237, 6.550578044520484, 6.54608420530955, 6.54098516702652] min patience loss:6.54098516702652 current loss:6.5459147691726685 absolute loss difference:0.004929602146148682\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:55 INFO 139667671275328] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:55 INFO 139667671275328] Timing: train: 1.57s, val: 0.00s, epoch: 1.57s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:55 INFO 139667671275328] #progress_metric: host=algo-1, completed 62.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309093.5710251, \"EndTime\": 1646309095.1404228, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 61, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 280488.0, \"count\": 1, \"min\": 280488, \"max\": 280488}, \"Total Batches Seen\": {\"sum\": 2232.0, \"count\": 1, \"min\": 2232, \"max\": 2232}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 124.0, \"count\": 1, \"min\": 124, \"max\": 124}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:55 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=2882.3179685912164 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:55 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:55 INFO 139667671275328] # Starting training for epoch 63\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:04:56.633] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 188, \"duration\": 1491, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:56 INFO 139667671275328] # Finished training epoch 63 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:56 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:56 INFO 139667671275328] Loss (name: value) total: 6.538152588738336\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:56 INFO 139667671275328] Loss (name: value) kld: 0.1825095350957579\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:56 INFO 139667671275328] Loss (name: value) recons: 6.355643053849538\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:56 INFO 139667671275328] Loss (name: value) logppx: 6.538152588738336\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:56 INFO 139667671275328] #quality_metric: host=algo-1, epoch=63, train total_loss <loss>=6.538152588738336\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:56 INFO 139667671275328] patience losses:[6.553048763010237, 6.550578044520484, 6.54608420530955, 6.54098516702652, 6.5459147691726685] min patience loss:6.54098516702652 current loss:6.538152588738336 absolute loss difference:0.0028325782881841732\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:56 INFO 139667671275328] Timing: train: 1.49s, val: 0.00s, epoch: 1.50s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:56 INFO 139667671275328] #progress_metric: host=algo-1, completed 63.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309095.140737, \"EndTime\": 1646309096.6385741, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 62, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 285012.0, \"count\": 1, \"min\": 285012, \"max\": 285012}, \"Total Batches Seen\": {\"sum\": 2268.0, \"count\": 1, \"min\": 2268, \"max\": 2268}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 126.0, \"count\": 1, \"min\": 126, \"max\": 126}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:56 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=3018.979412583149 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:56 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:56 INFO 139667671275328] # Starting training for epoch 64\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:04:53.240] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 191, \"duration\": 1461, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:53 INFO 140501882455872] # Finished training epoch 64 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:53 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:53 INFO 140501882455872] Loss (name: value) total: 6.575486878554027\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:53 INFO 140501882455872] Loss (name: value) kld: 0.17502397402293152\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:53 INFO 140501882455872] Loss (name: value) recons: 6.400462912188636\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:53 INFO 140501882455872] Loss (name: value) logppx: 6.575486878554027\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:53 INFO 140501882455872] #quality_metric: host=algo-2, epoch=64, train total_loss <loss>=6.575486878554027\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:53 INFO 140501882455872] patience losses:[6.589462849828932, 6.588746560944451, 6.590031378799015, 6.58939786752065, 6.586519996325175] min patience loss:6.586519996325175 current loss:6.575486878554027 absolute loss difference:0.011033117771148682\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:53 INFO 140501882455872] Timing: train: 1.46s, val: 0.00s, epoch: 1.47s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:53 INFO 140501882455872] #progress_metric: host=algo-2, completed 64.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309091.7792513, \"EndTime\": 1646309093.2473135, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 63, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 289728.0, \"count\": 1, \"min\": 289728, \"max\": 289728}, \"Total Batches Seen\": {\"sum\": 2304.0, \"count\": 1, \"min\": 2304, \"max\": 2304}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 128.0, \"count\": 1, \"min\": 128, \"max\": 128}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:53 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=3083.252297750015 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:53 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:53 INFO 140501882455872] # Starting training for epoch 65\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:04:54.791] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 194, \"duration\": 1543, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:54 INFO 140501882455872] # Finished training epoch 65 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:54 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:54 INFO 140501882455872] Loss (name: value) total: 6.574808524714576\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:54 INFO 140501882455872] Loss (name: value) kld: 0.17864568241768414\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:54 INFO 140501882455872] Loss (name: value) recons: 6.396162847677867\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:54 INFO 140501882455872] Loss (name: value) logppx: 6.574808524714576\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:54 INFO 140501882455872] #quality_metric: host=algo-2, epoch=65, train total_loss <loss>=6.574808524714576\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:54 INFO 140501882455872] patience losses:[6.588746560944451, 6.590031378799015, 6.58939786752065, 6.586519996325175, 6.575486878554027] min patience loss:6.575486878554027 current loss:6.574808524714576 absolute loss difference:0.0006783538394508071\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:54 INFO 140501882455872] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:54 INFO 140501882455872] Timing: train: 1.54s, val: 0.00s, epoch: 1.55s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:54 INFO 140501882455872] #progress_metric: host=algo-2, completed 65.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309093.247683, \"EndTime\": 1646309094.7961702, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 64, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 294255.0, \"count\": 1, \"min\": 294255, \"max\": 294255}, \"Total Batches Seen\": {\"sum\": 2340.0, \"count\": 1, \"min\": 2340, \"max\": 2340}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 130.0, \"count\": 1, \"min\": 130, \"max\": 130}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:54 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=2923.1690653828355 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:54 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:54 INFO 140501882455872] # Starting training for epoch 66\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:04:56.376] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 197, \"duration\": 1579, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:56 INFO 140501882455872] # Finished training epoch 66 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:56 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:56 INFO 140501882455872] Loss (name: value) total: 6.579525874720679\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:56 INFO 140501882455872] Loss (name: value) kld: 0.17788875020212597\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:56 INFO 140501882455872] Loss (name: value) recons: 6.4016371501816645\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:56 INFO 140501882455872] Loss (name: value) logppx: 6.579525874720679\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:56 INFO 140501882455872] #quality_metric: host=algo-2, epoch=66, train total_loss <loss>=6.579525874720679\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:56 INFO 140501882455872] patience losses:[6.590031378799015, 6.58939786752065, 6.586519996325175, 6.575486878554027, 6.574808524714576] min patience loss:6.574808524714576 current loss:6.579525874720679 absolute loss difference:0.004717350006103516\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:56 INFO 140501882455872] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:56 INFO 140501882455872] Timing: train: 1.58s, val: 0.00s, epoch: 1.58s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:56 INFO 140501882455872] #progress_metric: host=algo-2, completed 66.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309094.7965307, \"EndTime\": 1646309096.37926, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 65, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 298782.0, \"count\": 1, \"min\": 298782, \"max\": 298782}, \"Total Batches Seen\": {\"sum\": 2376.0, \"count\": 1, \"min\": 2376, \"max\": 2376}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 132.0, \"count\": 1, \"min\": 132, \"max\": 132}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:56 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=2859.886684854357 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:56 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:56 INFO 140501882455872] # Starting training for epoch 67\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:04:58.170] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 191, \"duration\": 1530, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:58 INFO 139667671275328] # Finished training epoch 64 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:58 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:58 INFO 139667671275328] Loss (name: value) total: 6.532335586018032\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:58 INFO 139667671275328] Loss (name: value) kld: 0.18496369384229183\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:58 INFO 139667671275328] Loss (name: value) recons: 6.34737190273073\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:58 INFO 139667671275328] Loss (name: value) logppx: 6.532335586018032\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:58 INFO 139667671275328] #quality_metric: host=algo-1, epoch=64, train total_loss <loss>=6.532335586018032\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:58 INFO 139667671275328] patience losses:[6.550578044520484, 6.54608420530955, 6.54098516702652, 6.5459147691726685, 6.538152588738336] min patience loss:6.538152588738336 current loss:6.532335586018032 absolute loss difference:0.005817002720303499\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:58 INFO 139667671275328] Timing: train: 1.53s, val: 0.01s, epoch: 1.54s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:58 INFO 139667671275328] #progress_metric: host=algo-1, completed 64.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309096.6394775, \"EndTime\": 1646309098.1782715, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 63, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 289536.0, \"count\": 1, \"min\": 289536, \"max\": 289536}, \"Total Batches Seen\": {\"sum\": 2304.0, \"count\": 1, \"min\": 2304, \"max\": 2304}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 128.0, \"count\": 1, \"min\": 128, \"max\": 128}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:58 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=2939.563342876764 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:58 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:58 INFO 139667671275328] # Starting training for epoch 65\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:04:57.898] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 200, \"duration\": 1518, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:57 INFO 140501882455872] # Finished training epoch 67 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:57 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:57 INFO 140501882455872] Loss (name: value) total: 6.577978531519572\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:57 INFO 140501882455872] Loss (name: value) kld: 0.18203511689272192\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:57 INFO 140501882455872] Loss (name: value) recons: 6.3959434032440186\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:57 INFO 140501882455872] Loss (name: value) logppx: 6.577978531519572\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:57 INFO 140501882455872] #quality_metric: host=algo-2, epoch=67, train total_loss <loss>=6.577978531519572\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:57 INFO 140501882455872] patience losses:[6.58939786752065, 6.586519996325175, 6.575486878554027, 6.574808524714576, 6.579525874720679] min patience loss:6.574808524714576 current loss:6.577978531519572 absolute loss difference:0.0031700068049964614\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:57 INFO 140501882455872] Bad epoch: loss has not improved (enough). Bad count:3\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:57 INFO 140501882455872] Timing: train: 1.52s, val: 0.00s, epoch: 1.52s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:57 INFO 140501882455872] #progress_metric: host=algo-2, completed 67.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309096.3796432, \"EndTime\": 1646309097.9001758, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 66, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 303309.0, \"count\": 1, \"min\": 303309, \"max\": 303309}, \"Total Batches Seen\": {\"sum\": 2412.0, \"count\": 1, \"min\": 2412, \"max\": 2412}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 134.0, \"count\": 1, \"min\": 134, \"max\": 134}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:57 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=2976.875162639489 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:57 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:57 INFO 140501882455872] # Starting training for epoch 68\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:04:59.497] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 203, \"duration\": 1595, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:59 INFO 140501882455872] # Finished training epoch 68 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:59 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:59 INFO 140501882455872] Loss (name: value) total: 6.578451216220856\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:59 INFO 140501882455872] Loss (name: value) kld: 0.1840050369501114\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:59 INFO 140501882455872] Loss (name: value) recons: 6.394446180926429\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:59 INFO 140501882455872] Loss (name: value) logppx: 6.578451216220856\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:59 INFO 140501882455872] #quality_metric: host=algo-2, epoch=68, train total_loss <loss>=6.578451216220856\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:59 INFO 140501882455872] patience losses:[6.586519996325175, 6.575486878554027, 6.574808524714576, 6.579525874720679, 6.577978531519572] min patience loss:6.574808524714576 current loss:6.578451216220856 absolute loss difference:0.003642691506279938\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:59 INFO 140501882455872] Bad epoch: loss has not improved (enough). Bad count:4\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:59 INFO 140501882455872] Timing: train: 1.60s, val: 0.00s, epoch: 1.60s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:59 INFO 140501882455872] #progress_metric: host=algo-2, completed 68.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309097.900824, \"EndTime\": 1646309099.5009263, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 67, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 307836.0, \"count\": 1, \"min\": 307836, \"max\": 307836}, \"Total Batches Seen\": {\"sum\": 2448.0, \"count\": 1, \"min\": 2448, \"max\": 2448}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 136.0, \"count\": 1, \"min\": 136, \"max\": 136}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:59 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=2828.4007425373125 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:59 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:04:59 INFO 140501882455872] # Starting training for epoch 69\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:04:59.718] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 194, \"duration\": 1536, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:59 INFO 139667671275328] # Finished training epoch 65 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:59 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:59 INFO 139667671275328] Loss (name: value) total: 6.537404146459368\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:59 INFO 139667671275328] Loss (name: value) kld: 0.18780342116951942\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:59 INFO 139667671275328] Loss (name: value) recons: 6.3496006859673395\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:59 INFO 139667671275328] Loss (name: value) logppx: 6.537404146459368\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:59 INFO 139667671275328] #quality_metric: host=algo-1, epoch=65, train total_loss <loss>=6.537404146459368\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:59 INFO 139667671275328] patience losses:[6.54608420530955, 6.54098516702652, 6.5459147691726685, 6.538152588738336, 6.532335586018032] min patience loss:6.532335586018032 current loss:6.537404146459368 absolute loss difference:0.0050685604413356344\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:59 INFO 139667671275328] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:59 INFO 139667671275328] Timing: train: 1.54s, val: 0.00s, epoch: 1.54s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:59 INFO 139667671275328] #progress_metric: host=algo-1, completed 65.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309098.178668, \"EndTime\": 1646309099.72143, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 64, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 294060.0, \"count\": 1, \"min\": 294060, \"max\": 294060}, \"Total Batches Seen\": {\"sum\": 2340.0, \"count\": 1, \"min\": 2340, \"max\": 2340}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 130.0, \"count\": 1, \"min\": 130, \"max\": 130}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:59 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=2932.0586039900963 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:59 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:04:59 INFO 139667671275328] # Starting training for epoch 66\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:05:01.298] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 197, \"duration\": 1576, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:01 INFO 139667671275328] # Finished training epoch 66 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:01 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:01 INFO 139667671275328] Loss (name: value) total: 6.528410024113125\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:01 INFO 139667671275328] Loss (name: value) kld: 0.18914532557957703\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:01 INFO 139667671275328] Loss (name: value) recons: 6.339264664385054\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:01 INFO 139667671275328] Loss (name: value) logppx: 6.528410024113125\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:01 INFO 139667671275328] #quality_metric: host=algo-1, epoch=66, train total_loss <loss>=6.528410024113125\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:01 INFO 139667671275328] patience losses:[6.54098516702652, 6.5459147691726685, 6.538152588738336, 6.532335586018032, 6.537404146459368] min patience loss:6.532335586018032 current loss:6.528410024113125 absolute loss difference:0.0039255619049072266\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:01 INFO 139667671275328] Timing: train: 1.58s, val: 0.00s, epoch: 1.58s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:01 INFO 139667671275328] #progress_metric: host=algo-1, completed 66.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309099.7217634, \"EndTime\": 1646309101.305349, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 65, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 298584.0, \"count\": 1, \"min\": 298584, \"max\": 298584}, \"Total Batches Seen\": {\"sum\": 2376.0, \"count\": 1, \"min\": 2376, \"max\": 2376}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 132.0, \"count\": 1, \"min\": 132, \"max\": 132}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:01 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=2856.381136806205 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:01 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:01 INFO 139667671275328] # Starting training for epoch 67\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:05:03.228] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 200, \"duration\": 1916, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:03 INFO 139667671275328] # Finished training epoch 67 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:03 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:03 INFO 139667671275328] Loss (name: value) total: 6.527373108598921\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:03 INFO 139667671275328] Loss (name: value) kld: 0.1923037247939242\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:03 INFO 139667671275328] Loss (name: value) recons: 6.335069411330753\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:03 INFO 139667671275328] Loss (name: value) logppx: 6.527373108598921\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:03 INFO 139667671275328] #quality_metric: host=algo-1, epoch=67, train total_loss <loss>=6.527373108598921\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:03 INFO 139667671275328] patience losses:[6.5459147691726685, 6.538152588738336, 6.532335586018032, 6.537404146459368, 6.528410024113125] min patience loss:6.528410024113125 current loss:6.527373108598921 absolute loss difference:0.0010369155142040398\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:03 INFO 139667671275328] Timing: train: 1.92s, val: 0.00s, epoch: 1.93s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:03 INFO 139667671275328] #progress_metric: host=algo-1, completed 67.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309101.3058038, \"EndTime\": 1646309103.2344484, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 66, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 303108.0, \"count\": 1, \"min\": 303108, \"max\": 303108}, \"Total Batches Seen\": {\"sum\": 2412.0, \"count\": 1, \"min\": 2412, \"max\": 2412}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 134.0, \"count\": 1, \"min\": 134, \"max\": 134}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:03 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=2345.2031061190605 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:03 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:03 INFO 139667671275328] # Starting training for epoch 68\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:05:01.118] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 206, \"duration\": 1616, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:01 INFO 140501882455872] # Finished training epoch 69 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:01 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:01 INFO 140501882455872] Loss (name: value) total: 6.571760694185893\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:01 INFO 140501882455872] Loss (name: value) kld: 0.18536008749571112\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:01 INFO 140501882455872] Loss (name: value) recons: 6.3864006996154785\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:01 INFO 140501882455872] Loss (name: value) logppx: 6.571760694185893\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:01 INFO 140501882455872] #quality_metric: host=algo-2, epoch=69, train total_loss <loss>=6.571760694185893\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:01 INFO 140501882455872] patience losses:[6.575486878554027, 6.574808524714576, 6.579525874720679, 6.577978531519572, 6.578451216220856] min patience loss:6.574808524714576 current loss:6.571760694185893 absolute loss difference:0.003047830528682738\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:01 INFO 140501882455872] Timing: train: 1.62s, val: 0.00s, epoch: 1.62s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:01 INFO 140501882455872] #progress_metric: host=algo-2, completed 69.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309099.5017228, \"EndTime\": 1646309101.1245253, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 68, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 312363.0, \"count\": 1, \"min\": 312363, \"max\": 312363}, \"Total Batches Seen\": {\"sum\": 2484.0, \"count\": 1, \"min\": 2484, \"max\": 2484}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 138.0, \"count\": 1, \"min\": 138, \"max\": 138}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:01 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=2789.1817574875586 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:01 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:01 INFO 140501882455872] # Starting training for epoch 70\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:05:02.881] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 209, \"duration\": 1755, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:02 INFO 140501882455872] # Finished training epoch 70 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:02 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:02 INFO 140501882455872] Loss (name: value) total: 6.569517029656304\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:02 INFO 140501882455872] Loss (name: value) kld: 0.18754884786903858\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:02 INFO 140501882455872] Loss (name: value) recons: 6.3819681604703264\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:02 INFO 140501882455872] Loss (name: value) logppx: 6.569517029656304\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:02 INFO 140501882455872] #quality_metric: host=algo-2, epoch=70, train total_loss <loss>=6.569517029656304\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:02 INFO 140501882455872] patience losses:[6.574808524714576, 6.579525874720679, 6.577978531519572, 6.578451216220856, 6.571760694185893] min patience loss:6.571760694185893 current loss:6.569517029656304 absolute loss difference:0.002243664529588685\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:02 INFO 140501882455872] Timing: train: 1.76s, val: 0.00s, epoch: 1.76s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:02 INFO 140501882455872] #progress_metric: host=algo-2, completed 70.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309101.1250472, \"EndTime\": 1646309102.8884068, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 69, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 316890.0, \"count\": 1, \"min\": 316890, \"max\": 316890}, \"Total Batches Seen\": {\"sum\": 2520.0, \"count\": 1, \"min\": 2520, \"max\": 2520}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 140.0, \"count\": 1, \"min\": 140, \"max\": 140}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:02 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=2566.9816570914377 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:02 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:02 INFO 140501882455872] # Starting training for epoch 71\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:05:04.500] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 212, \"duration\": 1611, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:04 INFO 140501882455872] # Finished training epoch 71 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:04 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:04 INFO 140501882455872] Loss (name: value) total: 6.562816123167674\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:04 INFO 140501882455872] Loss (name: value) kld: 0.18750276809765232\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:04 INFO 140501882455872] Loss (name: value) recons: 6.375313447581397\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:04 INFO 140501882455872] Loss (name: value) logppx: 6.562816123167674\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:04 INFO 140501882455872] #quality_metric: host=algo-2, epoch=71, train total_loss <loss>=6.562816123167674\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:04 INFO 140501882455872] patience losses:[6.579525874720679, 6.577978531519572, 6.578451216220856, 6.571760694185893, 6.569517029656304] min patience loss:6.569517029656304 current loss:6.562816123167674 absolute loss difference:0.006700906488630309\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:04 INFO 140501882455872] Timing: train: 1.61s, val: 0.00s, epoch: 1.62s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:04 INFO 140501882455872] #progress_metric: host=algo-2, completed 71.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309102.8887756, \"EndTime\": 1646309104.5076747, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 70, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 321417.0, \"count\": 1, \"min\": 321417, \"max\": 321417}, \"Total Batches Seen\": {\"sum\": 2556.0, \"count\": 1, \"min\": 2556, \"max\": 2556}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 142.0, \"count\": 1, \"min\": 142, \"max\": 142}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:04 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=2796.000496540196 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:04 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:04 INFO 140501882455872] # Starting training for epoch 72\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:05:05.051] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 203, \"duration\": 1814, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:05 INFO 139667671275328] # Finished training epoch 68 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:05 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:05 INFO 139667671275328] Loss (name: value) total: 6.528285629219479\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:05 INFO 139667671275328] Loss (name: value) kld: 0.19226234820153978\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:05 INFO 139667671275328] Loss (name: value) recons: 6.336023304197523\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:05 INFO 139667671275328] Loss (name: value) logppx: 6.528285629219479\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:05 INFO 139667671275328] #quality_metric: host=algo-1, epoch=68, train total_loss <loss>=6.528285629219479\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:05 INFO 139667671275328] patience losses:[6.538152588738336, 6.532335586018032, 6.537404146459368, 6.528410024113125, 6.527373108598921] min patience loss:6.527373108598921 current loss:6.528285629219479 absolute loss difference:0.0009125206205577996\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:05 INFO 139667671275328] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:05 INFO 139667671275328] Timing: train: 1.82s, val: 0.00s, epoch: 1.82s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:05 INFO 139667671275328] #progress_metric: host=algo-1, completed 68.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309103.2353525, \"EndTime\": 1646309105.053064, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 67, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 307632.0, \"count\": 1, \"min\": 307632, \"max\": 307632}, \"Total Batches Seen\": {\"sum\": 2448.0, \"count\": 1, \"min\": 2448, \"max\": 2448}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 136.0, \"count\": 1, \"min\": 136, \"max\": 136}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:05 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=2488.5300000157376 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:05 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:05 INFO 139667671275328] # Starting training for epoch 69\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:05:06.348] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 215, \"duration\": 1836, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:06 INFO 140501882455872] # Finished training epoch 72 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:06 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:06 INFO 140501882455872] Loss (name: value) total: 6.559296256966061\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:06 INFO 140501882455872] Loss (name: value) kld: 0.1887037002791961\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:06 INFO 140501882455872] Loss (name: value) recons: 6.370592527919346\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:06 INFO 140501882455872] Loss (name: value) logppx: 6.559296256966061\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:06 INFO 140501882455872] #quality_metric: host=algo-2, epoch=72, train total_loss <loss>=6.559296256966061\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:06 INFO 140501882455872] patience losses:[6.577978531519572, 6.578451216220856, 6.571760694185893, 6.569517029656304, 6.562816123167674] min patience loss:6.562816123167674 current loss:6.559296256966061 absolute loss difference:0.0035198662016133753\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:06 INFO 140501882455872] Timing: train: 1.84s, val: 0.00s, epoch: 1.84s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:06 INFO 140501882455872] #progress_metric: host=algo-2, completed 72.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309104.5087001, \"EndTime\": 1646309106.3562927, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 71, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 325944.0, \"count\": 1, \"min\": 325944, \"max\": 325944}, \"Total Batches Seen\": {\"sum\": 2592.0, \"count\": 1, \"min\": 2592, \"max\": 2592}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 144.0, \"count\": 1, \"min\": 144, \"max\": 144}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:06 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=2449.9903946760787 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:06 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:06 INFO 140501882455872] # Starting training for epoch 73\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:05:07.007] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 206, \"duration\": 1953, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:07 INFO 139667671275328] # Finished training epoch 69 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:07 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:07 INFO 139667671275328] Loss (name: value) total: 6.521621624628703\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:07 INFO 139667671275328] Loss (name: value) kld: 0.19256873283949164\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:07 INFO 139667671275328] Loss (name: value) recons: 6.329052872127956\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:07 INFO 139667671275328] Loss (name: value) logppx: 6.521621624628703\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:07 INFO 139667671275328] #quality_metric: host=algo-1, epoch=69, train total_loss <loss>=6.521621624628703\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:07 INFO 139667671275328] patience losses:[6.532335586018032, 6.537404146459368, 6.528410024113125, 6.527373108598921, 6.528285629219479] min patience loss:6.527373108598921 current loss:6.521621624628703 absolute loss difference:0.005751483970217741\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:07 INFO 139667671275328] Timing: train: 1.96s, val: 0.01s, epoch: 1.96s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:07 INFO 139667671275328] #progress_metric: host=algo-1, completed 69.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309105.0535164, \"EndTime\": 1646309107.016933, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 68, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 312156.0, \"count\": 1, \"min\": 312156, \"max\": 312156}, \"Total Batches Seen\": {\"sum\": 2484.0, \"count\": 1, \"min\": 2484, \"max\": 2484}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 138.0, \"count\": 1, \"min\": 138, \"max\": 138}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:07 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=2303.8910860506453 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:07 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:07 INFO 139667671275328] # Starting training for epoch 70\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:05:08.166] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 218, \"duration\": 1809, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:08 INFO 140501882455872] # Finished training epoch 73 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:08 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:08 INFO 140501882455872] Loss (name: value) total: 6.560993472735087\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:08 INFO 140501882455872] Loss (name: value) kld: 0.19258027192619112\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:08 INFO 140501882455872] Loss (name: value) recons: 6.3684131900469465\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:08 INFO 140501882455872] Loss (name: value) logppx: 6.560993472735087\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:08 INFO 140501882455872] #quality_metric: host=algo-2, epoch=73, train total_loss <loss>=6.560993472735087\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:08 INFO 140501882455872] patience losses:[6.578451216220856, 6.571760694185893, 6.569517029656304, 6.562816123167674, 6.559296256966061] min patience loss:6.559296256966061 current loss:6.560993472735087 absolute loss difference:0.0016972157690267053\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:08 INFO 140501882455872] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:08 INFO 140501882455872] Timing: train: 1.81s, val: 0.00s, epoch: 1.81s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:08 INFO 140501882455872] #progress_metric: host=algo-2, completed 73.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309106.3565905, \"EndTime\": 1646309108.1683419, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 72, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 330471.0, \"count\": 1, \"min\": 330471, \"max\": 330471}, \"Total Batches Seen\": {\"sum\": 2628.0, \"count\": 1, \"min\": 2628, \"max\": 2628}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 146.0, \"count\": 1, \"min\": 146, \"max\": 146}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:08 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=2498.4245326378796 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:08 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:08 INFO 140501882455872] # Starting training for epoch 74\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:05:08.774] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 209, \"duration\": 1756, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:08 INFO 139667671275328] # Finished training epoch 70 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:08 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:08 INFO 139667671275328] Loss (name: value) total: 6.52058302031623\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:08 INFO 139667671275328] Loss (name: value) kld: 0.19687501113447878\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:08 INFO 139667671275328] Loss (name: value) recons: 6.323708030912611\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:08 INFO 139667671275328] Loss (name: value) logppx: 6.52058302031623\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:08 INFO 139667671275328] #quality_metric: host=algo-1, epoch=70, train total_loss <loss>=6.52058302031623\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:08 INFO 139667671275328] patience losses:[6.537404146459368, 6.528410024113125, 6.527373108598921, 6.528285629219479, 6.521621624628703] min patience loss:6.521621624628703 current loss:6.52058302031623 absolute loss difference:0.001038604312473268\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:08 INFO 139667671275328] Timing: train: 1.76s, val: 0.01s, epoch: 1.77s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:08 INFO 139667671275328] #progress_metric: host=algo-1, completed 70.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309107.017354, \"EndTime\": 1646309108.785084, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 69, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 316680.0, \"count\": 1, \"min\": 316680, \"max\": 316680}, \"Total Batches Seen\": {\"sum\": 2520.0, \"count\": 1, \"min\": 2520, \"max\": 2520}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 140.0, \"count\": 1, \"min\": 140, \"max\": 140}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:08 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=2558.080297839595 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:08 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:08 INFO 139667671275328] # Starting training for epoch 71\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:05:10.386] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 212, \"duration\": 1598, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:10 INFO 139667671275328] # Finished training epoch 71 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:10 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:10 INFO 139667671275328] Loss (name: value) total: 6.518545733557807\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:10 INFO 139667671275328] Loss (name: value) kld: 0.197626282978389\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:10 INFO 139667671275328] Loss (name: value) recons: 6.320919440852271\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:10 INFO 139667671275328] Loss (name: value) logppx: 6.518545733557807\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:10 INFO 139667671275328] #quality_metric: host=algo-1, epoch=71, train total_loss <loss>=6.518545733557807\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:10 INFO 139667671275328] patience losses:[6.528410024113125, 6.527373108598921, 6.528285629219479, 6.521621624628703, 6.52058302031623] min patience loss:6.52058302031623 current loss:6.518545733557807 absolute loss difference:0.0020372867584228516\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:10 INFO 139667671275328] Timing: train: 1.60s, val: 0.00s, epoch: 1.61s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:10 INFO 139667671275328] #progress_metric: host=algo-1, completed 71.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309108.7865863, \"EndTime\": 1646309110.3933613, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 70, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 321204.0, \"count\": 1, \"min\": 321204, \"max\": 321204}, \"Total Batches Seen\": {\"sum\": 2556.0, \"count\": 1, \"min\": 2556, \"max\": 2556}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 142.0, \"count\": 1, \"min\": 142, \"max\": 142}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:10 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=2815.160820859484 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:10 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:10 INFO 139667671275328] # Starting training for epoch 72\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:05:09.800] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 221, \"duration\": 1631, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:09 INFO 140501882455872] # Finished training epoch 74 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:09 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:09 INFO 140501882455872] Loss (name: value) total: 6.559698250558641\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:09 INFO 140501882455872] Loss (name: value) kld: 0.1945910973267423\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:09 INFO 140501882455872] Loss (name: value) recons: 6.365107165442573\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:09 INFO 140501882455872] Loss (name: value) logppx: 6.559698250558641\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:09 INFO 140501882455872] #quality_metric: host=algo-2, epoch=74, train total_loss <loss>=6.559698250558641\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:09 INFO 140501882455872] patience losses:[6.571760694185893, 6.569517029656304, 6.562816123167674, 6.559296256966061, 6.560993472735087] min patience loss:6.559296256966061 current loss:6.559698250558641 absolute loss difference:0.0004019935925807516\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:09 INFO 140501882455872] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:09 INFO 140501882455872] Timing: train: 1.63s, val: 0.00s, epoch: 1.63s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:09 INFO 140501882455872] #progress_metric: host=algo-2, completed 74.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309108.1687775, \"EndTime\": 1646309109.802008, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 73, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 334998.0, \"count\": 1, \"min\": 334998, \"max\": 334998}, \"Total Batches Seen\": {\"sum\": 2664.0, \"count\": 1, \"min\": 2664, \"max\": 2664}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 148.0, \"count\": 1, \"min\": 148, \"max\": 148}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:09 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=2771.477520040636 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:09 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:09 INFO 140501882455872] # Starting training for epoch 75\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:05:11.295] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 224, \"duration\": 1492, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:11 INFO 140501882455872] # Finished training epoch 75 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:11 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:11 INFO 140501882455872] Loss (name: value) total: 6.564304755793677\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:11 INFO 140501882455872] Loss (name: value) kld: 0.1948320637974474\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:11 INFO 140501882455872] Loss (name: value) recons: 6.369472702344258\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:11 INFO 140501882455872] Loss (name: value) logppx: 6.564304755793677\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:11 INFO 140501882455872] #quality_metric: host=algo-2, epoch=75, train total_loss <loss>=6.564304755793677\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:11 INFO 140501882455872] patience losses:[6.569517029656304, 6.562816123167674, 6.559296256966061, 6.560993472735087, 6.559698250558641] min patience loss:6.559296256966061 current loss:6.564304755793677 absolute loss difference:0.00500849882761667\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:11 INFO 140501882455872] Bad epoch: loss has not improved (enough). Bad count:3\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:11 INFO 140501882455872] Timing: train: 1.50s, val: 0.00s, epoch: 1.50s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:11 INFO 140501882455872] #progress_metric: host=algo-2, completed 75.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309109.8023732, \"EndTime\": 1646309111.2991905, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 74, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 339525.0, \"count\": 1, \"min\": 339525, \"max\": 339525}, \"Total Batches Seen\": {\"sum\": 2700.0, \"count\": 1, \"min\": 2700, \"max\": 2700}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 150.0, \"count\": 1, \"min\": 150, \"max\": 150}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:11 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=3023.437082792077 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:11 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:11 INFO 140501882455872] # Starting training for epoch 76\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:05:11.895] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 215, \"duration\": 1499, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:11 INFO 139667671275328] # Finished training epoch 72 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:11 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:11 INFO 139667671275328] Loss (name: value) total: 6.514567746056451\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:11 INFO 139667671275328] Loss (name: value) kld: 0.19814756471249792\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:11 INFO 139667671275328] Loss (name: value) recons: 6.316420151127709\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:11 INFO 139667671275328] Loss (name: value) logppx: 6.514567746056451\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:11 INFO 139667671275328] #quality_metric: host=algo-1, epoch=72, train total_loss <loss>=6.514567746056451\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:11 INFO 139667671275328] patience losses:[6.527373108598921, 6.528285629219479, 6.521621624628703, 6.52058302031623, 6.518545733557807] min patience loss:6.518545733557807 current loss:6.514567746056451 absolute loss difference:0.003977987501356139\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:11 INFO 139667671275328] Timing: train: 1.50s, val: 0.00s, epoch: 1.51s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:11 INFO 139667671275328] #progress_metric: host=algo-1, completed 72.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309110.3937464, \"EndTime\": 1646309111.9014452, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 71, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 325728.0, \"count\": 1, \"min\": 325728, \"max\": 325728}, \"Total Batches Seen\": {\"sum\": 2592.0, \"count\": 1, \"min\": 2592, \"max\": 2592}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 144.0, \"count\": 1, \"min\": 144, \"max\": 144}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:11 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=3000.2013247981763 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:11 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:11 INFO 139667671275328] # Starting training for epoch 73\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:05:12.856] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 227, \"duration\": 1556, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:12 INFO 140501882455872] # Finished training epoch 76 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:12 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:12 INFO 140501882455872] Loss (name: value) total: 6.55329344007704\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:12 INFO 140501882455872] Loss (name: value) kld: 0.19854252951012719\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:12 INFO 140501882455872] Loss (name: value) recons: 6.354750898149279\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:12 INFO 140501882455872] Loss (name: value) logppx: 6.55329344007704\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:12 INFO 140501882455872] #quality_metric: host=algo-2, epoch=76, train total_loss <loss>=6.55329344007704\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:12 INFO 140501882455872] patience losses:[6.562816123167674, 6.559296256966061, 6.560993472735087, 6.559698250558641, 6.564304755793677] min patience loss:6.559296256966061 current loss:6.55329344007704 absolute loss difference:0.006002816889020934\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:12 INFO 140501882455872] Timing: train: 1.56s, val: 0.01s, epoch: 1.56s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:12 INFO 140501882455872] #progress_metric: host=algo-2, completed 76.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309111.3000896, \"EndTime\": 1646309112.8643692, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 75, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 344052.0, \"count\": 1, \"min\": 344052, \"max\": 344052}, \"Total Batches Seen\": {\"sum\": 2736.0, \"count\": 1, \"min\": 2736, \"max\": 2736}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 152.0, \"count\": 1, \"min\": 152, \"max\": 152}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:12 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=2893.5945700262437 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:12 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:12 INFO 140501882455872] # Starting training for epoch 77\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:05:13.500] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 218, \"duration\": 1597, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:13 INFO 139667671275328] # Finished training epoch 73 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:13 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:13 INFO 139667671275328] Loss (name: value) total: 6.512685206201342\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:13 INFO 139667671275328] Loss (name: value) kld: 0.2019771732803848\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:13 INFO 139667671275328] Loss (name: value) recons: 6.310707992977566\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:13 INFO 139667671275328] Loss (name: value) logppx: 6.512685206201342\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:13 INFO 139667671275328] #quality_metric: host=algo-1, epoch=73, train total_loss <loss>=6.512685206201342\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:13 INFO 139667671275328] patience losses:[6.528285629219479, 6.521621624628703, 6.52058302031623, 6.518545733557807, 6.514567746056451] min patience loss:6.514567746056451 current loss:6.512685206201342 absolute loss difference:0.001882539855109222\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:13 INFO 139667671275328] Timing: train: 1.60s, val: 0.00s, epoch: 1.60s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:13 INFO 139667671275328] #progress_metric: host=algo-1, completed 73.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309111.9018016, \"EndTime\": 1646309113.5060859, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 72, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 330252.0, \"count\": 1, \"min\": 330252, \"max\": 330252}, \"Total Batches Seen\": {\"sum\": 2628.0, \"count\": 1, \"min\": 2628, \"max\": 2628}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 146.0, \"count\": 1, \"min\": 146, \"max\": 146}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:13 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=2819.584107349377 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:13 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:13 INFO 139667671275328] # Starting training for epoch 74\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:05:14.429] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 230, \"duration\": 1563, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:14 INFO 140501882455872] # Finished training epoch 77 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:14 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:14 INFO 140501882455872] Loss (name: value) total: 6.555045485496521\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:14 INFO 140501882455872] Loss (name: value) kld: 0.20140427433782154\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:14 INFO 140501882455872] Loss (name: value) recons: 6.353641264968449\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:14 INFO 140501882455872] Loss (name: value) logppx: 6.555045485496521\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:14 INFO 140501882455872] #quality_metric: host=algo-2, epoch=77, train total_loss <loss>=6.555045485496521\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:14 INFO 140501882455872] patience losses:[6.559296256966061, 6.560993472735087, 6.559698250558641, 6.564304755793677, 6.55329344007704] min patience loss:6.55329344007704 current loss:6.555045485496521 absolute loss difference:0.001752045419481263\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:14 INFO 140501882455872] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:14 INFO 140501882455872] Timing: train: 1.57s, val: 0.00s, epoch: 1.57s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:14 INFO 140501882455872] #progress_metric: host=algo-2, completed 77.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309112.864747, \"EndTime\": 1646309114.4317598, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 76, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 348579.0, \"count\": 1, \"min\": 348579, \"max\": 348579}, \"Total Batches Seen\": {\"sum\": 2772.0, \"count\": 1, \"min\": 2772, \"max\": 2772}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 154.0, \"count\": 1, \"min\": 154, \"max\": 154}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:14 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=2888.6035228960864 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:14 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:14 INFO 140501882455872] # Starting training for epoch 78\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:05:15.986] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 233, \"duration\": 1553, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:05:15.128] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 221, \"duration\": 1621, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:15 INFO 139667671275328] # Finished training epoch 74 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:15 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:15 INFO 139667671275328] Loss (name: value) total: 6.516095512443119\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:15 INFO 139667671275328] Loss (name: value) kld: 0.2051056364758147\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:15 INFO 139667671275328] Loss (name: value) recons: 6.310989922947353\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:15 INFO 139667671275328] Loss (name: value) logppx: 6.516095512443119\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:15 INFO 139667671275328] #quality_metric: host=algo-1, epoch=74, train total_loss <loss>=6.516095512443119\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:15 INFO 139667671275328] patience losses:[6.521621624628703, 6.52058302031623, 6.518545733557807, 6.514567746056451, 6.512685206201342] min patience loss:6.512685206201342 current loss:6.516095512443119 absolute loss difference:0.0034103062417774055\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:15 INFO 139667671275328] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:15 INFO 139667671275328] Timing: train: 1.62s, val: 0.00s, epoch: 1.62s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:15 INFO 139667671275328] #progress_metric: host=algo-1, completed 74.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309113.5064964, \"EndTime\": 1646309115.1299028, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 73, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 334776.0, \"count\": 1, \"min\": 334776, \"max\": 334776}, \"Total Batches Seen\": {\"sum\": 2664.0, \"count\": 1, \"min\": 2664, \"max\": 2664}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 148.0, \"count\": 1, \"min\": 148, \"max\": 148}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:15 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=2786.37889492175 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:15 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:15 INFO 139667671275328] # Starting training for epoch 75\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:05:16.812] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 224, \"duration\": 1681, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:16 INFO 139667671275328] # Finished training epoch 75 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:16 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:16 INFO 139667671275328] Loss (name: value) total: 6.513401342762841\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:16 INFO 139667671275328] Loss (name: value) kld: 0.20703368427024949\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:16 INFO 139667671275328] Loss (name: value) recons: 6.306367642349667\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:16 INFO 139667671275328] Loss (name: value) logppx: 6.513401342762841\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:16 INFO 139667671275328] #quality_metric: host=algo-1, epoch=75, train total_loss <loss>=6.513401342762841\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:16 INFO 139667671275328] patience losses:[6.52058302031623, 6.518545733557807, 6.514567746056451, 6.512685206201342, 6.516095512443119] min patience loss:6.512685206201342 current loss:6.513401342762841 absolute loss difference:0.0007161365614996029\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:16 INFO 139667671275328] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:16 INFO 139667671275328] Timing: train: 1.68s, val: 0.00s, epoch: 1.68s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:16 INFO 139667671275328] #progress_metric: host=algo-1, completed 75.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309115.1304827, \"EndTime\": 1646309116.8145804, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 74, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 339300.0, \"count\": 1, \"min\": 339300, \"max\": 339300}, \"Total Batches Seen\": {\"sum\": 2700.0, \"count\": 1, \"min\": 2700, \"max\": 2700}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 150.0, \"count\": 1, \"min\": 150, \"max\": 150}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:16 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=2685.8489464058302 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:16 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:16 INFO 139667671275328] # Starting training for epoch 76\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:15 INFO 140501882455872] # Finished training epoch 78 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:15 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:15 INFO 140501882455872] Loss (name: value) total: 6.559568895234002\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:15 INFO 140501882455872] Loss (name: value) kld: 0.2041096751474672\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:15 INFO 140501882455872] Loss (name: value) recons: 6.355459206634098\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:15 INFO 140501882455872] Loss (name: value) logppx: 6.559568895234002\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:15 INFO 140501882455872] #quality_metric: host=algo-2, epoch=78, train total_loss <loss>=6.559568895234002\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:15 INFO 140501882455872] patience losses:[6.560993472735087, 6.559698250558641, 6.564304755793677, 6.55329344007704, 6.555045485496521] min patience loss:6.55329344007704 current loss:6.559568895234002 absolute loss difference:0.006275455156962373\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:15 INFO 140501882455872] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:15 INFO 140501882455872] Timing: train: 1.56s, val: 0.00s, epoch: 1.56s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:15 INFO 140501882455872] #progress_metric: host=algo-2, completed 78.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309114.4325442, \"EndTime\": 1646309115.9895685, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 77, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 353106.0, \"count\": 1, \"min\": 353106, \"max\": 353106}, \"Total Batches Seen\": {\"sum\": 2808.0, \"count\": 1, \"min\": 2808, \"max\": 2808}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 156.0, \"count\": 1, \"min\": 156, \"max\": 156}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:15 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=2906.9541745830775 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:15 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:15 INFO 140501882455872] # Starting training for epoch 79\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:05:17.608] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 236, \"duration\": 1618, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:17 INFO 140501882455872] # Finished training epoch 79 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:17 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:17 INFO 140501882455872] Loss (name: value) total: 6.546716014544169\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:17 INFO 140501882455872] Loss (name: value) kld: 0.202987068436212\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:17 INFO 140501882455872] Loss (name: value) recons: 6.343728979428609\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:17 INFO 140501882455872] Loss (name: value) logppx: 6.546716014544169\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:17 INFO 140501882455872] #quality_metric: host=algo-2, epoch=79, train total_loss <loss>=6.546716014544169\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:17 INFO 140501882455872] patience losses:[6.559698250558641, 6.564304755793677, 6.55329344007704, 6.555045485496521, 6.559568895234002] min patience loss:6.55329344007704 current loss:6.546716014544169 absolute loss difference:0.006577425532870329\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:17 INFO 140501882455872] Timing: train: 1.62s, val: 0.01s, epoch: 1.63s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:17 INFO 140501882455872] #progress_metric: host=algo-2, completed 79.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309115.9901652, \"EndTime\": 1646309117.6212916, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 78, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 357633.0, \"count\": 1, \"min\": 357633, \"max\": 357633}, \"Total Batches Seen\": {\"sum\": 2844.0, \"count\": 1, \"min\": 2844, \"max\": 2844}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 158.0, \"count\": 1, \"min\": 158, \"max\": 158}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:17 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=2773.9133203137326 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:17 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:17 INFO 140501882455872] # Starting training for epoch 80\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:05:18.388] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 227, \"duration\": 1572, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:18 INFO 139667671275328] # Finished training epoch 76 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:18 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:18 INFO 139667671275328] Loss (name: value) total: 6.511091185940637\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:18 INFO 139667671275328] Loss (name: value) kld: 0.2077596593234274\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:18 INFO 139667671275328] Loss (name: value) recons: 6.303331487708622\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:18 INFO 139667671275328] Loss (name: value) logppx: 6.511091185940637\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:18 INFO 139667671275328] #quality_metric: host=algo-1, epoch=76, train total_loss <loss>=6.511091185940637\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:18 INFO 139667671275328] patience losses:[6.518545733557807, 6.514567746056451, 6.512685206201342, 6.516095512443119, 6.513401342762841] min patience loss:6.512685206201342 current loss:6.511091185940637 absolute loss difference:0.001594020260704987\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:18 INFO 139667671275328] Timing: train: 1.58s, val: 0.00s, epoch: 1.58s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:18 INFO 139667671275328] #progress_metric: host=algo-1, completed 76.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309116.8150072, \"EndTime\": 1646309118.3956451, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 75, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 343824.0, \"count\": 1, \"min\": 343824, \"max\": 343824}, \"Total Batches Seen\": {\"sum\": 2736.0, \"count\": 1, \"min\": 2736, \"max\": 2736}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 152.0, \"count\": 1, \"min\": 152, \"max\": 152}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:18 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=2861.7496378139435 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:18 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:18 INFO 139667671275328] # Starting training for epoch 77\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:05:19.324] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 239, \"duration\": 1701, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:19 INFO 140501882455872] # Finished training epoch 80 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:19 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:19 INFO 140501882455872] Loss (name: value) total: 6.544420467482673\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:19 INFO 140501882455872] Loss (name: value) kld: 0.2047787821955151\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:19 INFO 140501882455872] Loss (name: value) recons: 6.339641663763258\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:19 INFO 140501882455872] Loss (name: value) logppx: 6.544420467482673\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:19 INFO 140501882455872] #quality_metric: host=algo-2, epoch=80, train total_loss <loss>=6.544420467482673\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:19 INFO 140501882455872] patience losses:[6.564304755793677, 6.55329344007704, 6.555045485496521, 6.559568895234002, 6.546716014544169] min patience loss:6.546716014544169 current loss:6.544420467482673 absolute loss difference:0.0022955470614967055\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:19 INFO 140501882455872] Timing: train: 1.70s, val: 0.00s, epoch: 1.71s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:19 INFO 140501882455872] #progress_metric: host=algo-2, completed 80.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309117.6224256, \"EndTime\": 1646309119.330902, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 79, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 362160.0, \"count\": 1, \"min\": 362160, \"max\": 362160}, \"Total Batches Seen\": {\"sum\": 2880.0, \"count\": 1, \"min\": 2880, \"max\": 2880}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 160.0, \"count\": 1, \"min\": 160, \"max\": 160}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:19 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=2649.3667661524937 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:19 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:19 INFO 140501882455872] # Starting training for epoch 81\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:05:20.038] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 230, \"duration\": 1640, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:20 INFO 139667671275328] # Finished training epoch 77 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:20 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:20 INFO 139667671275328] Loss (name: value) total: 6.5017810927497015\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:20 INFO 139667671275328] Loss (name: value) kld: 0.20813951227400038\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:20 INFO 139667671275328] Loss (name: value) recons: 6.293641600343916\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:20 INFO 139667671275328] Loss (name: value) logppx: 6.5017810927497015\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:20 INFO 139667671275328] #quality_metric: host=algo-1, epoch=77, train total_loss <loss>=6.5017810927497015\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:20 INFO 139667671275328] patience losses:[6.514567746056451, 6.512685206201342, 6.516095512443119, 6.513401342762841, 6.511091185940637] min patience loss:6.511091185940637 current loss:6.5017810927497015 absolute loss difference:0.00931009319093512\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:20 INFO 139667671275328] Timing: train: 1.64s, val: 0.01s, epoch: 1.65s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:20 INFO 139667671275328] #progress_metric: host=algo-1, completed 77.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309118.396054, \"EndTime\": 1646309120.0452707, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 76, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 348348.0, \"count\": 1, \"min\": 348348, \"max\": 348348}, \"Total Batches Seen\": {\"sum\": 2772.0, \"count\": 1, \"min\": 2772, \"max\": 2772}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 154.0, \"count\": 1, \"min\": 154, \"max\": 154}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:20 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=2742.0355445254536 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:20 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:20 INFO 139667671275328] # Starting training for epoch 78\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:05:20.908] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 242, \"duration\": 1576, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:20 INFO 140501882455872] # Finished training epoch 81 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:20 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:20 INFO 140501882455872] Loss (name: value) total: 6.543445448080699\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:20 INFO 140501882455872] Loss (name: value) kld: 0.20662130332655376\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:20 INFO 140501882455872] Loss (name: value) recons: 6.336824138959249\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:20 INFO 140501882455872] Loss (name: value) logppx: 6.543445448080699\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:20 INFO 140501882455872] #quality_metric: host=algo-2, epoch=81, train total_loss <loss>=6.543445448080699\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:20 INFO 140501882455872] patience losses:[6.55329344007704, 6.555045485496521, 6.559568895234002, 6.546716014544169, 6.544420467482673] min patience loss:6.544420467482673 current loss:6.543445448080699 absolute loss difference:0.0009750194019737535\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:20 INFO 140501882455872] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:20 INFO 140501882455872] Timing: train: 1.58s, val: 0.01s, epoch: 1.58s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:20 INFO 140501882455872] #progress_metric: host=algo-2, completed 81.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309119.3312674, \"EndTime\": 1646309120.9164355, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 80, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 366687.0, \"count\": 1, \"min\": 366687, \"max\": 366687}, \"Total Batches Seen\": {\"sum\": 2916.0, \"count\": 1, \"min\": 2916, \"max\": 2916}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 162.0, \"count\": 1, \"min\": 162, \"max\": 162}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:20 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=2855.31983419921 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:20 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:20 INFO 140501882455872] # Starting training for epoch 82\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:05:21.583] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 233, \"duration\": 1536, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:21 INFO 139667671275328] # Finished training epoch 78 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:21 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:21 INFO 139667671275328] Loss (name: value) total: 6.508603261576758\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:21 INFO 139667671275328] Loss (name: value) kld: 0.21273912717070845\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:21 INFO 139667671275328] Loss (name: value) recons: 6.295864138338301\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:21 INFO 139667671275328] Loss (name: value) logppx: 6.508603261576758\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:21 INFO 139667671275328] #quality_metric: host=algo-1, epoch=78, train total_loss <loss>=6.508603261576758\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:21 INFO 139667671275328] patience losses:[6.512685206201342, 6.516095512443119, 6.513401342762841, 6.511091185940637, 6.5017810927497015] min patience loss:6.5017810927497015 current loss:6.508603261576758 absolute loss difference:0.006822168827056885\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:21 INFO 139667671275328] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:21 INFO 139667671275328] Timing: train: 1.54s, val: 0.00s, epoch: 1.54s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:21 INFO 139667671275328] #progress_metric: host=algo-1, completed 78.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309120.046492, \"EndTime\": 1646309121.5861857, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 77, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 352872.0, \"count\": 1, \"min\": 352872, \"max\": 352872}, \"Total Batches Seen\": {\"sum\": 2808.0, \"count\": 1, \"min\": 2808, \"max\": 2808}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 156.0, \"count\": 1, \"min\": 156, \"max\": 156}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:21 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=2937.8102281455535 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:21 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:21 INFO 139667671275328] # Starting training for epoch 79\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:05:22.414] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 245, \"duration\": 1496, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:22 INFO 140501882455872] # Finished training epoch 82 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:22 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:22 INFO 140501882455872] Loss (name: value) total: 6.545382916927338\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:22 INFO 140501882455872] Loss (name: value) kld: 0.21150383725762367\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:22 INFO 140501882455872] Loss (name: value) recons: 6.333879086706373\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:22 INFO 140501882455872] Loss (name: value) logppx: 6.545382916927338\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:22 INFO 140501882455872] #quality_metric: host=algo-2, epoch=82, train total_loss <loss>=6.545382916927338\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:22 INFO 140501882455872] patience losses:[6.555045485496521, 6.559568895234002, 6.546716014544169, 6.544420467482673, 6.543445448080699] min patience loss:6.543445448080699 current loss:6.545382916927338 absolute loss difference:0.0019374688466387013\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:22 INFO 140501882455872] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:22 INFO 140501882455872] Timing: train: 1.50s, val: 0.00s, epoch: 1.50s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:22 INFO 140501882455872] #progress_metric: host=algo-2, completed 82.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309120.9169667, \"EndTime\": 1646309122.4168756, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 81, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 371214.0, \"count\": 1, \"min\": 371214, \"max\": 371214}, \"Total Batches Seen\": {\"sum\": 2952.0, \"count\": 1, \"min\": 2952, \"max\": 2952}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 164.0, \"count\": 1, \"min\": 164, \"max\": 164}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:22 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=3017.78463088683 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:22 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:22 INFO 140501882455872] # Starting training for epoch 83\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:05:23.139] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 236, \"duration\": 1552, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:23 INFO 139667671275328] # Finished training epoch 79 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:23 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:23 INFO 139667671275328] Loss (name: value) total: 6.502852771017286\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:23 INFO 139667671275328] Loss (name: value) kld: 0.2141627880434195\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:23 INFO 139667671275328] Loss (name: value) recons: 6.2886899842156305\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:23 INFO 139667671275328] Loss (name: value) logppx: 6.502852771017286\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:23 INFO 139667671275328] #quality_metric: host=algo-1, epoch=79, train total_loss <loss>=6.502852771017286\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:23 INFO 139667671275328] patience losses:[6.516095512443119, 6.513401342762841, 6.511091185940637, 6.5017810927497015, 6.508603261576758] min patience loss:6.5017810927497015 current loss:6.502852771017286 absolute loss difference:0.001071678267584808\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:23 INFO 139667671275328] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:23 INFO 139667671275328] Timing: train: 1.55s, val: 0.00s, epoch: 1.55s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:23 INFO 139667671275328] #progress_metric: host=algo-1, completed 79.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309121.586612, \"EndTime\": 1646309123.1418962, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 78, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 357396.0, \"count\": 1, \"min\": 357396, \"max\": 357396}, \"Total Batches Seen\": {\"sum\": 2844.0, \"count\": 1, \"min\": 2844, \"max\": 2844}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 158.0, \"count\": 1, \"min\": 158, \"max\": 158}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:23 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=2908.243319097908 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:23 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:23 INFO 139667671275328] # Starting training for epoch 80\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:05:24.753] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 239, \"duration\": 1610, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:24 INFO 139667671275328] # Finished training epoch 80 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:24 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:24 INFO 139667671275328] Loss (name: value) total: 6.500975065761143\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:24 INFO 139667671275328] Loss (name: value) kld: 0.21510853163070148\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:24 INFO 139667671275328] Loss (name: value) recons: 6.285866479078929\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:24 INFO 139667671275328] Loss (name: value) logppx: 6.500975065761143\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:24 INFO 139667671275328] #quality_metric: host=algo-1, epoch=80, train total_loss <loss>=6.500975065761143\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:24 INFO 139667671275328] patience losses:[6.513401342762841, 6.511091185940637, 6.5017810927497015, 6.508603261576758, 6.502852771017286] min patience loss:6.5017810927497015 current loss:6.500975065761143 absolute loss difference:0.0008060269885588056\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:24 INFO 139667671275328] Bad epoch: loss has not improved (enough). Bad count:3\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:24 INFO 139667671275328] Timing: train: 1.61s, val: 0.01s, epoch: 1.62s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:24 INFO 139667671275328] #progress_metric: host=algo-1, completed 80.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309123.1423943, \"EndTime\": 1646309124.7601924, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 79, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 361920.0, \"count\": 1, \"min\": 361920, \"max\": 361920}, \"Total Batches Seen\": {\"sum\": 2880.0, \"count\": 1, \"min\": 2880, \"max\": 2880}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 160.0, \"count\": 1, \"min\": 160, \"max\": 160}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:24 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=2795.837633931084 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:24 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:24 INFO 139667671275328] # Starting training for epoch 81\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:05:26.322] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 242, \"duration\": 1560, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:26 INFO 139667671275328] # Finished training epoch 81 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:26 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:26 INFO 139667671275328] Loss (name: value) total: 6.498855378892687\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:26 INFO 139667671275328] Loss (name: value) kld: 0.21856274414393637\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:26 INFO 139667671275328] Loss (name: value) recons: 6.280292643441094\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:26 INFO 139667671275328] Loss (name: value) logppx: 6.498855378892687\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:26 INFO 139667671275328] #quality_metric: host=algo-1, epoch=81, train total_loss <loss>=6.498855378892687\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:26 INFO 139667671275328] patience losses:[6.511091185940637, 6.5017810927497015, 6.508603261576758, 6.502852771017286, 6.500975065761143] min patience loss:6.500975065761143 current loss:6.498855378892687 absolute loss difference:0.0021196868684558723\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:26 INFO 139667671275328] Timing: train: 1.56s, val: 0.00s, epoch: 1.57s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:26 INFO 139667671275328] #progress_metric: host=algo-1, completed 81.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309124.7609098, \"EndTime\": 1646309126.329855, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 80, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 366444.0, \"count\": 1, \"min\": 366444, \"max\": 366444}, \"Total Batches Seen\": {\"sum\": 2916.0, \"count\": 1, \"min\": 2916, \"max\": 2916}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 162.0, \"count\": 1, \"min\": 162, \"max\": 162}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:26 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=2883.057646015803 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:26 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:26 INFO 139667671275328] # Starting training for epoch 82\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:05:27.885] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 245, \"duration\": 1553, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:27 INFO 139667671275328] # Finished training epoch 82 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:27 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:27 INFO 139667671275328] Loss (name: value) total: 6.491222686237759\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:27 INFO 139667671275328] Loss (name: value) kld: 0.22013446254034838\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:27 INFO 139667671275328] Loss (name: value) recons: 6.271088202794393\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:27 INFO 139667671275328] Loss (name: value) logppx: 6.491222686237759\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:27 INFO 139667671275328] #quality_metric: host=algo-1, epoch=82, train total_loss <loss>=6.491222686237759\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:27 INFO 139667671275328] patience losses:[6.5017810927497015, 6.508603261576758, 6.502852771017286, 6.500975065761143, 6.498855378892687] min patience loss:6.498855378892687 current loss:6.491222686237759 absolute loss difference:0.007632692654928164\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:27 INFO 139667671275328] Timing: train: 1.56s, val: 0.01s, epoch: 1.56s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:27 INFO 139667671275328] #progress_metric: host=algo-1, completed 82.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309126.3306115, \"EndTime\": 1646309127.8925853, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 81, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 370968.0, \"count\": 1, \"min\": 370968, \"max\": 370968}, \"Total Batches Seen\": {\"sum\": 2952.0, \"count\": 1, \"min\": 2952, \"max\": 2952}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 164.0, \"count\": 1, \"min\": 164, \"max\": 164}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:27 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=2895.91190948104 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:27 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:27 INFO 139667671275328] # Starting training for epoch 83\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:05:29.506] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 248, \"duration\": 1612, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:29 INFO 139667671275328] # Finished training epoch 83 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:29 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:29 INFO 139667671275328] Loss (name: value) total: 6.4964655306604175\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:29 INFO 139667671275328] Loss (name: value) kld: 0.22166156851583058\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:29 INFO 139667671275328] Loss (name: value) recons: 6.274804029199812\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:29 INFO 139667671275328] Loss (name: value) logppx: 6.4964655306604175\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:29 INFO 139667671275328] #quality_metric: host=algo-1, epoch=83, train total_loss <loss>=6.4964655306604175\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:29 INFO 139667671275328] patience losses:[6.508603261576758, 6.502852771017286, 6.500975065761143, 6.498855378892687, 6.491222686237759] min patience loss:6.491222686237759 current loss:6.4964655306604175 absolute loss difference:0.005242844422658877\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:29 INFO 139667671275328] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:29 INFO 139667671275328] Timing: train: 1.62s, val: 0.00s, epoch: 1.62s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:29 INFO 139667671275328] #progress_metric: host=algo-1, completed 83.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309127.8930585, \"EndTime\": 1646309129.509226, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 82, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 375492.0, \"count\": 1, \"min\": 375492, \"max\": 375492}, \"Total Batches Seen\": {\"sum\": 2988.0, \"count\": 1, \"min\": 2988, \"max\": 2988}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 166.0, \"count\": 1, \"min\": 166, \"max\": 166}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:29 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=2798.4881398867155 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:29 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:29 INFO 139667671275328] # Starting training for epoch 84\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:05:23.959] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 248, \"duration\": 1542, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:23 INFO 140501882455872] # Finished training epoch 83 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:23 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:23 INFO 140501882455872] Loss (name: value) total: 6.531836880577935\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:23 INFO 140501882455872] Loss (name: value) kld: 0.20864897780120373\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:23 INFO 140501882455872] Loss (name: value) recons: 6.323187920782301\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:23 INFO 140501882455872] Loss (name: value) logppx: 6.531836880577935\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:23 INFO 140501882455872] #quality_metric: host=algo-2, epoch=83, train total_loss <loss>=6.531836880577935\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:23 INFO 140501882455872] patience losses:[6.559568895234002, 6.546716014544169, 6.544420467482673, 6.543445448080699, 6.545382916927338] min patience loss:6.543445448080699 current loss:6.531836880577935 absolute loss difference:0.011608567502763734\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:23 INFO 140501882455872] Timing: train: 1.55s, val: 0.00s, epoch: 1.55s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:23 INFO 140501882455872] #progress_metric: host=algo-2, completed 83.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309122.417254, \"EndTime\": 1646309123.967427, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 82, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 375741.0, \"count\": 1, \"min\": 375741, \"max\": 375741}, \"Total Batches Seen\": {\"sum\": 2988.0, \"count\": 1, \"min\": 2988, \"max\": 2988}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 166.0, \"count\": 1, \"min\": 166, \"max\": 166}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:23 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=2919.936508082966 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:23 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:23 INFO 140501882455872] # Starting training for epoch 84\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:05:25.677] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 251, \"duration\": 1707, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:25 INFO 140501882455872] # Finished training epoch 84 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:25 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:25 INFO 140501882455872] Loss (name: value) total: 6.540343165397644\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:25 INFO 140501882455872] Loss (name: value) kld: 0.21259001104368103\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:25 INFO 140501882455872] Loss (name: value) recons: 6.327753139866723\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:25 INFO 140501882455872] Loss (name: value) logppx: 6.540343165397644\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:25 INFO 140501882455872] #quality_metric: host=algo-2, epoch=84, train total_loss <loss>=6.540343165397644\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:25 INFO 140501882455872] patience losses:[6.546716014544169, 6.544420467482673, 6.543445448080699, 6.545382916927338, 6.531836880577935] min patience loss:6.531836880577935 current loss:6.540343165397644 absolute loss difference:0.008506284819708831\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:25 INFO 140501882455872] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:25 INFO 140501882455872] Timing: train: 1.71s, val: 0.00s, epoch: 1.71s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:25 INFO 140501882455872] #progress_metric: host=algo-2, completed 84.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309123.968175, \"EndTime\": 1646309125.679811, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 83, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 380268.0, \"count\": 1, \"min\": 380268, \"max\": 380268}, \"Total Batches Seen\": {\"sum\": 3024.0, \"count\": 1, \"min\": 3024, \"max\": 3024}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 168.0, \"count\": 1, \"min\": 168, \"max\": 168}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:25 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=2644.571235650458 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:25 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:25 INFO 140501882455872] # Starting training for epoch 85\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:05:27.221] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 254, \"duration\": 1540, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:27 INFO 140501882455872] # Finished training epoch 85 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:27 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:27 INFO 140501882455872] Loss (name: value) total: 6.539737039142185\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:27 INFO 140501882455872] Loss (name: value) kld: 0.2143228097508351\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:27 INFO 140501882455872] Loss (name: value) recons: 6.325414253605737\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:27 INFO 140501882455872] Loss (name: value) logppx: 6.539737039142185\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:27 INFO 140501882455872] #quality_metric: host=algo-2, epoch=85, train total_loss <loss>=6.539737039142185\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:27 INFO 140501882455872] patience losses:[6.544420467482673, 6.543445448080699, 6.545382916927338, 6.531836880577935, 6.540343165397644] min patience loss:6.531836880577935 current loss:6.539737039142185 absolute loss difference:0.00790015856424997\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:27 INFO 140501882455872] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:27 INFO 140501882455872] Timing: train: 1.54s, val: 0.00s, epoch: 1.54s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:27 INFO 140501882455872] #progress_metric: host=algo-2, completed 85.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309125.6801736, \"EndTime\": 1646309127.225398, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 84, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 384795.0, \"count\": 1, \"min\": 384795, \"max\": 384795}, \"Total Batches Seen\": {\"sum\": 3060.0, \"count\": 1, \"min\": 3060, \"max\": 3060}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 170.0, \"count\": 1, \"min\": 170, \"max\": 170}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:27 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=2929.3063256128676 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:27 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:27 INFO 140501882455872] # Starting training for epoch 86\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:05:28.784] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 257, \"duration\": 1557, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:28 INFO 140501882455872] # Finished training epoch 86 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:28 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:28 INFO 140501882455872] Loss (name: value) total: 6.535386946466234\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:28 INFO 140501882455872] Loss (name: value) kld: 0.21533890544540352\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:28 INFO 140501882455872] Loss (name: value) recons: 6.320048014322917\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:28 INFO 140501882455872] Loss (name: value) logppx: 6.535386946466234\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:28 INFO 140501882455872] #quality_metric: host=algo-2, epoch=86, train total_loss <loss>=6.535386946466234\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:28 INFO 140501882455872] patience losses:[6.543445448080699, 6.545382916927338, 6.531836880577935, 6.540343165397644, 6.539737039142185] min patience loss:6.531836880577935 current loss:6.535386946466234 absolute loss difference:0.003550065888298981\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:28 INFO 140501882455872] Bad epoch: loss has not improved (enough). Bad count:3\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:28 INFO 140501882455872] Timing: train: 1.56s, val: 0.00s, epoch: 1.56s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:28 INFO 140501882455872] #progress_metric: host=algo-2, completed 86.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309127.2257679, \"EndTime\": 1646309128.786145, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 85, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 389322.0, \"count\": 1, \"min\": 389322, \"max\": 389322}, \"Total Batches Seen\": {\"sum\": 3096.0, \"count\": 1, \"min\": 3096, \"max\": 3096}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 172.0, \"count\": 1, \"min\": 172, \"max\": 172}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:28 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=2900.8724500739745 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:28 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:28 INFO 140501882455872] # Starting training for epoch 87\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:05:30.383] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 260, \"duration\": 1596, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:30 INFO 140501882455872] # Finished training epoch 87 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:30 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:30 INFO 140501882455872] Loss (name: value) total: 6.538429141044617\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:30 INFO 140501882455872] Loss (name: value) kld: 0.2190022607230478\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:30 INFO 140501882455872] Loss (name: value) recons: 6.319426947169834\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:30 INFO 140501882455872] Loss (name: value) logppx: 6.538429141044617\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:30 INFO 140501882455872] #quality_metric: host=algo-2, epoch=87, train total_loss <loss>=6.538429141044617\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:30 INFO 140501882455872] patience losses:[6.545382916927338, 6.531836880577935, 6.540343165397644, 6.539737039142185, 6.535386946466234] min patience loss:6.531836880577935 current loss:6.538429141044617 absolute loss difference:0.006592260466681488\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:30 INFO 140501882455872] Bad epoch: loss has not improved (enough). Bad count:4\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:30 INFO 140501882455872] Timing: train: 1.60s, val: 0.00s, epoch: 1.60s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:30 INFO 140501882455872] #progress_metric: host=algo-2, completed 87.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309128.78649, \"EndTime\": 1646309130.3854928, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 86, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 393849.0, \"count\": 1, \"min\": 393849, \"max\": 393849}, \"Total Batches Seen\": {\"sum\": 3132.0, \"count\": 1, \"min\": 3132, \"max\": 3132}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 174.0, \"count\": 1, \"min\": 174, \"max\": 174}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:30 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=2830.78067027116 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:30 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:30 INFO 140501882455872] # Starting training for epoch 88\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:05:31.084] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 251, \"duration\": 1573, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:31 INFO 139667671275328] # Finished training epoch 84 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:31 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:31 INFO 139667671275328] Loss (name: value) total: 6.492601891358693\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:31 INFO 139667671275328] Loss (name: value) kld: 0.22089109693964323\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:31 INFO 139667671275328] Loss (name: value) recons: 6.271710846159193\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:31 INFO 139667671275328] Loss (name: value) logppx: 6.492601891358693\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:31 INFO 139667671275328] #quality_metric: host=algo-1, epoch=84, train total_loss <loss>=6.492601891358693\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:31 INFO 139667671275328] patience losses:[6.502852771017286, 6.500975065761143, 6.498855378892687, 6.491222686237759, 6.4964655306604175] min patience loss:6.491222686237759 current loss:6.492601891358693 absolute loss difference:0.0013792051209344791\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:31 INFO 139667671275328] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:31 INFO 139667671275328] Timing: train: 1.58s, val: 0.00s, epoch: 1.58s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:31 INFO 139667671275328] #progress_metric: host=algo-1, completed 84.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309129.509814, \"EndTime\": 1646309131.086043, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 83, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 380016.0, \"count\": 1, \"min\": 380016, \"max\": 380016}, \"Total Batches Seen\": {\"sum\": 3024.0, \"count\": 1, \"min\": 3024, \"max\": 3024}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 168.0, \"count\": 1, \"min\": 168, \"max\": 168}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:31 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=2869.688860604583 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:31 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:31 INFO 139667671275328] # Starting training for epoch 85\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:05:31.897] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 263, \"duration\": 1511, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:31 INFO 140501882455872] # Finished training epoch 88 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:31 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:31 INFO 140501882455872] Loss (name: value) total: 6.527465005715688\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:31 INFO 140501882455872] Loss (name: value) kld: 0.2183622136298153\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:31 INFO 140501882455872] Loss (name: value) recons: 6.30910278028912\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:31 INFO 140501882455872] Loss (name: value) logppx: 6.527465005715688\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:31 INFO 140501882455872] #quality_metric: host=algo-2, epoch=88, train total_loss <loss>=6.527465005715688\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:31 INFO 140501882455872] patience losses:[6.531836880577935, 6.540343165397644, 6.539737039142185, 6.535386946466234, 6.538429141044617] min patience loss:6.531836880577935 current loss:6.527465005715688 absolute loss difference:0.004371874862247438\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:31 INFO 140501882455872] Timing: train: 1.51s, val: 0.00s, epoch: 1.52s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:31 INFO 140501882455872] #progress_metric: host=algo-2, completed 88.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309130.3858776, \"EndTime\": 1646309131.9044716, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 87, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 398376.0, \"count\": 1, \"min\": 398376, \"max\": 398376}, \"Total Batches Seen\": {\"sum\": 3168.0, \"count\": 1, \"min\": 3168, \"max\": 3168}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 176.0, \"count\": 1, \"min\": 176, \"max\": 176}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:31 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=2980.7347512885176 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:31 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:31 INFO 140501882455872] # Starting training for epoch 89\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:05:32.514] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 254, \"duration\": 1427, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:32 INFO 139667671275328] # Finished training epoch 85 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:32 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:32 INFO 139667671275328] Loss (name: value) total: 6.494156314267053\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:32 INFO 139667671275328] Loss (name: value) kld: 0.2229203728752004\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:32 INFO 139667671275328] Loss (name: value) recons: 6.271235903104146\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:32 INFO 139667671275328] Loss (name: value) logppx: 6.494156314267053\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:32 INFO 139667671275328] #quality_metric: host=algo-1, epoch=85, train total_loss <loss>=6.494156314267053\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:32 INFO 139667671275328] patience losses:[6.500975065761143, 6.498855378892687, 6.491222686237759, 6.4964655306604175, 6.492601891358693] min patience loss:6.491222686237759 current loss:6.494156314267053 absolute loss difference:0.0029336280292939776\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:32 INFO 139667671275328] Bad epoch: loss has not improved (enough). Bad count:3\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:32 INFO 139667671275328] Timing: train: 1.43s, val: 0.00s, epoch: 1.43s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:32 INFO 139667671275328] #progress_metric: host=algo-1, completed 85.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309131.0864506, \"EndTime\": 1646309132.5165882, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 84, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 384540.0, \"count\": 1, \"min\": 384540, \"max\": 384540}, \"Total Batches Seen\": {\"sum\": 3060.0, \"count\": 1, \"min\": 3060, \"max\": 3060}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 170.0, \"count\": 1, \"min\": 170, \"max\": 170}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:32 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=3162.891088712823 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:32 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:32 INFO 139667671275328] # Starting training for epoch 86\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:05:34.059] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 257, \"duration\": 1541, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:34 INFO 139667671275328] # Finished training epoch 86 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:34 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:34 INFO 139667671275328] Loss (name: value) total: 6.486954443984562\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:34 INFO 139667671275328] Loss (name: value) kld: 0.22400711766547626\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:34 INFO 139667671275328] Loss (name: value) recons: 6.2629473606745405\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:34 INFO 139667671275328] Loss (name: value) logppx: 6.486954443984562\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:34 INFO 139667671275328] #quality_metric: host=algo-1, epoch=86, train total_loss <loss>=6.486954443984562\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:34 INFO 139667671275328] patience losses:[6.498855378892687, 6.491222686237759, 6.4964655306604175, 6.492601891358693, 6.494156314267053] min patience loss:6.491222686237759 current loss:6.486954443984562 absolute loss difference:0.0042682422531967745\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:34 INFO 139667671275328] Timing: train: 1.55s, val: 0.01s, epoch: 1.55s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:34 INFO 139667671275328] #progress_metric: host=algo-1, completed 86.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309132.5169578, \"EndTime\": 1646309134.0684853, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 85, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 389064.0, \"count\": 1, \"min\": 389064, \"max\": 389064}, \"Total Batches Seen\": {\"sum\": 3096.0, \"count\": 1, \"min\": 3096, \"max\": 3096}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 172.0, \"count\": 1, \"min\": 172, \"max\": 172}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:34 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=2915.3088871834934 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:34 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:34 INFO 139667671275328] # Starting training for epoch 87\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:05:35.671] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 260, \"duration\": 1600, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:35 INFO 139667671275328] # Finished training epoch 87 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:35 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:35 INFO 139667671275328] Loss (name: value) total: 6.491149087746938\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:35 INFO 139667671275328] Loss (name: value) kld: 0.22859761056800684\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:35 INFO 139667671275328] Loss (name: value) recons: 6.262551506360372\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:35 INFO 139667671275328] Loss (name: value) logppx: 6.491149087746938\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:35 INFO 139667671275328] #quality_metric: host=algo-1, epoch=87, train total_loss <loss>=6.491149087746938\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:35 INFO 139667671275328] patience losses:[6.491222686237759, 6.4964655306604175, 6.492601891358693, 6.494156314267053, 6.486954443984562] min patience loss:6.486954443984562 current loss:6.491149087746938 absolute loss difference:0.0041946437623758825\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:35 INFO 139667671275328] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:35 INFO 139667671275328] Timing: train: 1.61s, val: 0.00s, epoch: 1.61s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:35 INFO 139667671275328] #progress_metric: host=algo-1, completed 87.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309134.0691965, \"EndTime\": 1646309135.6766453, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 86, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 393588.0, \"count\": 1, \"min\": 393588, \"max\": 393588}, \"Total Batches Seen\": {\"sum\": 3132.0, \"count\": 1, \"min\": 3132, \"max\": 3132}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 174.0, \"count\": 1, \"min\": 174, \"max\": 174}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:35 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=2813.5278787903053 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:35 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:35 INFO 139667671275328] # Starting training for epoch 88\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:05:33.393] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 266, \"duration\": 1488, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:33 INFO 140501882455872] # Finished training epoch 89 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:33 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:33 INFO 140501882455872] Loss (name: value) total: 6.530314021640354\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:33 INFO 140501882455872] Loss (name: value) kld: 0.21902432810101244\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:33 INFO 140501882455872] Loss (name: value) recons: 6.3112896415922375\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:33 INFO 140501882455872] Loss (name: value) logppx: 6.530314021640354\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:33 INFO 140501882455872] #quality_metric: host=algo-2, epoch=89, train total_loss <loss>=6.530314021640354\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:33 INFO 140501882455872] patience losses:[6.540343165397644, 6.539737039142185, 6.535386946466234, 6.538429141044617, 6.527465005715688] min patience loss:6.527465005715688 current loss:6.530314021640354 absolute loss difference:0.0028490159246663538\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:33 INFO 140501882455872] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:33 INFO 140501882455872] Timing: train: 1.49s, val: 0.00s, epoch: 1.49s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:33 INFO 140501882455872] #progress_metric: host=algo-2, completed 89.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309131.9047837, \"EndTime\": 1646309133.3948963, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 88, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 402903.0, \"count\": 1, \"min\": 402903, \"max\": 402903}, \"Total Batches Seen\": {\"sum\": 3204.0, \"count\": 1, \"min\": 3204, \"max\": 3204}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 178.0, \"count\": 1, \"min\": 178, \"max\": 178}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:33 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=3037.6741655505266 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:33 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:33 INFO 140501882455872] # Starting training for epoch 90\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:05:35.005] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 269, \"duration\": 1609, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:35 INFO 140501882455872] # Finished training epoch 90 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:35 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:35 INFO 140501882455872] Loss (name: value) total: 6.527857184410095\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:35 INFO 140501882455872] Loss (name: value) kld: 0.22015643782085842\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:35 INFO 140501882455872] Loss (name: value) recons: 6.307700739966498\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:35 INFO 140501882455872] Loss (name: value) logppx: 6.527857184410095\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:35 INFO 140501882455872] #quality_metric: host=algo-2, epoch=90, train total_loss <loss>=6.527857184410095\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:35 INFO 140501882455872] patience losses:[6.539737039142185, 6.535386946466234, 6.538429141044617, 6.527465005715688, 6.530314021640354] min patience loss:6.527465005715688 current loss:6.527857184410095 absolute loss difference:0.00039217869440744124\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:35 INFO 140501882455872] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:35 INFO 140501882455872] Timing: train: 1.61s, val: 0.00s, epoch: 1.61s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:35 INFO 140501882455872] #progress_metric: host=algo-2, completed 90.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309133.3951993, \"EndTime\": 1646309135.0068235, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 89, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 407430.0, \"count\": 1, \"min\": 407430, \"max\": 407430}, \"Total Batches Seen\": {\"sum\": 3240.0, \"count\": 1, \"min\": 3240, \"max\": 3240}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 180.0, \"count\": 1, \"min\": 180, \"max\": 180}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:35 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=2808.6454146245715 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:35 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:35 INFO 140501882455872] # Starting training for epoch 91\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:05:36.502] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 272, \"duration\": 1494, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:36 INFO 140501882455872] # Finished training epoch 91 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:36 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:36 INFO 140501882455872] Loss (name: value) total: 6.525922331545088\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:36 INFO 140501882455872] Loss (name: value) kld: 0.2286226906710201\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:36 INFO 140501882455872] Loss (name: value) recons: 6.297299630112118\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:36 INFO 140501882455872] Loss (name: value) logppx: 6.525922331545088\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:36 INFO 140501882455872] #quality_metric: host=algo-2, epoch=91, train total_loss <loss>=6.525922331545088\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:36 INFO 140501882455872] patience losses:[6.535386946466234, 6.538429141044617, 6.527465005715688, 6.530314021640354, 6.527857184410095] min patience loss:6.527465005715688 current loss:6.525922331545088 absolute loss difference:0.0015426741705999447\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:36 INFO 140501882455872] Timing: train: 1.50s, val: 0.00s, epoch: 1.50s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:36 INFO 140501882455872] #progress_metric: host=algo-2, completed 91.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309135.007175, \"EndTime\": 1646309136.5078988, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 90, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 411957.0, \"count\": 1, \"min\": 411957, \"max\": 411957}, \"Total Batches Seen\": {\"sum\": 3276.0, \"count\": 1, \"min\": 3276, \"max\": 3276}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 182.0, \"count\": 1, \"min\": 182, \"max\": 182}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:36 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=3016.1154839249757 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:36 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:36 INFO 140501882455872] # Starting training for epoch 92\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:05:37.099] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 263, \"duration\": 1421, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:37 INFO 139667671275328] # Finished training epoch 88 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:37 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:37 INFO 139667671275328] Loss (name: value) total: 6.481370309988658\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:37 INFO 139667671275328] Loss (name: value) kld: 0.2291083244813813\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:37 INFO 139667671275328] Loss (name: value) recons: 6.252261983023749\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:37 INFO 139667671275328] Loss (name: value) logppx: 6.481370309988658\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:37 INFO 139667671275328] #quality_metric: host=algo-1, epoch=88, train total_loss <loss>=6.481370309988658\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:37 INFO 139667671275328] patience losses:[6.4964655306604175, 6.492601891358693, 6.494156314267053, 6.486954443984562, 6.491149087746938] min patience loss:6.486954443984562 current loss:6.481370309988658 absolute loss difference:0.0055841339959039615\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:37 INFO 139667671275328] Timing: train: 1.42s, val: 0.00s, epoch: 1.43s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:37 INFO 139667671275328] #progress_metric: host=algo-1, completed 88.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309135.6775672, \"EndTime\": 1646309137.1060243, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 87, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 398112.0, \"count\": 1, \"min\": 398112, \"max\": 398112}, \"Total Batches Seen\": {\"sum\": 3168.0, \"count\": 1, \"min\": 3168, \"max\": 3168}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 176.0, \"count\": 1, \"min\": 176, \"max\": 176}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:37 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=3166.593287934724 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:37 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:37 INFO 139667671275328] # Starting training for epoch 89\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:05:38.020] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 275, \"duration\": 1510, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:38 INFO 140501882455872] # Finished training epoch 92 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:38 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:38 INFO 140501882455872] Loss (name: value) total: 6.522417724132538\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:38 INFO 140501882455872] Loss (name: value) kld: 0.22408152516517374\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:38 INFO 140501882455872] Loss (name: value) recons: 6.298336254225837\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:38 INFO 140501882455872] Loss (name: value) logppx: 6.522417724132538\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:38 INFO 140501882455872] #quality_metric: host=algo-2, epoch=92, train total_loss <loss>=6.522417724132538\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:38 INFO 140501882455872] patience losses:[6.538429141044617, 6.527465005715688, 6.530314021640354, 6.527857184410095, 6.525922331545088] min patience loss:6.525922331545088 current loss:6.522417724132538 absolute loss difference:0.003504607412549987\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:38 INFO 140501882455872] Timing: train: 1.51s, val: 0.00s, epoch: 1.52s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:38 INFO 140501882455872] #progress_metric: host=algo-2, completed 92.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309136.5086827, \"EndTime\": 1646309138.0259242, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 91, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 416484.0, \"count\": 1, \"min\": 416484, \"max\": 416484}, \"Total Batches Seen\": {\"sum\": 3312.0, \"count\": 1, \"min\": 3312, \"max\": 3312}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 184.0, \"count\": 1, \"min\": 184, \"max\": 184}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:38 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=2983.361186983306 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:38 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:38 INFO 140501882455872] # Starting training for epoch 93\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:05:38.630] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 266, \"duration\": 1519, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:38 INFO 139667671275328] # Finished training epoch 89 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:38 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:38 INFO 139667671275328] Loss (name: value) total: 6.485215451982286\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:38 INFO 139667671275328] Loss (name: value) kld: 0.22978256560034221\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:38 INFO 139667671275328] Loss (name: value) recons: 6.255432923634847\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:38 INFO 139667671275328] Loss (name: value) logppx: 6.485215451982286\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:38 INFO 139667671275328] #quality_metric: host=algo-1, epoch=89, train total_loss <loss>=6.485215451982286\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:38 INFO 139667671275328] patience losses:[6.492601891358693, 6.494156314267053, 6.486954443984562, 6.491149087746938, 6.481370309988658] min patience loss:6.481370309988658 current loss:6.485215451982286 absolute loss difference:0.003845141993628509\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:38 INFO 139667671275328] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:38 INFO 139667671275328] Timing: train: 1.53s, val: 0.00s, epoch: 1.53s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:38 INFO 139667671275328] #progress_metric: host=algo-1, completed 89.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309137.1064339, \"EndTime\": 1646309138.6328478, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 88, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 402636.0, \"count\": 1, \"min\": 402636, \"max\": 402636}, \"Total Batches Seen\": {\"sum\": 3204.0, \"count\": 1, \"min\": 3204, \"max\": 3204}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 178.0, \"count\": 1, \"min\": 178, \"max\": 178}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:38 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=2963.3604774102505 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:38 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:38 INFO 139667671275328] # Starting training for epoch 90\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:05:39.570] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 278, \"duration\": 1543, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:39 INFO 140501882455872] # Finished training epoch 93 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:39 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:39 INFO 140501882455872] Loss (name: value) total: 6.519804265764025\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:39 INFO 140501882455872] Loss (name: value) kld: 0.22550048844681847\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:39 INFO 140501882455872] Loss (name: value) recons: 6.294303788079156\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:39 INFO 140501882455872] Loss (name: value) logppx: 6.519804265764025\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:39 INFO 140501882455872] #quality_metric: host=algo-2, epoch=93, train total_loss <loss>=6.519804265764025\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:39 INFO 140501882455872] patience losses:[6.527465005715688, 6.530314021640354, 6.527857184410095, 6.525922331545088, 6.522417724132538] min patience loss:6.522417724132538 current loss:6.519804265764025 absolute loss difference:0.002613458368513122\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:39 INFO 140501882455872] Timing: train: 1.55s, val: 0.00s, epoch: 1.55s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:39 INFO 140501882455872] #progress_metric: host=algo-2, completed 93.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309138.0265784, \"EndTime\": 1646309139.5763113, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 92, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 421011.0, \"count\": 1, \"min\": 421011, \"max\": 421011}, \"Total Batches Seen\": {\"sum\": 3348.0, \"count\": 1, \"min\": 3348, \"max\": 3348}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 186.0, \"count\": 1, \"min\": 186, \"max\": 186}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:39 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=2920.6371646786456 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:39 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:39 INFO 140501882455872] # Starting training for epoch 94\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:05:40.219] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 269, \"duration\": 1585, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:40 INFO 139667671275328] # Finished training epoch 90 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:40 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:40 INFO 139667671275328] Loss (name: value) total: 6.476872086524963\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:40 INFO 139667671275328] Loss (name: value) kld: 0.22939763300948673\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:40 INFO 139667671275328] Loss (name: value) recons: 6.247474451859792\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:40 INFO 139667671275328] Loss (name: value) logppx: 6.476872086524963\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:40 INFO 139667671275328] #quality_metric: host=algo-1, epoch=90, train total_loss <loss>=6.476872086524963\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:40 INFO 139667671275328] patience losses:[6.494156314267053, 6.486954443984562, 6.491149087746938, 6.481370309988658, 6.485215451982286] min patience loss:6.481370309988658 current loss:6.476872086524963 absolute loss difference:0.004498223463694551\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:40 INFO 139667671275328] Timing: train: 1.59s, val: 0.01s, epoch: 1.59s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:40 INFO 139667671275328] #progress_metric: host=algo-1, completed 90.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309138.6332629, \"EndTime\": 1646309140.2265637, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 89, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 407160.0, \"count\": 1, \"min\": 407160, \"max\": 407160}, \"Total Batches Seen\": {\"sum\": 3240.0, \"count\": 1, \"min\": 3240, \"max\": 3240}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 180.0, \"count\": 1, \"min\": 180, \"max\": 180}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:40 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=2838.6752894434217 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:40 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:40 INFO 139667671275328] # Starting training for epoch 91\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:05:41.183] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 281, \"duration\": 1605, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:05:41.726] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 272, \"duration\": 1498, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:41 INFO 139667671275328] # Finished training epoch 91 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:41 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:41 INFO 139667671275328] Loss (name: value) total: 6.476259251435597\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:41 INFO 139667671275328] Loss (name: value) kld: 0.2318873241957691\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:41 INFO 139667671275328] Loss (name: value) recons: 6.244371944003635\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:41 INFO 139667671275328] Loss (name: value) logppx: 6.476259251435597\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:41 INFO 139667671275328] #quality_metric: host=algo-1, epoch=91, train total_loss <loss>=6.476259251435597\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:41 INFO 139667671275328] patience losses:[6.486954443984562, 6.491149087746938, 6.481370309988658, 6.485215451982286, 6.476872086524963] min patience loss:6.476872086524963 current loss:6.476259251435597 absolute loss difference:0.0006128350893659373\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:41 INFO 139667671275328] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:41 INFO 139667671275328] Timing: train: 1.50s, val: 0.01s, epoch: 1.51s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:41 INFO 139667671275328] #progress_metric: host=algo-1, completed 91.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309140.2273471, \"EndTime\": 1646309141.7347984, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 90, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 411684.0, \"count\": 1, \"min\": 411684, \"max\": 411684}, \"Total Batches Seen\": {\"sum\": 3276.0, \"count\": 1, \"min\": 3276, \"max\": 3276}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 182.0, \"count\": 1, \"min\": 182, \"max\": 182}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:41 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=3000.488346818246 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:41 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:41 INFO 139667671275328] # Starting training for epoch 92\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:05:43.313] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 275, \"duration\": 1578, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:43 INFO 139667671275328] # Finished training epoch 92 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:43 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:43 INFO 139667671275328] Loss (name: value) total: 6.472435653209686\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:43 INFO 139667671275328] Loss (name: value) kld: 0.23240653797984123\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:43 INFO 139667671275328] Loss (name: value) recons: 6.24002910984887\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:43 INFO 139667671275328] Loss (name: value) logppx: 6.472435653209686\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:43 INFO 139667671275328] #quality_metric: host=algo-1, epoch=92, train total_loss <loss>=6.472435653209686\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:43 INFO 139667671275328] patience losses:[6.491149087746938, 6.481370309988658, 6.485215451982286, 6.476872086524963, 6.476259251435597] min patience loss:6.476259251435597 current loss:6.472435653209686 absolute loss difference:0.0038235982259111623\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:43 INFO 139667671275328] Timing: train: 1.58s, val: 0.01s, epoch: 1.59s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:43 INFO 139667671275328] #progress_metric: host=algo-1, completed 92.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309141.7352293, \"EndTime\": 1646309143.322623, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 91, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 416208.0, \"count\": 1, \"min\": 416208, \"max\": 416208}, \"Total Batches Seen\": {\"sum\": 3312.0, \"count\": 1, \"min\": 3312, \"max\": 3312}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 184.0, \"count\": 1, \"min\": 184, \"max\": 184}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:43 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=2849.589426426275 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:43 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:43 INFO 139667671275328] # Starting training for epoch 93\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:05:44.837] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 278, \"duration\": 1514, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:44 INFO 139667671275328] # Finished training epoch 93 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:44 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:44 INFO 139667671275328] Loss (name: value) total: 6.479144944085015\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:44 INFO 139667671275328] Loss (name: value) kld: 0.23512999899685383\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:44 INFO 139667671275328] Loss (name: value) recons: 6.244014945295122\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:44 INFO 139667671275328] Loss (name: value) logppx: 6.479144944085015\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:44 INFO 139667671275328] #quality_metric: host=algo-1, epoch=93, train total_loss <loss>=6.479144944085015\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:44 INFO 139667671275328] patience losses:[6.481370309988658, 6.485215451982286, 6.476872086524963, 6.476259251435597, 6.472435653209686] min patience loss:6.472435653209686 current loss:6.479144944085015 absolute loss difference:0.00670929087532901\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:44 INFO 139667671275328] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:44 INFO 139667671275328] Timing: train: 1.52s, val: 0.00s, epoch: 1.52s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:44 INFO 139667671275328] #progress_metric: host=algo-1, completed 93.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309143.323049, \"EndTime\": 1646309144.8397315, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 92, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 420732.0, \"count\": 1, \"min\": 420732, \"max\": 420732}, \"Total Batches Seen\": {\"sum\": 3348.0, \"count\": 1, \"min\": 3348, \"max\": 3348}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 186.0, \"count\": 1, \"min\": 186, \"max\": 186}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:44 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=2982.462410583252 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:44 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:44 INFO 139667671275328] # Starting training for epoch 94\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:05:46.544] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 281, \"duration\": 1703, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:46 INFO 139667671275328] # Finished training epoch 94 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:46 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:46 INFO 139667671275328] Loss (name: value) total: 6.468611770206028\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:46 INFO 139667671275328] Loss (name: value) kld: 0.23850385348002115\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:46 INFO 139667671275328] Loss (name: value) recons: 6.230107949839698\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:46 INFO 139667671275328] Loss (name: value) logppx: 6.468611770206028\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:46 INFO 139667671275328] #quality_metric: host=algo-1, epoch=94, train total_loss <loss>=6.468611770206028\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:46 INFO 139667671275328] patience losses:[6.485215451982286, 6.476872086524963, 6.476259251435597, 6.472435653209686, 6.479144944085015] min patience loss:6.472435653209686 current loss:6.468611770206028 absolute loss difference:0.003823883003658324\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:46 INFO 139667671275328] Timing: train: 1.71s, val: 0.00s, epoch: 1.71s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:46 INFO 139667671275328] #progress_metric: host=algo-1, completed 94.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309144.840073, \"EndTime\": 1646309146.5500526, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 93, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 425256.0, \"count\": 1, \"min\": 425256, \"max\": 425256}, \"Total Batches Seen\": {\"sum\": 3384.0, \"count\": 1, \"min\": 3384, \"max\": 3384}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 188.0, \"count\": 1, \"min\": 188, \"max\": 188}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:46 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=2644.6902854657956 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:46 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:46 INFO 139667671275328] # Starting training for epoch 95\u001b[0m\n",
      "\n",
      "2022-03-03 12:06:00 Uploading - Uploading generated training model\u001b[34m[2022-03-03 12:05:48.290] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 284, \"duration\": 1736, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:48 INFO 139667671275328] # Finished training epoch 95 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:48 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:48 INFO 139667671275328] Loss (name: value) total: 6.481170939074622\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:48 INFO 139667671275328] Loss (name: value) kld: 0.2381141318215264\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:48 INFO 139667671275328] Loss (name: value) recons: 6.243056800630358\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:48 INFO 139667671275328] Loss (name: value) logppx: 6.481170939074622\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:48 INFO 139667671275328] #quality_metric: host=algo-1, epoch=95, train total_loss <loss>=6.481170939074622\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:48 INFO 139667671275328] patience losses:[6.476872086524963, 6.476259251435597, 6.472435653209686, 6.479144944085015, 6.468611770206028] min patience loss:6.468611770206028 current loss:6.481170939074622 absolute loss difference:0.012559168868594206\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:48 INFO 139667671275328] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:48 INFO 139667671275328] Timing: train: 1.74s, val: 0.00s, epoch: 1.74s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:48 INFO 139667671275328] #progress_metric: host=algo-1, completed 95.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309146.551101, \"EndTime\": 1646309148.2933035, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 94, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 429780.0, \"count\": 1, \"min\": 429780, \"max\": 429780}, \"Total Batches Seen\": {\"sum\": 3420.0, \"count\": 1, \"min\": 3420, \"max\": 3420}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 190.0, \"count\": 1, \"min\": 190, \"max\": 190}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:48 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=2596.32717745973 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:48 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:48 INFO 139667671275328] # Starting training for epoch 96\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:05:49.777] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 287, \"duration\": 1483, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:49 INFO 139667671275328] # Finished training epoch 96 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:49 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:49 INFO 139667671275328] Loss (name: value) total: 6.476196924845378\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:49 INFO 139667671275328] Loss (name: value) kld: 0.24102767039504316\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:49 INFO 139667671275328] Loss (name: value) recons: 6.235169225268894\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:49 INFO 139667671275328] Loss (name: value) logppx: 6.476196924845378\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:49 INFO 139667671275328] #quality_metric: host=algo-1, epoch=96, train total_loss <loss>=6.476196924845378\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:49 INFO 139667671275328] patience losses:[6.476259251435597, 6.472435653209686, 6.479144944085015, 6.468611770206028, 6.481170939074622] min patience loss:6.468611770206028 current loss:6.476196924845378 absolute loss difference:0.007585154639349945\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:49 INFO 139667671275328] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:49 INFO 139667671275328] Timing: train: 1.49s, val: 0.00s, epoch: 1.49s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:49 INFO 139667671275328] #progress_metric: host=algo-1, completed 96.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309148.2937539, \"EndTime\": 1646309149.7828302, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 95, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 434304.0, \"count\": 1, \"min\": 434304, \"max\": 434304}, \"Total Batches Seen\": {\"sum\": 3456.0, \"count\": 1, \"min\": 3456, \"max\": 3456}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 192.0, \"count\": 1, \"min\": 192, \"max\": 192}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:49 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=3037.300112127811 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:49 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:49 INFO 139667671275328] # Starting training for epoch 97\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:05:51.257] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 290, \"duration\": 1472, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:51 INFO 139667671275328] # Finished training epoch 97 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:51 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:51 INFO 139667671275328] Loss (name: value) total: 6.460418158107334\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:51 INFO 139667671275328] Loss (name: value) kld: 0.24174469771484533\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:51 INFO 139667671275328] Loss (name: value) recons: 6.218673480881585\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:51 INFO 139667671275328] Loss (name: value) logppx: 6.460418158107334\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:51 INFO 139667671275328] #quality_metric: host=algo-1, epoch=97, train total_loss <loss>=6.460418158107334\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:51 INFO 139667671275328] patience losses:[6.472435653209686, 6.479144944085015, 6.468611770206028, 6.481170939074622, 6.476196924845378] min patience loss:6.468611770206028 current loss:6.460418158107334 absolute loss difference:0.008193612098693848\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:51 INFO 139667671275328] Timing: train: 1.48s, val: 0.00s, epoch: 1.48s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:51 INFO 139667671275328] #progress_metric: host=algo-1, completed 97.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309149.7836006, \"EndTime\": 1646309151.2637765, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 96, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 438828.0, \"count\": 1, \"min\": 438828, \"max\": 438828}, \"Total Batches Seen\": {\"sum\": 3492.0, \"count\": 1, \"min\": 3492, \"max\": 3492}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 194.0, \"count\": 1, \"min\": 194, \"max\": 194}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:51 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=3055.900139629041 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:51 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:51 INFO 139667671275328] # Starting training for epoch 98\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:41 INFO 140501882455872] # Finished training epoch 94 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:41 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:41 INFO 140501882455872] Loss (name: value) total: 6.519958522584703\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:41 INFO 140501882455872] Loss (name: value) kld: 0.2265371839619345\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:41 INFO 140501882455872] Loss (name: value) recons: 6.293421354558733\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:41 INFO 140501882455872] Loss (name: value) logppx: 6.519958522584703\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:41 INFO 140501882455872] #quality_metric: host=algo-2, epoch=94, train total_loss <loss>=6.519958522584703\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:41 INFO 140501882455872] patience losses:[6.530314021640354, 6.527857184410095, 6.525922331545088, 6.522417724132538, 6.519804265764025] min patience loss:6.519804265764025 current loss:6.519958522584703 absolute loss difference:0.00015425682067871094\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:41 INFO 140501882455872] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:41 INFO 140501882455872] Timing: train: 1.61s, val: 0.00s, epoch: 1.61s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:41 INFO 140501882455872] #progress_metric: host=algo-2, completed 94.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309139.5766637, \"EndTime\": 1646309141.184765, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 93, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 425538.0, \"count\": 1, \"min\": 425538, \"max\": 425538}, \"Total Batches Seen\": {\"sum\": 3384.0, \"count\": 1, \"min\": 3384, \"max\": 3384}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 188.0, \"count\": 1, \"min\": 188, \"max\": 188}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:41 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=2814.8076310884535 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:41 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:41 INFO 140501882455872] # Starting training for epoch 95\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:05:42.735] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 284, \"duration\": 1550, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:42 INFO 140501882455872] # Finished training epoch 95 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:42 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:42 INFO 140501882455872] Loss (name: value) total: 6.518453617890676\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:42 INFO 140501882455872] Loss (name: value) kld: 0.2278466717236572\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:42 INFO 140501882455872] Loss (name: value) recons: 6.290606962309943\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:42 INFO 140501882455872] Loss (name: value) logppx: 6.518453617890676\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:42 INFO 140501882455872] #quality_metric: host=algo-2, epoch=95, train total_loss <loss>=6.518453617890676\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:42 INFO 140501882455872] patience losses:[6.527857184410095, 6.525922331545088, 6.522417724132538, 6.519804265764025, 6.519958522584703] min patience loss:6.519804265764025 current loss:6.518453617890676 absolute loss difference:0.0013506478733491534\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:42 INFO 140501882455872] Timing: train: 1.55s, val: 0.00s, epoch: 1.56s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:42 INFO 140501882455872] #progress_metric: host=algo-2, completed 95.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309141.185087, \"EndTime\": 1646309142.741593, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 94, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 430065.0, \"count\": 1, \"min\": 430065, \"max\": 430065}, \"Total Batches Seen\": {\"sum\": 3420.0, \"count\": 1, \"min\": 3420, \"max\": 3420}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 190.0, \"count\": 1, \"min\": 190, \"max\": 190}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:42 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=2908.0952780474627 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:42 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:42 INFO 140501882455872] # Starting training for epoch 96\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:05:44.269] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 287, \"duration\": 1526, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:44 INFO 140501882455872] # Finished training epoch 96 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:44 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:44 INFO 140501882455872] Loss (name: value) total: 6.518418596850501\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:44 INFO 140501882455872] Loss (name: value) kld: 0.23149977210495207\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:44 INFO 140501882455872] Loss (name: value) recons: 6.286918838818868\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:44 INFO 140501882455872] Loss (name: value) logppx: 6.518418596850501\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:44 INFO 140501882455872] #quality_metric: host=algo-2, epoch=96, train total_loss <loss>=6.518418596850501\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:44 INFO 140501882455872] patience losses:[6.525922331545088, 6.522417724132538, 6.519804265764025, 6.519958522584703, 6.518453617890676] min patience loss:6.518453617890676 current loss:6.518418596850501 absolute loss difference:3.502104017449881e-05\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:44 INFO 140501882455872] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:44 INFO 140501882455872] Timing: train: 1.53s, val: 0.01s, epoch: 1.53s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:44 INFO 140501882455872] #progress_metric: host=algo-2, completed 96.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309142.741955, \"EndTime\": 1646309144.2758546, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 95, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 434592.0, \"count\": 1, \"min\": 434592, \"max\": 434592}, \"Total Batches Seen\": {\"sum\": 3456.0, \"count\": 1, \"min\": 3456, \"max\": 3456}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 192.0, \"count\": 1, \"min\": 192, \"max\": 192}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:44 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=2950.867586062633 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:44 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:44 INFO 140501882455872] # Starting training for epoch 97\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:05:45.966] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 290, \"duration\": 1690, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:45 INFO 140501882455872] # Finished training epoch 97 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:45 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:45 INFO 140501882455872] Loss (name: value) total: 6.514339069525401\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:45 INFO 140501882455872] Loss (name: value) kld: 0.23166686379247242\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:45 INFO 140501882455872] Loss (name: value) recons: 6.282672199938032\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:45 INFO 140501882455872] Loss (name: value) logppx: 6.514339069525401\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:45 INFO 140501882455872] #quality_metric: host=algo-2, epoch=97, train total_loss <loss>=6.514339069525401\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:45 INFO 140501882455872] patience losses:[6.522417724132538, 6.519804265764025, 6.519958522584703, 6.518453617890676, 6.518418596850501] min patience loss:6.518418596850501 current loss:6.514339069525401 absolute loss difference:0.004079527325099974\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:45 INFO 140501882455872] Timing: train: 1.69s, val: 0.00s, epoch: 1.70s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:45 INFO 140501882455872] #progress_metric: host=algo-2, completed 97.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309144.2762425, \"EndTime\": 1646309145.973595, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 96, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 439119.0, \"count\": 1, \"min\": 439119, \"max\": 439119}, \"Total Batches Seen\": {\"sum\": 3492.0, \"count\": 1, \"min\": 3492, \"max\": 3492}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 194.0, \"count\": 1, \"min\": 194, \"max\": 194}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:45 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=2666.8143558651514 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:45 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:45 INFO 140501882455872] # Starting training for epoch 98\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:05:47.634] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 293, \"duration\": 1659, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:47 INFO 140501882455872] # Finished training epoch 98 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:47 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:47 INFO 140501882455872] Loss (name: value) total: 6.510474933518304\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:47 INFO 140501882455872] Loss (name: value) kld: 0.2316855794439713\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:47 INFO 140501882455872] Loss (name: value) recons: 6.2787893414497375\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:47 INFO 140501882455872] Loss (name: value) logppx: 6.510474933518304\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:47 INFO 140501882455872] #quality_metric: host=algo-2, epoch=98, train total_loss <loss>=6.510474933518304\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:47 INFO 140501882455872] patience losses:[6.519804265764025, 6.519958522584703, 6.518453617890676, 6.518418596850501, 6.514339069525401] min patience loss:6.514339069525401 current loss:6.510474933518304 absolute loss difference:0.0038641360070972297\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:47 INFO 140501882455872] Timing: train: 1.66s, val: 0.01s, epoch: 1.67s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:47 INFO 140501882455872] #progress_metric: host=algo-2, completed 98.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309145.9743283, \"EndTime\": 1646309147.6410038, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 97, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 443646.0, \"count\": 1, \"min\": 443646, \"max\": 443646}, \"Total Batches Seen\": {\"sum\": 3528.0, \"count\": 1, \"min\": 3528, \"max\": 3528}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 196.0, \"count\": 1, \"min\": 196, \"max\": 196}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:47 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=2715.779130069126 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:47 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:47 INFO 140501882455872] # Starting training for epoch 99\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:05:49.155] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 296, \"duration\": 1512, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:49 INFO 140501882455872] # Finished training epoch 99 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:49 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:49 INFO 140501882455872] Loss (name: value) total: 6.518565793832143\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:49 INFO 140501882455872] Loss (name: value) kld: 0.2353427457726664\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:49 INFO 140501882455872] Loss (name: value) recons: 6.283223066065046\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:49 INFO 140501882455872] Loss (name: value) logppx: 6.518565793832143\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:49 INFO 140501882455872] #quality_metric: host=algo-2, epoch=99, train total_loss <loss>=6.518565793832143\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:49 INFO 140501882455872] patience losses:[6.519958522584703, 6.518453617890676, 6.518418596850501, 6.514339069525401, 6.510474933518304] min patience loss:6.510474933518304 current loss:6.518565793832143 absolute loss difference:0.008090860313838988\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:49 INFO 140501882455872] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:49 INFO 140501882455872] Timing: train: 1.52s, val: 0.00s, epoch: 1.52s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:49 INFO 140501882455872] #progress_metric: host=algo-2, completed 99.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309147.6416883, \"EndTime\": 1646309149.157766, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 98, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 448173.0, \"count\": 1, \"min\": 448173, \"max\": 448173}, \"Total Batches Seen\": {\"sum\": 3564.0, \"count\": 1, \"min\": 3564, \"max\": 3564}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 198.0, \"count\": 1, \"min\": 198, \"max\": 198}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:49 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=2985.6274383918794 records/second\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:49 INFO 140501882455872] \u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:49 INFO 140501882455872] # Starting training for epoch 100\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:05:50.790] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 299, \"duration\": 1632, \"num_examples\": 36, \"num_bytes\": 1074984}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:50 INFO 140501882455872] # Finished training epoch 100 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:50 INFO 140501882455872] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:50 INFO 140501882455872] Loss (name: value) total: 6.518793774975671\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:50 INFO 140501882455872] Loss (name: value) kld: 0.2381430524918768\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:50 INFO 140501882455872] Loss (name: value) recons: 6.280650715033214\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:50 INFO 140501882455872] Loss (name: value) logppx: 6.518793774975671\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:50 INFO 140501882455872] #quality_metric: host=algo-2, epoch=100, train total_loss <loss>=6.518793774975671\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:50 INFO 140501882455872] patience losses:[6.518453617890676, 6.518418596850501, 6.514339069525401, 6.510474933518304, 6.518565793832143] min patience loss:6.510474933518304 current loss:6.518793774975671 absolute loss difference:0.008318841457366943\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:50 INFO 140501882455872] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:50 INFO 140501882455872] Timing: train: 1.63s, val: 0.00s, epoch: 1.63s\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:50 INFO 140501882455872] #progress_metric: host=algo-2, completed 100.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309149.1580799, \"EndTime\": 1646309150.7923968, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 99, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 452700.0, \"count\": 1, \"min\": 452700, \"max\": 452700}, \"Total Batches Seen\": {\"sum\": 3600.0, \"count\": 1, \"min\": 3600, \"max\": 3600}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 200.0, \"count\": 1, \"min\": 200, \"max\": 200}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:50 INFO 140501882455872] #throughput_metric: host=algo-2, train throughput=2769.6332754831715 records/second\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:05:52.398] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 293, \"duration\": 1130, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:52 INFO 139667671275328] # Finished training epoch 98 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:52 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:52 INFO 139667671275328] Loss (name: value) total: 6.447542640897963\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:52 INFO 139667671275328] Loss (name: value) kld: 0.2428191676735878\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:52 INFO 139667671275328] Loss (name: value) recons: 6.2047234707408485\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:52 INFO 139667671275328] Loss (name: value) logppx: 6.447542640897963\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:52 INFO 139667671275328] #quality_metric: host=algo-1, epoch=98, train total_loss <loss>=6.447542640897963\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:52 INFO 139667671275328] patience losses:[6.479144944085015, 6.468611770206028, 6.481170939074622, 6.476196924845378, 6.460418158107334] min patience loss:6.460418158107334 current loss:6.447542640897963 absolute loss difference:0.012875517209371523\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:52 INFO 139667671275328] Timing: train: 1.14s, val: 0.01s, epoch: 1.14s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:52 INFO 139667671275328] #progress_metric: host=algo-1, completed 98.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309151.2641857, \"EndTime\": 1646309152.4067035, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 97, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 443352.0, \"count\": 1, \"min\": 443352, \"max\": 443352}, \"Total Batches Seen\": {\"sum\": 3528.0, \"count\": 1, \"min\": 3528, \"max\": 3528}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 196.0, \"count\": 1, \"min\": 196, \"max\": 196}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:52 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=3958.928876473486 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:52 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:52 INFO 139667671275328] # Starting training for epoch 99\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:05:53.554] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 296, \"duration\": 1147, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:53 INFO 139667671275328] # Finished training epoch 99 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:53 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:53 INFO 139667671275328] Loss (name: value) total: 6.432647479905023\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:53 INFO 139667671275328] Loss (name: value) kld: 0.24245855170819494\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:53 INFO 139667671275328] Loss (name: value) recons: 6.190188844998677\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:53 INFO 139667671275328] Loss (name: value) logppx: 6.432647479905023\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:53 INFO 139667671275328] #quality_metric: host=algo-1, epoch=99, train total_loss <loss>=6.432647479905023\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:53 INFO 139667671275328] patience losses:[6.468611770206028, 6.481170939074622, 6.476196924845378, 6.460418158107334, 6.447542640897963] min patience loss:6.447542640897963 current loss:6.432647479905023 absolute loss difference:0.014895160992939971\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:53 INFO 139667671275328] Timing: train: 1.15s, val: 0.01s, epoch: 1.15s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:53 INFO 139667671275328] #progress_metric: host=algo-1, completed 99.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309152.4070919, \"EndTime\": 1646309153.561597, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 98, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 447876.0, \"count\": 1, \"min\": 447876, \"max\": 447876}, \"Total Batches Seen\": {\"sum\": 3564.0, \"count\": 1, \"min\": 3564, \"max\": 3564}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 198.0, \"count\": 1, \"min\": 198, \"max\": 198}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:53 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=3917.8399442686896 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:53 INFO 139667671275328] \u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:53 INFO 139667671275328] # Starting training for epoch 100\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:54 INFO 140501882455872] Best model based on early stopping at epoch 98. Best loss: 6.510474933518304\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:05:54.709] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 299, \"duration\": 1146, \"num_examples\": 36, \"num_bytes\": 1017788}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:54 INFO 139667671275328] # Finished training epoch 100 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:54 INFO 139667671275328] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:54 INFO 139667671275328] Loss (name: value) total: 6.428928838835822\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:54 INFO 139667671275328] Loss (name: value) kld: 0.2467899312161737\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:54 INFO 139667671275328] Loss (name: value) recons: 6.182138899962108\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:54 INFO 139667671275328] Loss (name: value) logppx: 6.428928838835822\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:54 INFO 139667671275328] #quality_metric: host=algo-1, epoch=100, train total_loss <loss>=6.428928838835822\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:54 INFO 139667671275328] patience losses:[6.481170939074622, 6.476196924845378, 6.460418158107334, 6.447542640897963, 6.432647479905023] min patience loss:6.432647479905023 current loss:6.428928838835822 absolute loss difference:0.003718641069200501\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:54 INFO 139667671275328] Timing: train: 1.15s, val: 0.01s, epoch: 1.16s\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:54 INFO 139667671275328] #progress_metric: host=algo-1, completed 100.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309153.562006, \"EndTime\": 1646309154.7196748, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 99, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 452400.0, \"count\": 1, \"min\": 452400, \"max\": 452400}, \"Total Batches Seen\": {\"sum\": 3600.0, \"count\": 1, \"min\": 3600, \"max\": 3600}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 200.0, \"count\": 1, \"min\": 200, \"max\": 200}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:54 INFO 139667671275328] #throughput_metric: host=algo-1, train throughput=3906.8210400959624 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:54 INFO 139667671275328] Best model based on early stopping at epoch 100. Best loss: 6.428928838835822\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:54 INFO 140501882455872] Topics from epoch:final (num_topics:30) [, tu 0.60]:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:54 INFO 140501882455872] 48 1531 1547 1990 1823 1838 689 1171 367 819 704 1283 1763 1951 172 1429 781 69 916 1126\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:54 INFO 140501882455872] 1526 1141 527 1819 783 511 1713 642 1445 1993 1729 257 1427 246 414 1216 1128 1016 1934 185\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:54 INFO 140501882455872] 225 1651 1420 1725 1495 1149 439 383 726 1629 1636 196 1732 668 310 1216 511 1986 1097 1454\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:54 INFO 140501882455872] 341 1751 1141 513 1776 132 642 714 939 682 1713 1729 1528 146 153 540 933 1516 286 1499\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:54 INFO 140501882455872] 1575 1312 476 1174 1813 1574 1783 232 1826 1294 713 961 808 1584 201 192 314 674 1293 1098\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:54 INFO 140501882455872] 1327 681 832 891 1231 1366 1131 1992 1768 918 1109 1906 673 449 1955 877 378 1614 177 230\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:54 INFO 140501882455872] 409 309 1960 66 1285 1547 1592 1031 557 1588 1327 480 410 287 558 355 1111 1724 1265 1697\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:54 INFO 140501882455872] 68 530 1339 1724 952 527 1169 1386 1238 1739 511 246 1216 1909 1547 546 762 1091 1207 1414\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:54 INFO 140501882455872] 409 1724 1285 527 1497 762 386 68 145 417 1111 1448 1447 100 1683 1540 792 704 225 156\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:54 INFO 140501882455872] 952 527 642 1819 939 1713 1141 417 1516 194 1776 439 1751 1445 153 1672 714 1131 1366 246\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:54 INFO 140501882455872] 1145 1856 1121 1486 906 1487 721 1412 1122 893 892 896 88 3 929 163 91 904 895 97\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:54 INFO 140501882455872] 112 1216 113 1122 1196 31 1801 310 1736 1318 1896 1693 693 600 832 1160 48 504 1303 1969\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:54 INFO 140501882455872] 1298 730 1022 1137 731 421 1024 0 136 1418 1893 976 1831 134 137 1147 170 1419 1060 135\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:54 INFO 140501882455872] 1046 1196 380 1318 1880 1216 726 1992 689 1754 203 1990 112 819 635 493 798 978 1809 490\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:54 INFO 140501882455872] 174 225 751 1526 1969 1699 511 1497 1030 183 71 325 200 1683 1602 1110 673 1360 762 1414\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:54 INFO 140501882455872] 726 803 1318 31 704 1656 1801 493 1969 82 798 1720 689 225 795 635 83 1046 137 1216\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:54 INFO 140501882455872] 741 1765 145 1260 788 584 499 1904 1679 1141 904 1324 1242 639 508 291 1654 1701 1578 812\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:54 INFO 140501882455872] 409 527 309 278 246 547 1371 681 1592 1366 1724 1369 1602 1931 1697 1547 1285 1327 649 1614\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:54 INFO 140501882455872] 715 225 31 527 1526 1128 717 1699 174 716 1683 1366 1801 229 1724 732 955 1602 926 1969\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:54 INFO 140501882455872] 1092 1815 734 1432 198 245 814 767 154 1306 562 1211 439 1855 761 1454 955 1959 526 474\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:54 INFO 140501882455872] 642 513 1582 1776 1528 714 1526 142 341 540 153 939 682 257 1751 1993 219 1131 1722 174\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:54 INFO 140501882455872] 651 1420 1616 383 1776 1159 73 939 341 174 1416 1445 1819 1751 1528 1726 64 1149 1526 1651\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:54 INFO 140501882455872] 225 1969 34 71 1128 1526 1019 1801 1486 783 183 1676 861 527 1724 1538 935 1216 1214 1365\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:54 INFO 140501882455872] 704 1656 527 1031 89 1538 1969 31 751 795 493 1046 82 69 71 1725 225 756 861 1523\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:54 INFO 140501882455872] 1128 803 225 1651 1447 955 110 1748 83 640 1724 1741 1420 693 458 1216 377 1805 1185 1759\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:54 INFO 140501882455872] 225 1993 174 183 1702 439 528 1528 1526 1210 82 668 955 1211 1391 1141 1881 1117 642 31\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:54 INFO 140501882455872] 225 154 1208 59 174 861 34 1110 1000 1614 511 1112 1943 1496 71 539 287 1629 1029 1534\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:54 INFO 140501882455872] 1751 527 1993 1141 642 1729 156 682 939 1776 1582 1131 681 1656 341 484 952 1713 1365 1722\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:54 INFO 140501882455872] 547 681 527 704 68 952 1170 89 880 556 1656 171 246 145 71 1031 1366 1724 1118 849\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:54 INFO 140501882455872] 1556 1636 1205 104 350 595 853 1602 546 641 1780 200 1491 1601 695 1833 1241 58 852 1360\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:54 INFO 140501882455872] Serializing model to /opt/ml/model/model_algo-2\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:54 INFO 140501882455872] Saved checkpoint to \"/tmp/tmp9xkipuh0/state-0001.params\"\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:05:54.872] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/test\", \"epoch\": 0, \"duration\": 164266, \"num_examples\": 1, \"num_bytes\": 32048}\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:05:55.029] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/test\", \"epoch\": 1, \"duration\": 156, \"num_examples\": 18, \"num_bytes\": 529764}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:55 INFO 140501882455872] Finished scoring on 2176 examples from 17 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:55 INFO 140501882455872] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:55 INFO 140501882455872] Loss (name: value) total: 6.686145417830524\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:55 INFO 140501882455872] Loss (name: value) kld: 0.2191873385625727\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:55 INFO 140501882455872] Loss (name: value) recons: 6.466958045959473\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:55 INFO 140501882455872] Loss (name: value) logppx: 6.686145417830524\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646309154.872407, \"EndTime\": 1646309155.0308766, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"Meta\": \"test_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 2263.0, \"count\": 1, \"min\": 2263, \"max\": 2263}, \"Total Batches Seen\": {\"sum\": 18.0, \"count\": 1, \"min\": 18, \"max\": 18}, \"Max Records Seen Between Resets\": {\"sum\": 2263.0, \"count\": 1, \"min\": 2263, \"max\": 2263}, \"Max Batches Seen Between Resets\": {\"sum\": 18.0, \"count\": 1, \"min\": 18, \"max\": 18}, \"Reset Count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Number of Records Since Last Reset\": {\"sum\": 2263.0, \"count\": 1, \"min\": 2263, \"max\": 2263}, \"Number of Batches Since Last Reset\": {\"sum\": 18.0, \"count\": 1, \"min\": 18, \"max\": 18}}}\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:05:55 INFO 140501882455872] #test_score (algo-2) : ('log_perplexity', 6.686145417830524)\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646308990.5979218, \"EndTime\": 1646309155.0322356, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\"}, \"Metrics\": {\"initialize.time\": {\"sum\": 4510.171890258789, \"count\": 1, \"min\": 4510.171890258789, \"max\": 4510.171890258789}, \"epochs\": {\"sum\": 100.0, \"count\": 1, \"min\": 100, \"max\": 100}, \"early_stop.time\": {\"sum\": 355.78274726867676, \"count\": 100, \"min\": 0.2002716064453125, \"max\": 8.347749710083008}, \"update.time\": {\"sum\": 155595.8981513977, \"count\": 100, \"min\": 1409.3546867370605, \"max\": 1943.1662559509277}, \"finalize.time\": {\"sum\": 145.35284042358398, \"count\": 1, \"min\": 145.35284042358398, \"max\": 145.35284042358398}, \"model.serialize.time\": {\"sum\": 6.124973297119141, \"count\": 1, \"min\": 6.124973297119141, \"max\": 6.124973297119141}, \"model.score.time\": {\"sum\": 158.3869457244873, \"count\": 1, \"min\": 158.3869457244873, \"max\": 158.3869457244873}, \"setuptime\": {\"sum\": 5601.370573043823, \"count\": 1, \"min\": 5601.370573043823, \"max\": 5601.370573043823}, \"totaltime\": {\"sum\": 170075.5467414856, \"count\": 1, \"min\": 170075.5467414856, \"max\": 170075.5467414856}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:54 INFO 139667671275328] Topics from epoch:final (num_topics:30) [, tu 0.60]:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:54 INFO 139667671275328] 48 1838 1531 1990 1547 1823 1171 367 1429 704 1283 689 916 1763 781 1126 858 172 1656 1720\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:54 INFO 139667671275328] 1141 1526 185 1427 527 1445 1935 183 511 1510 1216 1819 1986 642 196 1516 1807 722 1170 1724\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:54 INFO 139667671275328] 225 1725 1651 174 1420 381 1210 1760 1216 1527 717 1528 1708 1411 795 1969 224 33 1977 1331\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:54 INFO 139667671275328] 341 1751 1141 714 513 132 1776 642 939 682 153 1713 286 1499 1445 1516 1563 257 1729 540\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:54 INFO 139667671275328] 1575 476 1312 1174 1574 1813 1783 1826 713 232 961 314 1294 674 1584 201 808 192 1293 1119\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:54 INFO 139667671275328] 1327 681 832 1231 891 1992 1366 1768 1131 449 1955 918 1906 1109 673 1614 1125 230 1907 877\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:54 INFO 139667671275328] 66 287 429 409 1285 1547 1592 379 1931 724 1031 557 1960 681 1366 1111 1186 309 1265 1118\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:54 INFO 139667671275328] 68 527 1386 704 941 1724 952 1414 246 681 417 1216 1909 949 530 1184 511 1968 156 71\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:54 INFO 139667671275328] 156 1724 527 1285 409 971 1592 386 861 83 171 641 1118 1111 120 292 1089 798 1216 704\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:54 INFO 139667671275328] 939 952 527 642 417 1141 1776 196 682 1445 808 1751 681 511 199 1170 1928 1819 933 341\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:54 INFO 139667671275328] 1145 1121 1856 1486 906 1487 721 893 892 1122 3 896 1412 1244 88 91 163 904 1858 929\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:54 INFO 139667671275328] 112 31 1196 1216 83 113 1160 1801 391 225 600 1318 1122 1548 1693 751 832 861 381 163\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:54 INFO 139667671275328] 1298 1418 421 1893 731 730 136 1022 1137 1024 0 1831 134 137 1060 976 1964 1146 1147 1419\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:54 INFO 139667671275328] 112 1046 689 203 31 1801 1216 1318 380 1992 89 82 137 113 225 1831 312 134 1725 798\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:54 INFO 139667671275328] 225 71 174 1969 183 1497 1414 1637 1464 1526 989 1683 795 1007 1765 1001 1789 703 861 784\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:54 INFO 139667671275328] 31 1318 1801 726 493 798 803 861 916 751 1656 704 82 1216 137 1795 1831 134 466 1969\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:54 INFO 139667671275328] 741 1765 145 584 788 1260 1141 1904 1679 499 1324 904 639 508 1242 1654 291 1578 1701 1071\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:54 INFO 139667671275328] 409 1366 1285 120 246 681 1031 174 71 31 1724 1186 278 287 309 1850 1592 547 1672 1931\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:54 INFO 139667671275328] 225 1526 83 527 1602 174 34 1216 1366 1724 939 1497 732 31 715 1102 1616 1026 608 1335\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:54 INFO 139667671275328] 1815 734 1432 198 1092 245 154 814 767 1211 1306 1855 562 439 1454 761 955 1959 59 526\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:54 INFO 139667671275328] 513 642 341 1141 1528 1776 939 1249 1582 1751 1993 714 1526 682 153 142 1909 1527 527 484\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:54 INFO 139667671275328] 1420 651 1210 1776 73 925 1141 225 1726 119 762 1729 593 1128 528 926 513 68 147 1728\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:54 INFO 139667671275328] 34 225 1128 1538 174 527 246 952 145 1216 1679 171 183 1724 1050 1909 1765 1162 31 1717\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:54 INFO 139667671275328] 704 527 1538 31 1656 1969 1031 71 861 952 246 89 1007 798 1523 1486 1801 156 1680 1216\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:54 INFO 139667671275328] 333 1420 458 511 1128 35 1724 1651 1216 1235 693 1839 1029 225 649 374 670 431 23 640\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:54 INFO 139667671275328] 225 439 1526 183 174 955 224 29 751 1651 1128 397 194 1211 511 517 1805 1117 642 1751\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:54 INFO 139667671275328] 225 71 34 1208 1016 183 1943 1175 511 528 798 154 1614 527 1110 174 350 1135 1348 1285\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:54 INFO 139667671275328] 1751 1141 1366 527 156 1365 704 642 1656 939 484 1729 672 1993 1905 682 1776 1101 132 714\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:54 INFO 139667671275328] 681 952 527 246 1118 71 530 145 704 68 83 1285 547 607 1462 964 1260 1765 861 1170\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:54 INFO 139667671275328] 1556 1636 104 1205 350 853 595 1602 641 546 1780 695 200 1833 58 759 1491 1601 852 1241\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:54 INFO 139667671275328] Serializing model to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:54 INFO 139667671275328] Saved checkpoint to \"/tmp/tmphluz697n/state-0001.params\"\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:05:54.895] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/test\", \"epoch\": 0, \"duration\": 160650, \"num_examples\": 1, \"num_bytes\": 32048}\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:05:55.042] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/test\", \"epoch\": 1, \"duration\": 147, \"num_examples\": 18, \"num_bytes\": 529764}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:55 INFO 139667671275328] Finished scoring on 2176 examples from 17 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:55 INFO 139667671275328] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:55 INFO 139667671275328] Loss (name: value) total: 6.684688596164479\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:55 INFO 139667671275328] Loss (name: value) kld: 0.22393249588854172\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:55 INFO 139667671275328] Loss (name: value) recons: 6.460756021387437\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:55 INFO 139667671275328] Loss (name: value) logppx: 6.684688596164479\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646309154.8947847, \"EndTime\": 1646309155.0436084, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"Meta\": \"test_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 2263.0, \"count\": 1, \"min\": 2263, \"max\": 2263}, \"Total Batches Seen\": {\"sum\": 18.0, \"count\": 1, \"min\": 18, \"max\": 18}, \"Max Records Seen Between Resets\": {\"sum\": 2263.0, \"count\": 1, \"min\": 2263, \"max\": 2263}, \"Max Batches Seen Between Resets\": {\"sum\": 18.0, \"count\": 1, \"min\": 18, \"max\": 18}, \"Reset Count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Number of Records Since Last Reset\": {\"sum\": 2263.0, \"count\": 1, \"min\": 2263, \"max\": 2263}, \"Number of Batches Since Last Reset\": {\"sum\": 18.0, \"count\": 1, \"min\": 18, \"max\": 18}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:05:55 INFO 139667671275328] #test_score (algo-1) : ('log_perplexity', 6.684688596164479)\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646308994.2263265, \"EndTime\": 1646309155.04515, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"initialize.time\": {\"sum\": 871.1204528808594, \"count\": 1, \"min\": 871.1204528808594, \"max\": 871.1204528808594}, \"epochs\": {\"sum\": 100.0, \"count\": 1, \"min\": 100, \"max\": 100}, \"early_stop.time\": {\"sum\": 383.1648826599121, \"count\": 100, \"min\": 0.2002716064453125, \"max\": 9.244918823242188}, \"update.time\": {\"sum\": 159499.08542633057, \"count\": 100, \"min\": 1142.2398090362549, \"max\": 2174.5893955230713}, \"finalize.time\": {\"sum\": 167.28568077087402, \"count\": 1, \"min\": 167.28568077087402, \"max\": 167.28568077087402}, \"model.serialize.time\": {\"sum\": 6.154775619506836, \"count\": 1, \"min\": 6.154775619506836, \"max\": 6.154775619506836}, \"model.score.time\": {\"sum\": 148.7424373626709, \"count\": 1, \"min\": 148.7424373626709, \"max\": 148.7424373626709}, \"setuptime\": {\"sum\": 88.27638626098633, \"count\": 1, \"min\": 88.27638626098633, \"max\": 88.27638626098633}, \"totaltime\": {\"sum\": 160972.74541854858, \"count\": 1, \"min\": 160972.74541854858, \"max\": 160972.74541854858}}}\u001b[0m\n",
      "\n",
      "2022-03-03 12:06:23 Completed - Training job completed\n",
      "ProfilerReport-1646308639: NoIssuesFound\n",
      "Training seconds: 494\n",
      "Billable seconds: 212\n",
      "Managed Spot Training savings: 57.1%\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.session import s3_input\n",
    "\n",
    "s3_train = s3_input(s3_train_data, distribution='ShardedByS3Key')\n",
    "\n",
    "'''\n",
    "#OR\n",
    "\n",
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "s3_train = TrainingInput(s3_train_data, distribution=\"ShardedByS3Key\")\n",
    "'''\n",
    "ntm.fit({'train': s3_train, 'test': s3_val_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70938a75",
   "metadata": {},
   "source": [
    "Success! You've trained your topic model with the NTM algorithm.\n",
    "\n",
    "In the next step, you deploy your model to Amazon Sagemaker hosting services."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d2de1a",
   "metadata": {},
   "source": [
    "### 2. Deploy the topic model\n",
    "\n",
    "A trained model by itself is simply a tar file consisting of the model weights and does nothing on its own. To make the model useful and get predictions, you need to deploy the model.\n",
    "\n",
    "There are two ways to deploy the model in Amazon SageMaker, depending on how you want to generate inferences:\n",
    "\n",
    "* To get one inference at a time, set up a persistent endpoint using Amazon SageMaker hosting services.\n",
    "* To get inferences for an entire dataset, use Amazon SageMaker batch transform.\n",
    "This lab provides both options for you to choose the best approach for your use case.\n",
    "\n",
    "#### 2.1. Option A - Deploy the topic model with Amazon SageMaker hosting services\n",
    "\n",
    "In the case of Amazon SageMaker hosting services, a live HTTPs endpoint lives on an Amazon EC2 instance that you can pass a payload to and obtain inferences.\n",
    "\n",
    "When you deploy the model, you call the deploy method of the sagemaker.estimator.Estimator object. When you call the deploy method, you specify the number and type of ML instances that you want to use to host the endpoint.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb80801a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------!"
     ]
    }
   ],
   "source": [
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "ntm_predictor = ntm.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge',\n",
    "    serializer=CSVSerializer(),\n",
    "    deserializer=JSONDeserializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2866317f",
   "metadata": {},
   "source": [
    "The deploy method creates the deployable model, configures the Amazon SageMaker hosting services endpoint, and launches the endpoint to host the model.\n",
    "\n",
    "To run inferences against an endpoint, you need to ensure that the input payload is serialized in a format that the trained model can read, and the inference output is deserialized into a human readable format. In the following code, you use a **csv_serializer** and a **json_deserializer** which passes CSV formatted data (as vectors) to the model to produce JSON output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b10ce5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sagemaker.predictor import csv_serializer, json_deserializer\n",
    "\n",
    "#ntm_predictor.content_type = 'text/csv'\n",
    "#ntm_predictor.serializer = csv_serializer\n",
    "#ntm_predictor.deserializer = json_deserializer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e48a3a2",
   "metadata": {},
   "source": [
    "Next, extract the topic vectors for the training data that you will use in the K-NN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c5e1c6d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for item in np.array(vectors.todense()):\n",
    "    np.shape(item)\n",
    "    results = ntm_predictor.predict(item)\n",
    "    predictions.append(np.array([prediction['topic_weights'] for prediction in results['predictions']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "22870fc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 4, 4, ..., 3, 1, 8])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fetch_20newsgroups(subset='train').target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a7868af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels=fetch_20newsgroups(subset='train').target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "80a4ab05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories=fetch_20newsgroups(subset='train')['target_names']\n",
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7705d3b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(topicvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dcd96f8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.65040761e-310, -1.02950849e+085,  3.86445972e-128, ...,\n",
       "        1.30355081e-076,  7.57515017e-096,  7.57515017e-096])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topicvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "70e93ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.array([np.ndarray.flatten(x) for x in predictions])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c05076ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "topicvec = train_labels[newidx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5cd8e5a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11, 16,  8, ...,  3,  2, 14])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topicvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6032021a",
   "metadata": {},
   "outputs": [],
   "source": [
    "topicnames = [categories[x] for x in topicvec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "516ebe86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['talk.politics.guns',\n",
       " 'sci.space',\n",
       " 'rec.sport.hockey',\n",
       " 'soc.religion.christian',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'alt.atheism',\n",
       " 'rec.motorcycles',\n",
       " 'talk.politics.misc',\n",
       " 'sci.crypt',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.electronics',\n",
       " 'sci.electronics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.graphics',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'sci.med',\n",
       " 'soc.religion.christian',\n",
       " 'sci.med',\n",
       " 'talk.politics.guns',\n",
       " 'sci.crypt',\n",
       " 'alt.atheism',\n",
       " 'sci.crypt',\n",
       " 'talk.politics.guns',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'talk.politics.mideast',\n",
       " 'soc.religion.christian',\n",
       " 'talk.religion.misc',\n",
       " 'alt.atheism',\n",
       " 'soc.religion.christian',\n",
       " 'sci.electronics',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'sci.space',\n",
       " 'comp.graphics',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'talk.politics.misc',\n",
       " 'talk.politics.guns',\n",
       " 'rec.sport.baseball',\n",
       " 'alt.atheism',\n",
       " 'rec.motorcycles',\n",
       " 'sci.crypt',\n",
       " 'rec.motorcycles',\n",
       " 'misc.forsale',\n",
       " 'talk.politics.guns',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'soc.religion.christian',\n",
       " 'rec.sport.baseball',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.graphics',\n",
       " 'rec.sport.baseball',\n",
       " 'sci.med',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'sci.crypt',\n",
       " 'rec.autos',\n",
       " 'talk.politics.guns',\n",
       " 'soc.religion.christian',\n",
       " 'rec.sport.baseball',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.misc',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.autos',\n",
       " 'talk.politics.guns',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'talk.religion.misc',\n",
       " 'rec.sport.baseball',\n",
       " 'talk.politics.guns',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'talk.politics.mideast',\n",
       " 'rec.motorcycles',\n",
       " 'sci.electronics',\n",
       " 'misc.forsale',\n",
       " 'soc.religion.christian',\n",
       " 'sci.crypt',\n",
       " 'misc.forsale',\n",
       " 'talk.politics.misc',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'sci.med',\n",
       " 'comp.windows.x',\n",
       " 'talk.politics.guns',\n",
       " 'comp.windows.x',\n",
       " 'talk.politics.misc',\n",
       " 'rec.motorcycles',\n",
       " 'sci.crypt',\n",
       " 'sci.crypt',\n",
       " 'talk.politics.guns',\n",
       " 'sci.crypt',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'misc.forsale',\n",
       " 'comp.windows.x',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'soc.religion.christian',\n",
       " 'sci.electronics',\n",
       " 'rec.sport.hockey',\n",
       " 'comp.graphics',\n",
       " 'rec.motorcycles',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'sci.electronics',\n",
       " 'misc.forsale',\n",
       " 'comp.graphics',\n",
       " 'sci.electronics',\n",
       " 'comp.graphics',\n",
       " 'sci.space',\n",
       " 'talk.religion.misc',\n",
       " 'talk.politics.guns',\n",
       " 'misc.forsale',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc',\n",
       " 'talk.politics.misc',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.hockey',\n",
       " 'rec.motorcycles',\n",
       " 'sci.med',\n",
       " 'comp.graphics',\n",
       " 'talk.politics.guns',\n",
       " 'sci.crypt',\n",
       " 'rec.motorcycles',\n",
       " 'sci.crypt',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'misc.forsale',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'sci.crypt',\n",
       " 'misc.forsale',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'sci.electronics',\n",
       " 'comp.graphics',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'misc.forsale',\n",
       " 'comp.windows.x',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'talk.religion.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'talk.religion.misc',\n",
       " 'rec.autos',\n",
       " 'comp.windows.x',\n",
       " 'sci.space',\n",
       " 'sci.med',\n",
       " 'talk.politics.guns',\n",
       " 'sci.crypt',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.mideast',\n",
       " 'alt.atheism',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'sci.space',\n",
       " 'sci.med',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'sci.electronics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'soc.religion.christian',\n",
       " 'sci.space',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'talk.politics.guns',\n",
       " 'alt.atheism',\n",
       " 'sci.crypt',\n",
       " 'rec.autos',\n",
       " 'sci.electronics',\n",
       " 'talk.religion.misc',\n",
       " 'comp.graphics',\n",
       " 'rec.sport.baseball',\n",
       " 'alt.atheism',\n",
       " 'rec.sport.baseball',\n",
       " 'sci.med',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.windows.x',\n",
       " 'rec.sport.baseball',\n",
       " 'sci.med',\n",
       " 'alt.atheism',\n",
       " 'rec.autos',\n",
       " 'talk.politics.mideast',\n",
       " 'misc.forsale',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'rec.sport.baseball',\n",
       " 'comp.windows.x',\n",
       " 'rec.sport.baseball',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'talk.politics.mideast',\n",
       " 'sci.electronics',\n",
       " 'misc.forsale',\n",
       " 'talk.religion.misc',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.windows.x',\n",
       " 'rec.motorcycles',\n",
       " 'talk.politics.misc',\n",
       " 'alt.atheism',\n",
       " 'talk.politics.mideast',\n",
       " 'sci.med',\n",
       " 'soc.religion.christian',\n",
       " 'rec.motorcycles',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'soc.religion.christian',\n",
       " 'sci.electronics',\n",
       " 'misc.forsale',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'rec.motorcycles',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'talk.politics.mideast',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.baseball',\n",
       " 'sci.space',\n",
       " 'alt.atheism',\n",
       " 'soc.religion.christian',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'sci.space',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.guns',\n",
       " 'rec.autos',\n",
       " 'rec.autos',\n",
       " 'sci.electronics',\n",
       " 'comp.graphics',\n",
       " 'talk.politics.misc',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.motorcycles',\n",
       " 'talk.politics.misc',\n",
       " 'sci.space',\n",
       " 'rec.sport.baseball',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.graphics',\n",
       " 'rec.motorcycles',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'rec.autos',\n",
       " 'comp.windows.x',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'comp.windows.x',\n",
       " 'talk.religion.misc',\n",
       " 'rec.motorcycles',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'rec.autos',\n",
       " 'sci.med',\n",
       " 'alt.atheism',\n",
       " 'rec.sport.baseball',\n",
       " 'comp.windows.x',\n",
       " 'soc.religion.christian',\n",
       " 'sci.crypt',\n",
       " 'rec.motorcycles',\n",
       " 'rec.motorcycles',\n",
       " 'talk.politics.misc',\n",
       " 'comp.windows.x',\n",
       " 'talk.religion.misc',\n",
       " 'talk.politics.mideast',\n",
       " 'sci.crypt',\n",
       " 'alt.atheism',\n",
       " 'rec.autos',\n",
       " 'talk.politics.guns',\n",
       " 'rec.sport.hockey',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'rec.motorcycles',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'sci.med',\n",
       " 'talk.politics.guns',\n",
       " 'comp.graphics',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.misc',\n",
       " 'sci.space',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'sci.space',\n",
       " 'talk.religion.misc',\n",
       " 'comp.windows.x',\n",
       " 'sci.electronics',\n",
       " 'talk.religion.misc',\n",
       " 'alt.atheism',\n",
       " 'sci.crypt',\n",
       " 'misc.forsale',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'rec.sport.hockey',\n",
       " 'rec.motorcycles',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'rec.sport.baseball',\n",
       " 'sci.crypt',\n",
       " 'talk.politics.misc',\n",
       " 'rec.sport.hockey',\n",
       " 'alt.atheism',\n",
       " 'misc.forsale',\n",
       " 'talk.politics.mideast',\n",
       " 'sci.crypt',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'sci.electronics',\n",
       " 'talk.politics.mideast',\n",
       " 'sci.electronics',\n",
       " 'soc.religion.christian',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'rec.autos',\n",
       " 'rec.sport.hockey',\n",
       " 'talk.religion.misc',\n",
       " 'rec.sport.hockey',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'rec.autos',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.windows.x',\n",
       " 'rec.sport.baseball',\n",
       " 'soc.religion.christian',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.electronics',\n",
       " 'alt.atheism',\n",
       " 'sci.crypt',\n",
       " 'comp.windows.x',\n",
       " 'alt.atheism',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'soc.religion.christian',\n",
       " 'rec.sport.baseball',\n",
       " 'talk.politics.misc',\n",
       " 'rec.sport.baseball',\n",
       " 'talk.politics.guns',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'rec.autos',\n",
       " 'rec.sport.baseball',\n",
       " 'sci.electronics',\n",
       " 'misc.forsale',\n",
       " 'rec.sport.hockey',\n",
       " 'rec.sport.baseball',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.space',\n",
       " 'rec.motorcycles',\n",
       " 'sci.med',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'talk.politics.mideast',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'sci.space',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'talk.politics.misc',\n",
       " 'talk.politics.guns',\n",
       " 'sci.crypt',\n",
       " 'rec.sport.hockey',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.med',\n",
       " 'talk.religion.misc',\n",
       " 'rec.autos',\n",
       " 'misc.forsale',\n",
       " 'alt.atheism',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'talk.politics.mideast',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'talk.politics.guns',\n",
       " 'sci.space',\n",
       " 'sci.electronics',\n",
       " 'talk.politics.mideast',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.graphics',\n",
       " 'talk.politics.guns',\n",
       " 'sci.electronics',\n",
       " 'sci.space',\n",
       " 'comp.graphics',\n",
       " 'rec.sport.hockey',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'alt.atheism',\n",
       " 'talk.politics.guns',\n",
       " 'sci.space',\n",
       " 'sci.med',\n",
       " 'talk.politics.guns',\n",
       " 'sci.electronics',\n",
       " 'comp.graphics',\n",
       " 'rec.autos',\n",
       " 'misc.forsale',\n",
       " 'rec.motorcycles',\n",
       " 'misc.forsale',\n",
       " 'talk.politics.mideast',\n",
       " 'rec.motorcycles',\n",
       " 'talk.politics.guns',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'sci.space',\n",
       " 'talk.politics.guns',\n",
       " 'rec.autos',\n",
       " 'talk.politics.guns',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'soc.religion.christian',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'comp.windows.x',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'rec.autos',\n",
       " 'talk.politics.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'rec.motorcycles',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'sci.electronics',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'sci.crypt',\n",
       " 'comp.windows.x',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'sci.crypt',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'sci.electronics',\n",
       " 'sci.crypt',\n",
       " 'talk.politics.mideast',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'talk.religion.misc',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.misc',\n",
       " 'rec.autos',\n",
       " 'comp.graphics',\n",
       " 'comp.graphics',\n",
       " 'comp.windows.x',\n",
       " 'sci.crypt',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'talk.religion.misc',\n",
       " 'sci.med',\n",
       " 'alt.atheism',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'sci.med',\n",
       " 'talk.politics.mideast',\n",
       " 'sci.space',\n",
       " 'talk.politics.mideast',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'sci.electronics',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'comp.windows.x',\n",
       " 'sci.electronics',\n",
       " 'rec.autos',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'rec.motorcycles',\n",
       " 'sci.electronics',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.windows.x',\n",
       " 'sci.electronics',\n",
       " 'rec.motorcycles',\n",
       " 'talk.politics.mideast',\n",
       " 'comp.windows.x',\n",
       " 'comp.windows.x',\n",
       " 'talk.politics.misc',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'sci.space',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'rec.sport.hockey',\n",
       " 'talk.politics.mideast',\n",
       " 'rec.autos',\n",
       " 'sci.crypt',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'soc.religion.christian',\n",
       " 'talk.religion.misc',\n",
       " 'soc.religion.christian',\n",
       " 'sci.space',\n",
       " 'talk.politics.mideast',\n",
       " 'sci.space',\n",
       " 'sci.space',\n",
       " 'alt.atheism',\n",
       " 'rec.motorcycles',\n",
       " 'sci.crypt',\n",
       " 'talk.politics.guns',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'misc.forsale',\n",
       " 'rec.sport.hockey',\n",
       " 'talk.religion.misc',\n",
       " 'alt.atheism',\n",
       " 'sci.crypt',\n",
       " 'alt.atheism',\n",
       " 'comp.windows.x',\n",
       " 'comp.windows.x',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'alt.atheism',\n",
       " 'talk.politics.mideast',\n",
       " 'misc.forsale',\n",
       " 'talk.politics.mideast',\n",
       " 'rec.sport.baseball',\n",
       " 'misc.forsale',\n",
       " 'sci.electronics',\n",
       " 'misc.forsale',\n",
       " 'sci.med',\n",
       " 'comp.windows.x',\n",
       " 'sci.med',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'soc.religion.christian',\n",
       " 'rec.sport.baseball',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'alt.atheism',\n",
       " 'sci.crypt',\n",
       " 'alt.atheism',\n",
       " 'sci.med',\n",
       " 'alt.atheism',\n",
       " 'rec.motorcycles',\n",
       " 'sci.electronics',\n",
       " 'talk.religion.misc',\n",
       " 'sci.crypt',\n",
       " 'comp.graphics',\n",
       " 'sci.med',\n",
       " 'comp.windows.x',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'sci.space',\n",
       " 'rec.autos',\n",
       " 'talk.religion.misc',\n",
       " 'talk.politics.misc',\n",
       " 'talk.politics.mideast',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'rec.sport.hockey',\n",
       " 'talk.politics.mideast',\n",
       " 'rec.sport.hockey',\n",
       " 'rec.autos',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'rec.sport.baseball',\n",
       " 'talk.politics.misc',\n",
       " 'sci.med',\n",
       " 'talk.religion.misc',\n",
       " 'misc.forsale',\n",
       " 'talk.religion.misc',\n",
       " 'comp.graphics',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'rec.autos',\n",
       " 'rec.sport.hockey',\n",
       " 'talk.religion.misc',\n",
       " 'sci.crypt',\n",
       " 'sci.crypt',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'talk.politics.mideast',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'rec.motorcycles',\n",
       " 'alt.atheism',\n",
       " 'soc.religion.christian',\n",
       " 'comp.graphics',\n",
       " 'rec.autos',\n",
       " 'rec.autos',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'talk.politics.mideast',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'comp.windows.x',\n",
       " 'sci.crypt',\n",
       " 'talk.politics.mideast',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'soc.religion.christian',\n",
       " 'comp.windows.x',\n",
       " 'rec.sport.baseball',\n",
       " 'talk.religion.misc',\n",
       " 'talk.politics.mideast',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'misc.forsale',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.mideast',\n",
       " 'rec.motorcycles',\n",
       " 'talk.politics.misc',\n",
       " 'comp.windows.x',\n",
       " 'talk.politics.misc',\n",
       " 'sci.space',\n",
       " 'alt.atheism',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.motorcycles',\n",
       " 'misc.forsale',\n",
       " 'rec.motorcycles',\n",
       " 'comp.graphics',\n",
       " 'talk.politics.guns',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.autos',\n",
       " 'misc.forsale',\n",
       " 'sci.crypt',\n",
       " 'misc.forsale',\n",
       " 'comp.graphics',\n",
       " 'rec.motorcycles',\n",
       " 'talk.politics.mideast',\n",
       " 'rec.sport.hockey',\n",
       " 'talk.religion.misc',\n",
       " 'rec.motorcycles',\n",
       " 'sci.med',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'talk.politics.guns',\n",
       " 'misc.forsale',\n",
       " 'comp.windows.x',\n",
       " 'sci.space',\n",
       " 'alt.atheism',\n",
       " 'soc.religion.christian',\n",
       " 'sci.space',\n",
       " 'talk.politics.mideast',\n",
       " 'sci.crypt',\n",
       " 'rec.sport.hockey',\n",
       " 'rec.motorcycles',\n",
       " 'talk.politics.guns',\n",
       " 'sci.crypt',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'rec.motorcycles',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'rec.sport.hockey',\n",
       " 'talk.religion.misc',\n",
       " 'sci.electronics',\n",
       " 'misc.forsale',\n",
       " 'rec.motorcycles',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'sci.med',\n",
       " 'talk.politics.guns',\n",
       " 'comp.windows.x',\n",
       " 'rec.sport.hockey',\n",
       " 'comp.windows.x',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'alt.atheism',\n",
       " 'soc.religion.christian',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'misc.forsale',\n",
       " 'rec.sport.baseball',\n",
       " 'misc.forsale',\n",
       " 'rec.motorcycles',\n",
       " 'talk.religion.misc',\n",
       " 'sci.electronics',\n",
       " 'rec.sport.baseball',\n",
       " 'misc.forsale',\n",
       " 'comp.graphics',\n",
       " 'talk.politics.guns',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.graphics',\n",
       " 'sci.electronics',\n",
       " 'rec.sport.baseball',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.motorcycles',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'sci.crypt',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'rec.motorcycles',\n",
       " 'sci.med',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'misc.forsale',\n",
       " 'sci.electronics',\n",
       " 'rec.motorcycles',\n",
       " 'comp.graphics',\n",
       " 'soc.religion.christian',\n",
       " 'rec.motorcycles',\n",
       " 'talk.politics.misc',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'alt.atheism',\n",
       " 'misc.forsale',\n",
       " 'sci.med',\n",
       " 'comp.graphics',\n",
       " 'sci.med',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.electronics',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'sci.med',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'talk.politics.guns',\n",
       " 'sci.electronics',\n",
       " 'comp.windows.x',\n",
       " 'rec.motorcycles',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'rec.sport.hockey',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.windows.x',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'soc.religion.christian',\n",
       " 'sci.electronics',\n",
       " 'rec.sport.baseball',\n",
       " 'talk.politics.misc',\n",
       " 'sci.med',\n",
       " 'sci.crypt',\n",
       " 'alt.atheism',\n",
       " 'sci.crypt',\n",
       " 'rec.sport.hockey',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.windows.x',\n",
       " 'alt.atheism',\n",
       " 'soc.religion.christian',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'rec.motorcycles',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'soc.religion.christian',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'rec.sport.baseball',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.mideast',\n",
       " 'sci.med',\n",
       " 'sci.electronics',\n",
       " 'talk.politics.misc',\n",
       " 'alt.atheism',\n",
       " 'rec.motorcycles',\n",
       " 'alt.atheism',\n",
       " 'rec.autos',\n",
       " 'rec.autos',\n",
       " 'comp.windows.x',\n",
       " 'sci.space',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'soc.religion.christian',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'rec.autos',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.autos',\n",
       " 'rec.sport.baseball',\n",
       " 'sci.crypt',\n",
       " 'rec.autos',\n",
       " 'talk.religion.misc',\n",
       " 'soc.religion.christian',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.misc',\n",
       " 'alt.atheism',\n",
       " 'misc.forsale',\n",
       " 'rec.sport.hockey',\n",
       " 'rec.sport.hockey',\n",
       " 'misc.forsale',\n",
       " 'rec.motorcycles',\n",
       " 'comp.graphics',\n",
       " 'talk.politics.guns',\n",
       " 'comp.windows.x',\n",
       " 'rec.sport.baseball',\n",
       " 'talk.politics.misc',\n",
       " 'soc.religion.christian',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.windows.x',\n",
       " 'sci.med',\n",
       " 'rec.sport.baseball',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'rec.motorcycles',\n",
       " 'comp.graphics',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.graphics',\n",
       " 'rec.sport.baseball',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'rec.sport.baseball',\n",
       " 'talk.politics.guns',\n",
       " 'rec.sport.hockey',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.misc',\n",
       " 'sci.electronics',\n",
       " 'talk.politics.mideast',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'rec.motorcycles',\n",
       " 'comp.graphics',\n",
       " 'sci.space',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'rec.motorcycles',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.graphics',\n",
       " 'comp.windows.x',\n",
       " 'talk.politics.guns',\n",
       " 'rec.sport.hockey',\n",
       " 'rec.sport.hockey',\n",
       " 'comp.windows.x',\n",
       " 'soc.religion.christian',\n",
       " 'sci.med',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'misc.forsale',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.motorcycles',\n",
       " 'sci.crypt',\n",
       " 'comp.windows.x',\n",
       " 'talk.politics.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'misc.forsale',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.guns',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'rec.motorcycles',\n",
       " 'misc.forsale',\n",
       " 'rec.motorcycles',\n",
       " 'talk.religion.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.misc',\n",
       " 'misc.forsale',\n",
       " 'talk.politics.mideast',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'misc.forsale',\n",
       " 'alt.atheism',\n",
       " 'rec.sport.baseball',\n",
       " 'alt.atheism',\n",
       " 'talk.politics.guns',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.graphics',\n",
       " 'talk.politics.guns',\n",
       " 'sci.space',\n",
       " 'rec.sport.hockey',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.hockey',\n",
       " 'alt.atheism',\n",
       " 'talk.politics.mideast',\n",
       " 'sci.crypt',\n",
       " 'sci.crypt',\n",
       " 'comp.windows.x',\n",
       " 'sci.crypt',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'talk.politics.misc',\n",
       " 'comp.graphics',\n",
       " 'talk.politics.guns',\n",
       " 'sci.med',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.windows.x',\n",
       " 'soc.religion.christian',\n",
       " 'alt.atheism',\n",
       " 'talk.religion.misc',\n",
       " 'sci.electronics',\n",
       " 'soc.religion.christian',\n",
       " 'sci.crypt',\n",
       " 'talk.religion.misc',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'talk.politics.mideast',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'misc.forsale',\n",
       " 'talk.politics.mideast',\n",
       " 'comp.windows.x',\n",
       " 'comp.graphics',\n",
       " 'rec.autos',\n",
       " 'talk.politics.guns',\n",
       " 'soc.religion.christian',\n",
       " 'sci.electronics',\n",
       " 'rec.autos',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'talk.religion.misc',\n",
       " 'talk.politics.mideast',\n",
       " 'misc.forsale',\n",
       " 'soc.religion.christian',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'rec.autos',\n",
       " 'talk.politics.misc',\n",
       " 'alt.atheism',\n",
       " 'sci.electronics',\n",
       " 'comp.windows.x',\n",
       " 'talk.politics.guns',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'sci.space',\n",
       " 'rec.sport.baseball',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'sci.med',\n",
       " 'rec.motorcycles',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'talk.religion.misc',\n",
       " 'sci.electronics',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'sci.med',\n",
       " 'alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'sci.crypt',\n",
       " 'talk.religion.misc',\n",
       " 'comp.windows.x',\n",
       " 'rec.motorcycles',\n",
       " 'sci.space',\n",
       " 'talk.politics.misc',\n",
       " 'misc.forsale',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'rec.sport.baseball',\n",
       " 'alt.atheism',\n",
       " 'comp.windows.x',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.space',\n",
       " 'comp.windows.x',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'misc.forsale',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'sci.crypt',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.med',\n",
       " 'rec.motorcycles',\n",
       " 'sci.med',\n",
       " 'sci.med',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'soc.religion.christian',\n",
       " 'rec.motorcycles',\n",
       " 'talk.politics.mideast',\n",
       " 'rec.sport.baseball',\n",
       " 'soc.religion.christian',\n",
       " 'comp.windows.x',\n",
       " 'talk.religion.misc',\n",
       " 'comp.windows.x',\n",
       " 'talk.politics.misc',\n",
       " 'comp.graphics',\n",
       " 'sci.space',\n",
       " 'rec.motorcycles',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'sci.crypt',\n",
       " 'rec.autos',\n",
       " 'rec.sport.hockey',\n",
       " 'talk.politics.mideast',\n",
       " 'comp.windows.x',\n",
       " 'sci.electronics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'talk.politics.guns',\n",
       " 'sci.med',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.mideast',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'rec.motorcycles',\n",
       " 'talk.politics.mideast',\n",
       " 'sci.space',\n",
       " 'rec.sport.baseball',\n",
       " 'alt.atheism',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.guns',\n",
       " 'sci.crypt',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'sci.space',\n",
       " 'talk.politics.guns',\n",
       " 'sci.space',\n",
       " 'talk.religion.misc',\n",
       " 'sci.electronics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'sci.electronics',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'soc.religion.christian',\n",
       " 'soc.religion.christian',\n",
       " 'misc.forsale',\n",
       " 'sci.space',\n",
       " 'rec.sport.hockey',\n",
       " 'rec.sport.hockey',\n",
       " 'talk.politics.mideast',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'sci.electronics',\n",
       " 'rec.motorcycles',\n",
       " 'talk.religion.misc',\n",
       " 'rec.autos',\n",
       " 'comp.graphics',\n",
       " 'sci.crypt',\n",
       " 'talk.religion.misc',\n",
       " 'misc.forsale',\n",
       " 'alt.atheism',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.graphics',\n",
       " 'sci.crypt',\n",
       " 'sci.med',\n",
       " 'rec.sport.baseball',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'soc.religion.christian',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'sci.crypt',\n",
       " 'talk.religion.misc',\n",
       " 'comp.windows.x',\n",
       " 'sci.crypt',\n",
       " 'rec.autos',\n",
       " 'rec.sport.hockey',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'sci.electronics',\n",
       " 'talk.politics.guns',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'rec.autos',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'soc.religion.christian',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'rec.sport.baseball',\n",
       " 'comp.graphics',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.graphics',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " ...]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topicnames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7c30ca",
   "metadata": {},
   "source": [
    "#### 2.2. Deploy the topic model with batch transform\n",
    "\n",
    "With batch transform, you can run inferences on a batch of data at a time. Amazon SageMaker creates the necessary compute infrastructure and tears it down once the batch job is completed.\n",
    "\n",
    "The batch transform code creates a sagemaker.transformer.Transformer object from the topic model. Then, it calls that object's transform method to create a transform job. When you create the sagemaker.transformer.Transformer object, you specify the number and type of instances to use to perform the batch transform job, and the location in Amazon S3 where you want to store the inferences.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2829873b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-recom-bucket/20newsgroups/batch/train/trainvectors.csv\n",
      ".....................................\u001b[34mDocker entrypoint called with argument(s): serve\u001b[0m\n",
      "\u001b[34mRunning default environment configuration script\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.7/site-packages/jsonref.py:8: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  from collections import Mapping, MutableMapping, Sequence\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.7/site-packages/google/protobuf/internal/api_implementation.py:151: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  from google.protobuf.pyext import _message\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:21:41 INFO 140208043214656] loaded entry point class algorithm.serve.server_config:config_api\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:21:41 INFO 140208043214656] loading entry points\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.7/site-packages/ai_algorithms_sdk/serve.py:221: DeprecationWarning: entrypoint algorithm.request_iterators is deprecated in favor of algorithm.io.data_handlers.serve\n",
      "  \"in favor of algorithm.io.data_handlers.serve\", DeprecationWarning)\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:21:41 INFO 140208043214656] loaded request iterator application/json\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:21:41 INFO 140208043214656] loaded request iterator application/jsonlines\u001b[0m\n",
      "\u001b[35mDocker entrypoint called with argument(s): serve\u001b[0m\n",
      "\u001b[35mRunning default environment configuration script\u001b[0m\n",
      "\u001b[35m/opt/amazon/lib/python3.7/site-packages/jsonref.py:8: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  from collections import Mapping, MutableMapping, Sequence\u001b[0m\n",
      "\u001b[35m/opt/amazon/lib/python3.7/site-packages/google/protobuf/internal/api_implementation.py:151: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  from google.protobuf.pyext import _message\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:21:41 INFO 140208043214656] loaded entry point class algorithm.serve.server_config:config_api\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:21:41 INFO 140208043214656] loading entry points\u001b[0m\n",
      "\u001b[35m/opt/amazon/lib/python3.7/site-packages/ai_algorithms_sdk/serve.py:221: DeprecationWarning: entrypoint algorithm.request_iterators is deprecated in favor of algorithm.io.data_handlers.serve\n",
      "  \"in favor of algorithm.io.data_handlers.serve\", DeprecationWarning)\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:21:41 INFO 140208043214656] loaded request iterator application/json\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:21:41 INFO 140208043214656] loaded request iterator application/jsonlines\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:21:41 INFO 140208043214656] loaded request iterator application/x-recordio-protobuf\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:21:41 INFO 140208043214656] loaded request iterator text/csv\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:21:41 INFO 140208043214656] loaded response encoder application/json\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:21:41 INFO 140208043214656] loaded response encoder application/jsonlines\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:21:41 INFO 140208043214656] loaded response encoder application/x-recordio-protobuf\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:21:41 INFO 140208043214656] loaded entry point class algorithm:model\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:21:41 INFO 140208043214656] Number of server workers: 1\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:21:41 +0000] [1] [INFO] Starting gunicorn 20.1.0\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:21:41 +0000] [1] [INFO] Listening at: http://0.0.0.0:8080 (1)\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:21:41 +0000] [1] [INFO] Using worker: sync\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:21:41 +0000] [31] [INFO] Booting worker with pid: 31\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:21:41 INFO 140208043214656] loading model...\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:21:41 INFO 140208043214656] Deserialized model from /opt/ml/model\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:21:41 INFO 140208043214656] loaded request iterator application/x-recordio-protobuf\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:21:41 INFO 140208043214656] loaded request iterator text/csv\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:21:41 INFO 140208043214656] loaded response encoder application/json\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:21:41 INFO 140208043214656] loaded response encoder application/jsonlines\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:21:41 INFO 140208043214656] loaded response encoder application/x-recordio-protobuf\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:21:41 INFO 140208043214656] loaded entry point class algorithm:model\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:21:41 INFO 140208043214656] Number of server workers: 1\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:21:41 +0000] [1] [INFO] Starting gunicorn 20.1.0\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:21:41 +0000] [1] [INFO] Listening at: http://0.0.0.0:8080 (1)\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:21:41 +0000] [1] [INFO] Using worker: sync\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:21:41 +0000] [31] [INFO] Booting worker with pid: 31\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:21:41 INFO 140208043214656] loading model...\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:21:41 INFO 140208043214656] Deserialized model from /opt/ml/model\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:21:41 WARNING 140208043214656] Requesting context without setting the requested num of gpus. Using 'auto'\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:21:41 INFO 140208043214656] nvidia-smi: took 0.031 seconds to run.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:21:41 INFO 140208043214656] nvidia-smi identified 0 GPUs.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:21:41 INFO 140208043214656] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:21:41 WARNING 140208043214656] Requesting context without setting the requested num of gpus. Using 'auto'\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:21:41 INFO 140208043214656] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:21:41 INFO 140208043214656] ...model loaded.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:21:41 WARNING 140208043214656] Requesting context without setting the requested num of gpus. Using 'auto'\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:21:41 INFO 140208043214656] nvidia-smi: took 0.031 seconds to run.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:21:41 INFO 140208043214656] nvidia-smi identified 0 GPUs.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:21:41 INFO 140208043214656] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:21:41 WARNING 140208043214656] Requesting context without setting the requested num of gpus. Using 'auto'\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:21:41 INFO 140208043214656] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:21:41 INFO 140208043214656] ...model loaded.\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646310101.4721084, \"EndTime\": 1646310104.1727362, \"Dimensions\": {\"Algorithm\": \"NVDMModel\", \"Host\": \"UNKNOWN\", \"Operation\": \"scoring\"}, \"Metrics\": {\"execution_parameters.count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646310101.4721084, \"EndTime\": 1646310104.1727362, \"Dimensions\": {\"Algorithm\": \"NVDMModel\", \"Host\": \"UNKNOWN\", \"Operation\": \"scoring\"}, \"Metrics\": {\"execution_parameters.count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646310104.1729133, \"EndTime\": 1646310106.1294258, \"Dimensions\": {\"Algorithm\": \"NVDMModel\", \"Host\": \"UNKNOWN\", \"Operation\": \"scoring\"}, \"Metrics\": {\"model.evaluate.time\": {\"sum\": 4.79578971862793, \"count\": 1, \"min\": 4.79578971862793, \"max\": 4.79578971862793}, \"json.encoder.time\": {\"sum\": 78.78398895263672, \"count\": 1, \"min\": 78.78398895263672, \"max\": 78.78398895263672}, \"invocations.count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646310104.1729133, \"EndTime\": 1646310106.1294258, \"Dimensions\": {\"Algorithm\": \"NVDMModel\", \"Host\": \"UNKNOWN\", \"Operation\": \"scoring\"}, \"Metrics\": {\"model.evaluate.time\": {\"sum\": 4.79578971862793, \"count\": 1, \"min\": 4.79578971862793, \"max\": 4.79578971862793}, \"json.encoder.time\": {\"sum\": 78.78398895263672, \"count\": 1, \"min\": 78.78398895263672, \"max\": 78.78398895263672}, \"invocations.count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646310106.1295547, \"EndTime\": 1646310106.7394466, \"Dimensions\": {\"Algorithm\": \"NVDMModel\", \"Host\": \"UNKNOWN\", \"Operation\": \"scoring\"}, \"Metrics\": {\"model.evaluate.time\": {\"sum\": 0.8707046508789062, \"count\": 1, \"min\": 0.8707046508789062, \"max\": 0.8707046508789062}, \"json.encoder.time\": {\"sum\": 22.689342498779297, \"count\": 1, \"min\": 22.689342498779297, \"max\": 22.689342498779297}, \"invocations.count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646310106.1295547, \"EndTime\": 1646310106.7394466, \"Dimensions\": {\"Algorithm\": \"NVDMModel\", \"Host\": \"UNKNOWN\", \"Operation\": \"scoring\"}, \"Metrics\": {\"model.evaluate.time\": {\"sum\": 0.8707046508789062, \"count\": 1, \"min\": 0.8707046508789062, \"max\": 0.8707046508789062}, \"json.encoder.time\": {\"sum\": 22.689342498779297, \"count\": 1, \"min\": 22.689342498779297, \"max\": 22.689342498779297}, \"invocations.count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[32m2022-03-03T12:21:44.179:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646310106.7395742, \"EndTime\": 1646310107.301682, \"Dimensions\": {\"Algorithm\": \"NVDMModel\", \"Host\": \"UNKNOWN\", \"Operation\": \"scoring\"}, \"Metrics\": {\"model.evaluate.time\": {\"sum\": 0.8742809295654297, \"count\": 1, \"min\": 0.8742809295654297, \"max\": 0.8742809295654297}, \"json.encoder.time\": {\"sum\": 22.658348083496094, \"count\": 1, \"min\": 22.658348083496094, \"max\": 22.658348083496094}, \"invocations.count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646310107.3018088, \"EndTime\": 1646310107.8729863, \"Dimensions\": {\"Algorithm\": \"NVDMModel\", \"Host\": \"UNKNOWN\", \"Operation\": \"scoring\"}, \"Metrics\": {\"model.evaluate.time\": {\"sum\": 0.9069442749023438, \"count\": 1, \"min\": 0.9069442749023438, \"max\": 0.9069442749023438}, \"json.encoder.time\": {\"sum\": 22.29022979736328, \"count\": 1, \"min\": 22.29022979736328, \"max\": 22.29022979736328}, \"invocations.count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646310106.7395742, \"EndTime\": 1646310107.301682, \"Dimensions\": {\"Algorithm\": \"NVDMModel\", \"Host\": \"UNKNOWN\", \"Operation\": \"scoring\"}, \"Metrics\": {\"model.evaluate.time\": {\"sum\": 0.8742809295654297, \"count\": 1, \"min\": 0.8742809295654297, \"max\": 0.8742809295654297}, \"json.encoder.time\": {\"sum\": 22.658348083496094, \"count\": 1, \"min\": 22.658348083496094, \"max\": 22.658348083496094}, \"invocations.count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646310107.3018088, \"EndTime\": 1646310107.8729863, \"Dimensions\": {\"Algorithm\": \"NVDMModel\", \"Host\": \"UNKNOWN\", \"Operation\": \"scoring\"}, \"Metrics\": {\"model.evaluate.time\": {\"sum\": 0.9069442749023438, \"count\": 1, \"min\": 0.9069442749023438, \"max\": 0.9069442749023438}, \"json.encoder.time\": {\"sum\": 22.29022979736328, \"count\": 1, \"min\": 22.29022979736328, \"max\": 22.29022979736328}, \"invocations.count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646310107.8731205, \"EndTime\": 1646310108.438773, \"Dimensions\": {\"Algorithm\": \"NVDMModel\", \"Host\": \"UNKNOWN\", \"Operation\": \"scoring\"}, \"Metrics\": {\"model.evaluate.time\": {\"sum\": 0.8952617645263672, \"count\": 1, \"min\": 0.8952617645263672, \"max\": 0.8952617645263672}, \"json.encoder.time\": {\"sum\": 22.327184677124023, \"count\": 1, \"min\": 22.327184677124023, \"max\": 22.327184677124023}, \"invocations.count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646310107.8731205, \"EndTime\": 1646310108.438773, \"Dimensions\": {\"Algorithm\": \"NVDMModel\", \"Host\": \"UNKNOWN\", \"Operation\": \"scoring\"}, \"Metrics\": {\"model.evaluate.time\": {\"sum\": 0.8952617645263672, \"count\": 1, \"min\": 0.8952617645263672, \"max\": 0.8952617645263672}, \"json.encoder.time\": {\"sum\": 22.327184677124023, \"count\": 1, \"min\": 22.327184677124023, \"max\": 22.327184677124023}, \"invocations.count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646310108.438922, \"EndTime\": 1646310108.9989111, \"Dimensions\": {\"Algorithm\": \"NVDMModel\", \"Host\": \"UNKNOWN\", \"Operation\": \"scoring\"}, \"Metrics\": {\"model.evaluate.time\": {\"sum\": 0.8931159973144531, \"count\": 1, \"min\": 0.8931159973144531, \"max\": 0.8931159973144531}, \"json.encoder.time\": {\"sum\": 24.309635162353516, \"count\": 1, \"min\": 24.309635162353516, \"max\": 24.309635162353516}, \"invocations.count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646310108.438922, \"EndTime\": 1646310108.9989111, \"Dimensions\": {\"Algorithm\": \"NVDMModel\", \"Host\": \"UNKNOWN\", \"Operation\": \"scoring\"}, \"Metrics\": {\"model.evaluate.time\": {\"sum\": 0.8931159973144531, \"count\": 1, \"min\": 0.8931159973144531, \"max\": 0.8931159973144531}, \"json.encoder.time\": {\"sum\": 24.309635162353516, \"count\": 1, \"min\": 24.309635162353516, \"max\": 24.309635162353516}, \"invocations.count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646310108.9990456, \"EndTime\": 1646310109.5799873, \"Dimensions\": {\"Algorithm\": \"NVDMModel\", \"Host\": \"UNKNOWN\", \"Operation\": \"scoring\"}, \"Metrics\": {\"model.evaluate.time\": {\"sum\": 0.7317066192626953, \"count\": 1, \"min\": 0.7317066192626953, \"max\": 0.7317066192626953}, \"json.encoder.time\": {\"sum\": 28.41973304748535, \"count\": 1, \"min\": 28.41973304748535, \"max\": 28.41973304748535}, \"invocations.count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646310109.5801182, \"EndTime\": 1646310109.7002532, \"Dimensions\": {\"Algorithm\": \"NVDMModel\", \"Host\": \"UNKNOWN\", \"Operation\": \"scoring\"}, \"Metrics\": {\"model.evaluate.time\": {\"sum\": 2.6187896728515625, \"count\": 1, \"min\": 2.6187896728515625, \"max\": 2.6187896728515625}, \"json.encoder.time\": {\"sum\": 5.117654800415039, \"count\": 1, \"min\": 5.117654800415039, \"max\": 5.117654800415039}, \"invocations.count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646310108.9990456, \"EndTime\": 1646310109.5799873, \"Dimensions\": {\"Algorithm\": \"NVDMModel\", \"Host\": \"UNKNOWN\", \"Operation\": \"scoring\"}, \"Metrics\": {\"model.evaluate.time\": {\"sum\": 0.7317066192626953, \"count\": 1, \"min\": 0.7317066192626953, \"max\": 0.7317066192626953}, \"json.encoder.time\": {\"sum\": 28.41973304748535, \"count\": 1, \"min\": 28.41973304748535, \"max\": 28.41973304748535}, \"invocations.count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646310109.5801182, \"EndTime\": 1646310109.7002532, \"Dimensions\": {\"Algorithm\": \"NVDMModel\", \"Host\": \"UNKNOWN\", \"Operation\": \"scoring\"}, \"Metrics\": {\"model.evaluate.time\": {\"sum\": 2.6187896728515625, \"count\": 1, \"min\": 2.6187896728515625, \"max\": 2.6187896728515625}, \"json.encoder.time\": {\"sum\": 5.117654800415039, \"count\": 1, \"min\": 5.117654800415039, \"max\": 5.117654800415039}, \"invocations.count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\n",
      "\u001b[34mDocker entrypoint called with argument(s): serve\u001b[0m\n",
      "\u001b[34mRunning default environment configuration script\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.7/site-packages/jsonref.py:8: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  from collections import Mapping, MutableMapping, Sequence\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.7/site-packages/google/protobuf/internal/api_implementation.py:151: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  from google.protobuf.pyext import _message\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:21:41 INFO 140208043214656] loaded entry point class algorithm.serve.server_config:config_api\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:21:41 INFO 140208043214656] loading entry points\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.7/site-packages/ai_algorithms_sdk/serve.py:221: DeprecationWarning: entrypoint algorithm.request_iterators is deprecated in favor of algorithm.io.data_handlers.serve\n",
      "  \"in favor of algorithm.io.data_handlers.serve\", DeprecationWarning)\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:21:41 INFO 140208043214656] loaded request iterator application/json\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:21:41 INFO 140208043214656] loaded request iterator application/jsonlines\u001b[0m\n",
      "\u001b[35mDocker entrypoint called with argument(s): serve\u001b[0m\n",
      "\u001b[35mRunning default environment configuration script\u001b[0m\n",
      "\u001b[35m/opt/amazon/lib/python3.7/site-packages/jsonref.py:8: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  from collections import Mapping, MutableMapping, Sequence\u001b[0m\n",
      "\u001b[35m/opt/amazon/lib/python3.7/site-packages/google/protobuf/internal/api_implementation.py:151: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  from google.protobuf.pyext import _message\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:21:41 INFO 140208043214656] loaded entry point class algorithm.serve.server_config:config_api\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:21:41 INFO 140208043214656] loading entry points\u001b[0m\n",
      "\u001b[35m/opt/amazon/lib/python3.7/site-packages/ai_algorithms_sdk/serve.py:221: DeprecationWarning: entrypoint algorithm.request_iterators is deprecated in favor of algorithm.io.data_handlers.serve\n",
      "  \"in favor of algorithm.io.data_handlers.serve\", DeprecationWarning)\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:21:41 INFO 140208043214656] loaded request iterator application/json\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:21:41 INFO 140208043214656] loaded request iterator application/jsonlines\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:21:41 INFO 140208043214656] loaded request iterator application/x-recordio-protobuf\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:21:41 INFO 140208043214656] loaded request iterator text/csv\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:21:41 INFO 140208043214656] loaded response encoder application/json\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:21:41 INFO 140208043214656] loaded response encoder application/jsonlines\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:21:41 INFO 140208043214656] loaded response encoder application/x-recordio-protobuf\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:21:41 INFO 140208043214656] loaded entry point class algorithm:model\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:21:41 INFO 140208043214656] Number of server workers: 1\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:21:41 +0000] [1] [INFO] Starting gunicorn 20.1.0\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:21:41 +0000] [1] [INFO] Listening at: http://0.0.0.0:8080 (1)\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:21:41 +0000] [1] [INFO] Using worker: sync\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:21:41 +0000] [31] [INFO] Booting worker with pid: 31\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:21:41 INFO 140208043214656] loading model...\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:21:41 INFO 140208043214656] Deserialized model from /opt/ml/model\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:21:41 INFO 140208043214656] loaded request iterator application/x-recordio-protobuf\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:21:41 INFO 140208043214656] loaded request iterator text/csv\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:21:41 INFO 140208043214656] loaded response encoder application/json\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:21:41 INFO 140208043214656] loaded response encoder application/jsonlines\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:21:41 INFO 140208043214656] loaded response encoder application/x-recordio-protobuf\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:21:41 INFO 140208043214656] loaded entry point class algorithm:model\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:21:41 INFO 140208043214656] Number of server workers: 1\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:21:41 +0000] [1] [INFO] Starting gunicorn 20.1.0\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:21:41 +0000] [1] [INFO] Listening at: http://0.0.0.0:8080 (1)\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:21:41 +0000] [1] [INFO] Using worker: sync\u001b[0m\n",
      "\u001b[35m[2022-03-03 12:21:41 +0000] [31] [INFO] Booting worker with pid: 31\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:21:41 INFO 140208043214656] loading model...\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:21:41 INFO 140208043214656] Deserialized model from /opt/ml/model\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:21:41 WARNING 140208043214656] Requesting context without setting the requested num of gpus. Using 'auto'\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:21:41 INFO 140208043214656] nvidia-smi: took 0.031 seconds to run.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:21:41 INFO 140208043214656] nvidia-smi identified 0 GPUs.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:21:41 INFO 140208043214656] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:21:41 WARNING 140208043214656] Requesting context without setting the requested num of gpus. Using 'auto'\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:21:41 INFO 140208043214656] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:21:41 INFO 140208043214656] ...model loaded.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:21:41 WARNING 140208043214656] Requesting context without setting the requested num of gpus. Using 'auto'\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:21:41 INFO 140208043214656] nvidia-smi: took 0.031 seconds to run.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:21:41 INFO 140208043214656] nvidia-smi identified 0 GPUs.\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:21:41 INFO 140208043214656] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:21:41 WARNING 140208043214656] Requesting context without setting the requested num of gpus. Using 'auto'\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:21:41 INFO 140208043214656] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[35m[03/03/2022 12:21:41 INFO 140208043214656] ...model loaded.\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646310101.4721084, \"EndTime\": 1646310104.1727362, \"Dimensions\": {\"Algorithm\": \"NVDMModel\", \"Host\": \"UNKNOWN\", \"Operation\": \"scoring\"}, \"Metrics\": {\"execution_parameters.count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646310101.4721084, \"EndTime\": 1646310104.1727362, \"Dimensions\": {\"Algorithm\": \"NVDMModel\", \"Host\": \"UNKNOWN\", \"Operation\": \"scoring\"}, \"Metrics\": {\"execution_parameters.count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646310104.1729133, \"EndTime\": 1646310106.1294258, \"Dimensions\": {\"Algorithm\": \"NVDMModel\", \"Host\": \"UNKNOWN\", \"Operation\": \"scoring\"}, \"Metrics\": {\"model.evaluate.time\": {\"sum\": 4.79578971862793, \"count\": 1, \"min\": 4.79578971862793, \"max\": 4.79578971862793}, \"json.encoder.time\": {\"sum\": 78.78398895263672, \"count\": 1, \"min\": 78.78398895263672, \"max\": 78.78398895263672}, \"invocations.count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646310104.1729133, \"EndTime\": 1646310106.1294258, \"Dimensions\": {\"Algorithm\": \"NVDMModel\", \"Host\": \"UNKNOWN\", \"Operation\": \"scoring\"}, \"Metrics\": {\"model.evaluate.time\": {\"sum\": 4.79578971862793, \"count\": 1, \"min\": 4.79578971862793, \"max\": 4.79578971862793}, \"json.encoder.time\": {\"sum\": 78.78398895263672, \"count\": 1, \"min\": 78.78398895263672, \"max\": 78.78398895263672}, \"invocations.count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646310106.1295547, \"EndTime\": 1646310106.7394466, \"Dimensions\": {\"Algorithm\": \"NVDMModel\", \"Host\": \"UNKNOWN\", \"Operation\": \"scoring\"}, \"Metrics\": {\"model.evaluate.time\": {\"sum\": 0.8707046508789062, \"count\": 1, \"min\": 0.8707046508789062, \"max\": 0.8707046508789062}, \"json.encoder.time\": {\"sum\": 22.689342498779297, \"count\": 1, \"min\": 22.689342498779297, \"max\": 22.689342498779297}, \"invocations.count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646310106.1295547, \"EndTime\": 1646310106.7394466, \"Dimensions\": {\"Algorithm\": \"NVDMModel\", \"Host\": \"UNKNOWN\", \"Operation\": \"scoring\"}, \"Metrics\": {\"model.evaluate.time\": {\"sum\": 0.8707046508789062, \"count\": 1, \"min\": 0.8707046508789062, \"max\": 0.8707046508789062}, \"json.encoder.time\": {\"sum\": 22.689342498779297, \"count\": 1, \"min\": 22.689342498779297, \"max\": 22.689342498779297}, \"invocations.count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[32m2022-03-03T12:21:44.179:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646310106.7395742, \"EndTime\": 1646310107.301682, \"Dimensions\": {\"Algorithm\": \"NVDMModel\", \"Host\": \"UNKNOWN\", \"Operation\": \"scoring\"}, \"Metrics\": {\"model.evaluate.time\": {\"sum\": 0.8742809295654297, \"count\": 1, \"min\": 0.8742809295654297, \"max\": 0.8742809295654297}, \"json.encoder.time\": {\"sum\": 22.658348083496094, \"count\": 1, \"min\": 22.658348083496094, \"max\": 22.658348083496094}, \"invocations.count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646310107.3018088, \"EndTime\": 1646310107.8729863, \"Dimensions\": {\"Algorithm\": \"NVDMModel\", \"Host\": \"UNKNOWN\", \"Operation\": \"scoring\"}, \"Metrics\": {\"model.evaluate.time\": {\"sum\": 0.9069442749023438, \"count\": 1, \"min\": 0.9069442749023438, \"max\": 0.9069442749023438}, \"json.encoder.time\": {\"sum\": 22.29022979736328, \"count\": 1, \"min\": 22.29022979736328, \"max\": 22.29022979736328}, \"invocations.count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646310106.7395742, \"EndTime\": 1646310107.301682, \"Dimensions\": {\"Algorithm\": \"NVDMModel\", \"Host\": \"UNKNOWN\", \"Operation\": \"scoring\"}, \"Metrics\": {\"model.evaluate.time\": {\"sum\": 0.8742809295654297, \"count\": 1, \"min\": 0.8742809295654297, \"max\": 0.8742809295654297}, \"json.encoder.time\": {\"sum\": 22.658348083496094, \"count\": 1, \"min\": 22.658348083496094, \"max\": 22.658348083496094}, \"invocations.count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646310107.3018088, \"EndTime\": 1646310107.8729863, \"Dimensions\": {\"Algorithm\": \"NVDMModel\", \"Host\": \"UNKNOWN\", \"Operation\": \"scoring\"}, \"Metrics\": {\"model.evaluate.time\": {\"sum\": 0.9069442749023438, \"count\": 1, \"min\": 0.9069442749023438, \"max\": 0.9069442749023438}, \"json.encoder.time\": {\"sum\": 22.29022979736328, \"count\": 1, \"min\": 22.29022979736328, \"max\": 22.29022979736328}, \"invocations.count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646310107.8731205, \"EndTime\": 1646310108.438773, \"Dimensions\": {\"Algorithm\": \"NVDMModel\", \"Host\": \"UNKNOWN\", \"Operation\": \"scoring\"}, \"Metrics\": {\"model.evaluate.time\": {\"sum\": 0.8952617645263672, \"count\": 1, \"min\": 0.8952617645263672, \"max\": 0.8952617645263672}, \"json.encoder.time\": {\"sum\": 22.327184677124023, \"count\": 1, \"min\": 22.327184677124023, \"max\": 22.327184677124023}, \"invocations.count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646310107.8731205, \"EndTime\": 1646310108.438773, \"Dimensions\": {\"Algorithm\": \"NVDMModel\", \"Host\": \"UNKNOWN\", \"Operation\": \"scoring\"}, \"Metrics\": {\"model.evaluate.time\": {\"sum\": 0.8952617645263672, \"count\": 1, \"min\": 0.8952617645263672, \"max\": 0.8952617645263672}, \"json.encoder.time\": {\"sum\": 22.327184677124023, \"count\": 1, \"min\": 22.327184677124023, \"max\": 22.327184677124023}, \"invocations.count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646310108.438922, \"EndTime\": 1646310108.9989111, \"Dimensions\": {\"Algorithm\": \"NVDMModel\", \"Host\": \"UNKNOWN\", \"Operation\": \"scoring\"}, \"Metrics\": {\"model.evaluate.time\": {\"sum\": 0.8931159973144531, \"count\": 1, \"min\": 0.8931159973144531, \"max\": 0.8931159973144531}, \"json.encoder.time\": {\"sum\": 24.309635162353516, \"count\": 1, \"min\": 24.309635162353516, \"max\": 24.309635162353516}, \"invocations.count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646310108.438922, \"EndTime\": 1646310108.9989111, \"Dimensions\": {\"Algorithm\": \"NVDMModel\", \"Host\": \"UNKNOWN\", \"Operation\": \"scoring\"}, \"Metrics\": {\"model.evaluate.time\": {\"sum\": 0.8931159973144531, \"count\": 1, \"min\": 0.8931159973144531, \"max\": 0.8931159973144531}, \"json.encoder.time\": {\"sum\": 24.309635162353516, \"count\": 1, \"min\": 24.309635162353516, \"max\": 24.309635162353516}, \"invocations.count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646310108.9990456, \"EndTime\": 1646310109.5799873, \"Dimensions\": {\"Algorithm\": \"NVDMModel\", \"Host\": \"UNKNOWN\", \"Operation\": \"scoring\"}, \"Metrics\": {\"model.evaluate.time\": {\"sum\": 0.7317066192626953, \"count\": 1, \"min\": 0.7317066192626953, \"max\": 0.7317066192626953}, \"json.encoder.time\": {\"sum\": 28.41973304748535, \"count\": 1, \"min\": 28.41973304748535, \"max\": 28.41973304748535}, \"invocations.count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646310109.5801182, \"EndTime\": 1646310109.7002532, \"Dimensions\": {\"Algorithm\": \"NVDMModel\", \"Host\": \"UNKNOWN\", \"Operation\": \"scoring\"}, \"Metrics\": {\"model.evaluate.time\": {\"sum\": 2.6187896728515625, \"count\": 1, \"min\": 2.6187896728515625, \"max\": 2.6187896728515625}, \"json.encoder.time\": {\"sum\": 5.117654800415039, \"count\": 1, \"min\": 5.117654800415039, \"max\": 5.117654800415039}, \"invocations.count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646310108.9990456, \"EndTime\": 1646310109.5799873, \"Dimensions\": {\"Algorithm\": \"NVDMModel\", \"Host\": \"UNKNOWN\", \"Operation\": \"scoring\"}, \"Metrics\": {\"model.evaluate.time\": {\"sum\": 0.7317066192626953, \"count\": 1, \"min\": 0.7317066192626953, \"max\": 0.7317066192626953}, \"json.encoder.time\": {\"sum\": 28.41973304748535, \"count\": 1, \"min\": 28.41973304748535, \"max\": 28.41973304748535}, \"invocations.count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1646310109.5801182, \"EndTime\": 1646310109.7002532, \"Dimensions\": {\"Algorithm\": \"NVDMModel\", \"Host\": \"UNKNOWN\", \"Operation\": \"scoring\"}, \"Metrics\": {\"model.evaluate.time\": {\"sum\": 2.6187896728515625, \"count\": 1, \"min\": 2.6187896728515625, \"max\": 2.6187896728515625}, \"json.encoder.time\": {\"sum\": 5.117654800415039, \"count\": 1, \"min\": 5.117654800415039, \"max\": 5.117654800415039}, \"invocations.count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "np.savetxt('trainvectors.csv',\n",
    "           vectors.todense(),\n",
    "           delimiter=',',\n",
    "           fmt='%i')\n",
    "batch_prefix = '20newsgroups/batch'\n",
    "\n",
    "train_s3 = sess.upload_data('trainvectors.csv', \n",
    "                            bucket=bucket, \n",
    "                            key_prefix='{}/train'.format(batch_prefix))\n",
    "print(train_s3)\n",
    "batch_output_path = 's3://{}/{}/test'.format(bucket, batch_prefix)\n",
    "\n",
    "ntm_transformer = ntm.transformer(instance_count=1,\n",
    "                                  instance_type ='ml.m4.xlarge',\n",
    "                                  output_path=batch_output_path\n",
    "                                 )\n",
    "ntm_transformer.transform(train_s3, content_type='text/csv', split_type='Line')\n",
    "ntm_transformer.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a253d538",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c825f90",
   "metadata": {},
   "source": [
    "Once the transform job is done, you can use the following code to download the outputs back to your local notebook instance for inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ce45c958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-recom-bucket/20newsgroups/batch/test/trainvectors.csv.out to ./trainvectors.csv.out\n",
      "{\"topic_weights\":[0.0254854951,0.0303337499,0.0302192811,0.029727269,0.0350844637,0.0186748207,0.0325257555,0.0344059207,0.0334466733,0.0327919312,0.0833307952,0.0270293597,0.0264614262,0.0269578565,0.0307428613,0.0276742168,0.0181090757,0.0321479477,0.0312982909,0.0215841737,0.0291812774,0.0311863758,0.0328368284,0.0299315043,0.0320728421,0.0287800785,0.031023033,0.0321675204,0.0332737155,0.0915155336]}\n",
      "{\"topic_weights\":[0.0297647156,0.0332522057,0.0264585316,0.0372286253,0.0199021511,0.0184025634,0.0330165662,0.0369104892,0.0347854346,0.0375940986,0.0240096152,0.024134431,0.1383230835,0.0292924866,0.0282232128,0.0303283501,0.0271595046,0.0314200334,0.0292207859,0.0201016199,0.0322106555,0.0287737418,0.0319978893,0.0369888172,0.0303008892,0.0273630843,0.0277077928,0.0411639884,0.0358183533,0.018146228]}\n",
      "{\"topic_weights\":[0.0136012305,0.0253070388,0.0207016431,0.1085734591,0.1908048093,0.0128779327,0.0168665256,0.0194721408,0.0160860494,0.0282421131,0.0246154871,0.0122959055,0.0365113877,0.012329353,0.0189570077,0.0120461937,0.0707505196,0.0158803016,0.0208110102,0.0966751799,0.0364158265,0.0267763585,0.0195793584,0.014779144,0.0189977195,0.0258398168,0.0166617315,0.0366386026,0.0166638289,0.0142421396]}\n",
      "{\"topic_weights\":[0.0388745777,0.0304892771,0.0303126853,0.0301783215,0.025279019,0.0241878442,0.0299601462,0.0308615994,0.0292287469,0.0304349139,0.1245399714,0.0305664614,0.0226017796,0.0324733891,0.0314259268,0.0326884948,0.0339272767,0.0312403608,0.0304119382,0.0258356892,0.0297486968,0.0294611361,0.0320906043,0.0326777026,0.030471988,0.0305701327,0.0303640123,0.03088082,0.0306903291,0.0275261514]}\n",
      "{\"topic_weights\":[0.0179976579,0.0229760073,0.0257202033,0.0203589872,0.062961176,0.0633624122,0.0242387615,0.0249201898,0.0230551623,0.0229527336,0.0163905285,0.0234586429,0.0232422911,0.0206668824,0.0252430346,0.0192691702,0.0167513266,0.0239809621,0.0235359687,0.0773691982,0.0224299785,0.0269634724,0.0246885046,0.0197826624,0.0263110641,0.025234567,0.0268176049,0.0216169879,0.0223589074,0.2053449303]}\n",
      "{\"topic_weights\":[0.0687848851,0.0224014223,0.0314255767,0.0113606192,0.012803873,0.0134588545,0.0223131347,0.0220238343,0.0229236744,0.0174279194,0.2037509382,0.044414781,0.0101461066,0.0370026976,0.032667771,0.0277037136,0.0632820874,0.0270366501,0.0289032627,0.0370361805,0.0173721723,0.0263593961,0.0293217413,0.0253086649,0.0305688418,0.0263292994,0.0309628211,0.0134644369,0.0226196833,0.0208249334]}\n",
      "{\"topic_weights\":[0.0244727898,0.03221884,0.0338711403,0.0326177962,0.0372323394,0.0437968969,0.0306575261,0.0309299063,0.02997078,0.0317688286,0.0689824298,0.0326199904,0.0273640323,0.0305606797,0.0337662026,0.0305079985,0.0258073043,0.0319509283,0.0334995203,0.0367173143,0.031833712,0.0329365581,0.033757709,0.031195987,0.0325867459,0.033847224,0.0324973613,0.0317247622,0.0307733417,0.0295334607]}\n",
      "{\"topic_weights\":[0.1690673977,0.0242581181,0.0213592015,0.0238780323,0.0534084328,0.0155990692,0.0250372216,0.0283366702,0.0247053616,0.026195243,0.0826539919,0.0189776253,0.0166396406,0.0221059471,0.0230753478,0.0207901597,0.0221440997,0.0238651726,0.0220491569,0.0165939219,0.0228259005,0.0226839986,0.0238109063,0.0244290046,0.0245919004,0.0210824944,0.0219268426,0.0257078744,0.0266262144,0.1055750847]}\n",
      "{\"topic_weights\":[0.1955300719,0.0185559802,0.0206674635,0.0156590845,0.0136218723,0.0146212317,0.0283321328,0.0214156602,0.0216176677,0.0187556576,0.0489385985,0.0277751237,0.2180535495,0.021950867,0.0216063894,0.016753979,0.0208570827,0.0246471073,0.0197059773,0.0188508648,0.0172858406,0.0193145256,0.0197122227,0.0199191794,0.0215196088,0.0193998367,0.0227971952,0.0166414659,0.0216697343,0.0138239842]}\n",
      "{\"topic_weights\":[0.0250813644,0.0304100383,0.0291548558,0.0326907188,0.1681012213,0.0197630487,0.031645067,0.0301890001,0.0294409189,0.0307590868,0.0193148423,0.0279421192,0.0333855413,0.027809184,0.0299085937,0.0270841885,0.02549267,0.0302992892,0.0296764746,0.0258731246,0.0301057268,0.0288654398,0.0304420292,0.0294386968,0.0292107314,0.0296658557,0.0293894876,0.0315298289,0.0311646387,0.0261663906]}\n",
      "{\"topic_weights\":[0.026117852,0.0281359013,0.030810969,0.0253194738,0.0385547876,0.0917337313,0.0322725475,0.0310912002,0.0299229845,0.0281291623,0.0326054804,0.0308633205,0.0634302795,0.0267977938,0.030177122,0.0245397147,0.0216048323,0.0306898914,0.0291622635,0.0358456224,0.0276475064,0.0307583231,0.0299659092,0.0262516811,0.0319465101,0.0294528324,0.0316602662,0.0271941405,0.028665917,0.0486520156]}\n",
      "{\"topic_weights\":[0.0451771207,0.0267946478,0.0273471512,0.0257687382,0.0198471379,0.0190680753,0.0345610194,0.0311295316,0.0319692157,0.0290190354,0.0196816511,0.030563863,0.1739955693,0.0294712689,0.0279374793,0.0287767146,0.022328116,0.0318948627,0.0281857457,0.0212404933,0.026457468,0.0270397831,0.0291399322,0.0295385551,0.0287898611,0.0267871153,0.0300422627,0.0278836098,0.0313082002,0.0382557064]}\n",
      "{\"topic_weights\":[0.1950316727,0.0284592994,0.0232922565,0.0309499241,0.0155138206,0.0270830635,0.0283979177,0.031988"
     ]
    }
   ],
   "source": [
    "!aws s3 cp --recursive $ntm_transformer.output_path ./\n",
    "!head -c 5000 trainvectors.csv.out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba41a8ca",
   "metadata": {},
   "source": [
    "### 3. Explore the topic model\n",
    "\n",
    "One approach for exploring the model outputs is to visualize the topic vectors generated using a T-SNE plot. A T-SNE, or t-Distributed Stochastic Neighbor Embedding, is a non-linear technique for dimensionality reduction which aims to ensure that the distance between nearest neighbors in the original high dimensional space is preserved in the resulting lower dimensional space. By setting the number of dimensions to 2, it can be used as an visualization tool to visualize the topic vectors in 2D space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8a08289b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6bbaae1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing 151 nearest neighbors...\n",
      "[t-SNE] Indexed 11314 samples in 0.043s...\n",
      "[t-SNE] Computed neighbors for 11314 samples in 4.328s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 11314\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 11314\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 11314\n",
      "[t-SNE] Computed conditional probabilities for sample 4000 / 11314\n",
      "[t-SNE] Computed conditional probabilities for sample 5000 / 11314\n",
      "[t-SNE] Computed conditional probabilities for sample 6000 / 11314\n",
      "[t-SNE] Computed conditional probabilities for sample 7000 / 11314\n",
      "[t-SNE] Computed conditional probabilities for sample 8000 / 11314\n",
      "[t-SNE] Computed conditional probabilities for sample 9000 / 11314\n",
      "[t-SNE] Computed conditional probabilities for sample 10000 / 11314\n",
      "[t-SNE] Computed conditional probabilities for sample 11000 / 11314\n",
      "[t-SNE] Computed conditional probabilities for sample 11314 / 11314\n",
      "[t-SNE] Mean sigma: 0.000000\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 82.522469\n",
      "[t-SNE] KL divergence after 5000 iterations: 1.492504\n",
      "t-SNE done! Time elapsed: 386.1935315132141 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x1800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 530.375x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "time_start = time.time()\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=50, n_iter=5000)\n",
    "tsne_results = tsne.fit_transform(predictions)\n",
    "print('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))\n",
    "tsnedf = pd.DataFrame()\n",
    "tsnedf['tsne-2d-one'] = tsne_results[:,0]\n",
    "tsnedf['tsne-2d-two'] = tsne_results[:,1]\n",
    "tsnedf['Topic']=topicnames\n",
    "plt.figure(figsize=(25,25))\n",
    "sns.lmplot(\n",
    "    x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n",
    "    hue='Topic',\n",
    "    palette=sns.color_palette(\"hls\", NUM_TOPICS),\n",
    "    data=tsnedf,\n",
    "    legend=\"full\",\n",
    "    fit_reg=False\n",
    ")\n",
    "plt.axis('Off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96abe22",
   "metadata": {},
   "source": [
    "The TSNE plot should show some large topic clusters like the following image. Plots like these can be used to extract the number of distinct topic clusters in the dataset. Currently, NUM_TOPICS is set to 30, but there appear to be a lot of topics that are close to each other in the TSNE plot and may be combined into a single topic. Ultimately, as topic modeling is largely an unsupervised learning problem, you must use visualizations such as these to determine what is the right number of topics to partition the dataset into.\n",
    "\n",
    "Try experimenting with different topic numbers to see what the visualization looks like."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35732599",
   "metadata": {},
   "source": [
    "## Train and deploy the content recommendation model\n",
    "\n",
    "In this module, you use the built-in Amazon SageMaker k-Nearest Neighbors (k-NN) Algorithm to train the content recommendation model.\n",
    "\n",
    "[Amazon SageMaker K-Nearest Neighbors (k-NN)](https://docs.aws.amazon.com/sagemaker/latest/dg/k-nearest-neighbors.html) is a non-parametric, index-based, supervised learning algorithm that can be used for classification and regression tasks. For classification, the algorithm queries the k closest points to the target and returns the most frequently used label of their class as the predicted label. For regression problems, the algorithm returns the average of the predicted values returned by the k closest neighbors.\n",
    "\n",
    "Training with the k-NN algorithm has three steps: sampling, dimension reduction, and index building. Sampling reduces the size of the initial dataset so that it fits into memory. For dimension reduction, the algorithm decreases the feature dimension of the data to reduce the footprint of the k-NN model in memory and inference latency. We provide two methods of dimension reduction methods: random projection and the fast Johnson-Lindenstrauss transform. Typically, you use dimension reduction for high-dimensional (d >1000) datasets to avoid the “curse of dimensionality” that troubles the statistical analysis of data that becomes sparse as dimensionality increases. The main objective of k-NN's training is to construct the index. The index enables efficient lookups of distances between points whose values or class labels have not yet been determined and the k nearest points to use for inference.\n",
    "In the following steps, you specify your k-NN algorithm for the training job, set the hyperparameter values to tune the model, and run the model. Then, you deploy the model to an endpoint managed by Amazon SageMaker to make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806dd8f8",
   "metadata": {},
   "source": [
    "### 1. Create and run the training job\n",
    "\n",
    "In the previous module, you created topic vectors. In this module, you build and deploy the content recommendation module which retains an index of the topic vectors.\n",
    "\n",
    "First, create a dictionary which links the shuffled labels to the original labels in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "181305e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{10740: 0,\n",
       " 6572: 1,\n",
       " 6839: 2,\n",
       " 354: 3,\n",
       " 1641: 4,\n",
       " 5200: 5,\n",
       " 7625: 6,\n",
       " 10838: 7,\n",
       " 1954: 8,\n",
       " 8421: 9,\n",
       " 6805: 10,\n",
       " 5134: 11,\n",
       " 6853: 12,\n",
       " 723: 13,\n",
       " 3380: 14,\n",
       " 6203: 15,\n",
       " 7585: 16,\n",
       " 8866: 17,\n",
       " 5021: 18,\n",
       " 10154: 19,\n",
       " 1609: 20,\n",
       " 3924: 21,\n",
       " 10744: 22,\n",
       " 5870: 23,\n",
       " 136: 24,\n",
       " 5396: 25,\n",
       " 681: 26,\n",
       " 9185: 27,\n",
       " 1936: 28,\n",
       " 7656: 29,\n",
       " 10877: 30,\n",
       " 3457: 31,\n",
       " 8761: 32,\n",
       " 9945: 33,\n",
       " 6434: 34,\n",
       " 668: 35,\n",
       " 7274: 36,\n",
       " 4256: 37,\n",
       " 9926: 38,\n",
       " 6627: 39,\n",
       " 3881: 40,\n",
       " 10920: 41,\n",
       " 9179: 42,\n",
       " 2401: 43,\n",
       " 562: 44,\n",
       " 9327: 45,\n",
       " 3619: 46,\n",
       " 4090: 47,\n",
       " 4543: 48,\n",
       " 5311: 49,\n",
       " 2189: 50,\n",
       " 7770: 51,\n",
       " 9026: 52,\n",
       " 3112: 53,\n",
       " 10623: 54,\n",
       " 6776: 55,\n",
       " 1606: 56,\n",
       " 11221: 57,\n",
       " 3688: 58,\n",
       " 4597: 59,\n",
       " 10242: 60,\n",
       " 2834: 61,\n",
       " 9755: 62,\n",
       " 1529: 63,\n",
       " 10603: 64,\n",
       " 296: 65,\n",
       " 2643: 66,\n",
       " 6674: 67,\n",
       " 9828: 68,\n",
       " 9298: 69,\n",
       " 5421: 70,\n",
       " 2142: 71,\n",
       " 9633: 72,\n",
       " 1955: 73,\n",
       " 22: 74,\n",
       " 6234: 75,\n",
       " 1053: 76,\n",
       " 930: 77,\n",
       " 7407: 78,\n",
       " 11251: 79,\n",
       " 3095: 80,\n",
       " 3453: 81,\n",
       " 9045: 82,\n",
       " 4330: 83,\n",
       " 8176: 84,\n",
       " 8430: 85,\n",
       " 2358: 86,\n",
       " 11118: 87,\n",
       " 2241: 88,\n",
       " 464: 89,\n",
       " 713: 90,\n",
       " 9902: 91,\n",
       " 3774: 92,\n",
       " 10274: 93,\n",
       " 5360: 94,\n",
       " 11312: 95,\n",
       " 2752: 96,\n",
       " 6882: 97,\n",
       " 1637: 98,\n",
       " 7245: 99,\n",
       " 10732: 100,\n",
       " 8480: 101,\n",
       " 7111: 102,\n",
       " 470: 103,\n",
       " 8627: 104,\n",
       " 1604: 105,\n",
       " 9037: 106,\n",
       " 209: 107,\n",
       " 1962: 108,\n",
       " 7291: 109,\n",
       " 3127: 110,\n",
       " 1942: 111,\n",
       " 11053: 112,\n",
       " 1279: 113,\n",
       " 10078: 114,\n",
       " 1319: 115,\n",
       " 3102: 116,\n",
       " 8597: 117,\n",
       " 10983: 118,\n",
       " 6939: 119,\n",
       " 2907: 120,\n",
       " 337: 121,\n",
       " 4603: 122,\n",
       " 11015: 123,\n",
       " 8137: 124,\n",
       " 8953: 125,\n",
       " 7196: 126,\n",
       " 1854: 127,\n",
       " 6993: 128,\n",
       " 9374: 129,\n",
       " 8864: 130,\n",
       " 3002: 131,\n",
       " 8527: 132,\n",
       " 7511: 133,\n",
       " 1932: 134,\n",
       " 9016: 135,\n",
       " 7458: 136,\n",
       " 4722: 137,\n",
       " 1796: 138,\n",
       " 6128: 139,\n",
       " 1499: 140,\n",
       " 526: 141,\n",
       " 320: 142,\n",
       " 8516: 143,\n",
       " 9484: 144,\n",
       " 6212: 145,\n",
       " 4089: 146,\n",
       " 8628: 147,\n",
       " 2723: 148,\n",
       " 9702: 149,\n",
       " 9383: 150,\n",
       " 10437: 151,\n",
       " 4983: 152,\n",
       " 5026: 153,\n",
       " 8618: 154,\n",
       " 7712: 155,\n",
       " 2665: 156,\n",
       " 7966: 157,\n",
       " 7683: 158,\n",
       " 7257: 159,\n",
       " 4773: 160,\n",
       " 6366: 161,\n",
       " 10786: 162,\n",
       " 6119: 163,\n",
       " 10918: 164,\n",
       " 8502: 165,\n",
       " 2638: 166,\n",
       " 1737: 167,\n",
       " 4430: 168,\n",
       " 946: 169,\n",
       " 9631: 170,\n",
       " 6147: 171,\n",
       " 3143: 172,\n",
       " 6228: 173,\n",
       " 7972: 174,\n",
       " 3083: 175,\n",
       " 6449: 176,\n",
       " 11195: 177,\n",
       " 958: 178,\n",
       " 3623: 179,\n",
       " 4261: 180,\n",
       " 10741: 181,\n",
       " 10037: 182,\n",
       " 11031: 183,\n",
       " 3981: 184,\n",
       " 10334: 185,\n",
       " 987: 186,\n",
       " 6193: 187,\n",
       " 2642: 188,\n",
       " 7954: 189,\n",
       " 4950: 190,\n",
       " 3302: 191,\n",
       " 10539: 192,\n",
       " 9956: 193,\n",
       " 745: 194,\n",
       " 2460: 195,\n",
       " 4546: 196,\n",
       " 728: 197,\n",
       " 3577: 198,\n",
       " 9147: 199,\n",
       " 3007: 200,\n",
       " 366: 201,\n",
       " 38: 202,\n",
       " 6339: 203,\n",
       " 2013: 204,\n",
       " 6514: 205,\n",
       " 9931: 206,\n",
       " 1426: 207,\n",
       " 1948: 208,\n",
       " 2069: 209,\n",
       " 8917: 210,\n",
       " 1448: 211,\n",
       " 3299: 212,\n",
       " 40: 213,\n",
       " 9011: 214,\n",
       " 7582: 215,\n",
       " 3987: 216,\n",
       " 4071: 217,\n",
       " 4343: 218,\n",
       " 3398: 219,\n",
       " 294: 220,\n",
       " 10245: 221,\n",
       " 7907: 222,\n",
       " 3778: 223,\n",
       " 10236: 224,\n",
       " 7849: 225,\n",
       " 8000: 226,\n",
       " 7780: 227,\n",
       " 4841: 228,\n",
       " 641: 229,\n",
       " 9693: 230,\n",
       " 8562: 231,\n",
       " 10224: 232,\n",
       " 1798: 233,\n",
       " 1328: 234,\n",
       " 7172: 235,\n",
       " 4356: 236,\n",
       " 1717: 237,\n",
       " 5274: 238,\n",
       " 8116: 239,\n",
       " 5083: 240,\n",
       " 6746: 241,\n",
       " 6699: 242,\n",
       " 3009: 243,\n",
       " 11250: 244,\n",
       " 56: 245,\n",
       " 3605: 246,\n",
       " 8004: 247,\n",
       " 3354: 248,\n",
       " 10512: 249,\n",
       " 2798: 250,\n",
       " 10611: 251,\n",
       " 6724: 252,\n",
       " 8771: 253,\n",
       " 893: 254,\n",
       " 1780: 255,\n",
       " 4735: 256,\n",
       " 858: 257,\n",
       " 4277: 258,\n",
       " 3721: 259,\n",
       " 8996: 260,\n",
       " 788: 261,\n",
       " 11223: 262,\n",
       " 6906: 263,\n",
       " 7860: 264,\n",
       " 7430: 265,\n",
       " 4104: 266,\n",
       " 5380: 267,\n",
       " 1181: 268,\n",
       " 8434: 269,\n",
       " 3341: 270,\n",
       " 7106: 271,\n",
       " 5664: 272,\n",
       " 9561: 273,\n",
       " 1410: 274,\n",
       " 8028: 275,\n",
       " 322: 276,\n",
       " 4631: 277,\n",
       " 3222: 278,\n",
       " 1265: 279,\n",
       " 2623: 280,\n",
       " 686: 281,\n",
       " 6222: 282,\n",
       " 5209: 283,\n",
       " 7452: 284,\n",
       " 9541: 285,\n",
       " 8433: 286,\n",
       " 2159: 287,\n",
       " 5404: 288,\n",
       " 2169: 289,\n",
       " 11019: 290,\n",
       " 1541: 291,\n",
       " 6337: 292,\n",
       " 2009: 293,\n",
       " 3384: 294,\n",
       " 266: 295,\n",
       " 11013: 296,\n",
       " 4249: 297,\n",
       " 5611: 298,\n",
       " 276: 299,\n",
       " 515: 300,\n",
       " 9869: 301,\n",
       " 9155: 302,\n",
       " 4779: 303,\n",
       " 3166: 304,\n",
       " 6427: 305,\n",
       " 1919: 306,\n",
       " 9316: 307,\n",
       " 4848: 308,\n",
       " 4890: 309,\n",
       " 7729: 310,\n",
       " 7763: 311,\n",
       " 9282: 312,\n",
       " 7065: 313,\n",
       " 1103: 314,\n",
       " 5108: 315,\n",
       " 4670: 316,\n",
       " 7528: 317,\n",
       " 947: 318,\n",
       " 8642: 319,\n",
       " 7030: 320,\n",
       " 9856: 321,\n",
       " 5480: 322,\n",
       " 555: 323,\n",
       " 1219: 324,\n",
       " 1850: 325,\n",
       " 6540: 326,\n",
       " 5008: 327,\n",
       " 3210: 328,\n",
       " 10253: 329,\n",
       " 5051: 330,\n",
       " 6402: 331,\n",
       " 10479: 332,\n",
       " 6854: 333,\n",
       " 167: 334,\n",
       " 8915: 335,\n",
       " 63: 336,\n",
       " 174: 337,\n",
       " 6912: 338,\n",
       " 8890: 339,\n",
       " 10894: 340,\n",
       " 1242: 341,\n",
       " 9901: 342,\n",
       " 172: 343,\n",
       " 8290: 344,\n",
       " 7042: 345,\n",
       " 1812: 346,\n",
       " 606: 347,\n",
       " 8252: 348,\n",
       " 3346: 349,\n",
       " 1872: 350,\n",
       " 1456: 351,\n",
       " 4295: 352,\n",
       " 4068: 353,\n",
       " 2072: 354,\n",
       " 7976: 355,\n",
       " 7830: 356,\n",
       " 853: 357,\n",
       " 4078: 358,\n",
       " 7700: 359,\n",
       " 10544: 360,\n",
       " 3592: 361,\n",
       " 8870: 362,\n",
       " 10576: 363,\n",
       " 424: 364,\n",
       " 5055: 365,\n",
       " 9640: 366,\n",
       " 8737: 367,\n",
       " 2812: 368,\n",
       " 8577: 369,\n",
       " 8804: 370,\n",
       " 5212: 371,\n",
       " 10514: 372,\n",
       " 6764: 373,\n",
       " 9820: 374,\n",
       " 10218: 375,\n",
       " 4446: 376,\n",
       " 5123: 377,\n",
       " 3527: 378,\n",
       " 2384: 379,\n",
       " 4693: 380,\n",
       " 2831: 381,\n",
       " 2500: 382,\n",
       " 873: 383,\n",
       " 5456: 384,\n",
       " 6009: 385,\n",
       " 5057: 386,\n",
       " 8374: 387,\n",
       " 6206: 388,\n",
       " 5343: 389,\n",
       " 4228: 390,\n",
       " 9668: 391,\n",
       " 10947: 392,\n",
       " 5257: 393,\n",
       " 11257: 394,\n",
       " 7203: 395,\n",
       " 2685: 396,\n",
       " 10729: 397,\n",
       " 4456: 398,\n",
       " 490: 399,\n",
       " 1535: 400,\n",
       " 8818: 401,\n",
       " 4101: 402,\n",
       " 163: 403,\n",
       " 8537: 404,\n",
       " 1299: 405,\n",
       " 5525: 406,\n",
       " 255: 407,\n",
       " 7807: 408,\n",
       " 34: 409,\n",
       " 2311: 410,\n",
       " 9957: 411,\n",
       " 8397: 412,\n",
       " 3546: 413,\n",
       " 10651: 414,\n",
       " 5530: 415,\n",
       " 7719: 416,\n",
       " 4510: 417,\n",
       " 7664: 418,\n",
       " 234: 419,\n",
       " 10506: 420,\n",
       " 3912: 421,\n",
       " 5166: 422,\n",
       " 9391: 423,\n",
       " 10553: 424,\n",
       " 3905: 425,\n",
       " 577: 426,\n",
       " 9424: 427,\n",
       " 8664: 428,\n",
       " 6309: 429,\n",
       " 10997: 430,\n",
       " 10311: 431,\n",
       " 11114: 432,\n",
       " 3275: 433,\n",
       " 8265: 434,\n",
       " 10123: 435,\n",
       " 6856: 436,\n",
       " 2710: 437,\n",
       " 4771: 438,\n",
       " 4645: 439,\n",
       " 5787: 440,\n",
       " 8001: 441,\n",
       " 7630: 442,\n",
       " 142: 443,\n",
       " 7092: 444,\n",
       " 3510: 445,\n",
       " 8896: 446,\n",
       " 367: 447,\n",
       " 2742: 448,\n",
       " 8323: 449,\n",
       " 8371: 450,\n",
       " 3334: 451,\n",
       " 1635: 452,\n",
       " 1575: 453,\n",
       " 42: 454,\n",
       " 8832: 455,\n",
       " 8666: 456,\n",
       " 7802: 457,\n",
       " 5454: 458,\n",
       " 7687: 459,\n",
       " 7401: 460,\n",
       " 3465: 461,\n",
       " 10962: 462,\n",
       " 1397: 463,\n",
       " 7460: 464,\n",
       " 8201: 465,\n",
       " 6648: 466,\n",
       " 6174: 467,\n",
       " 10626: 468,\n",
       " 3170: 469,\n",
       " 2799: 470,\n",
       " 1286: 471,\n",
       " 775: 472,\n",
       " 2862: 473,\n",
       " 6957: 474,\n",
       " 5448: 475,\n",
       " 11000: 476,\n",
       " 5137: 477,\n",
       " 1519: 478,\n",
       " 2877: 479,\n",
       " 10156: 480,\n",
       " 5596: 481,\n",
       " 10869: 482,\n",
       " 11161: 483,\n",
       " 6392: 484,\n",
       " 6146: 485,\n",
       " 10910: 486,\n",
       " 1182: 487,\n",
       " 1193: 488,\n",
       " 6666: 489,\n",
       " 10563: 490,\n",
       " 1795: 491,\n",
       " 901: 492,\n",
       " 6783: 493,\n",
       " 7315: 494,\n",
       " 160: 495,\n",
       " 3128: 496,\n",
       " 1032: 497,\n",
       " 1092: 498,\n",
       " 1736: 499,\n",
       " 998: 500,\n",
       " 5979: 501,\n",
       " 5447: 502,\n",
       " 1966: 503,\n",
       " 1557: 504,\n",
       " 6071: 505,\n",
       " 6670: 506,\n",
       " 6500: 507,\n",
       " 4300: 508,\n",
       " 1019: 509,\n",
       " 1323: 510,\n",
       " 10671: 511,\n",
       " 4473: 512,\n",
       " 3835: 513,\n",
       " 983: 514,\n",
       " 6535: 515,\n",
       " 9844: 516,\n",
       " 9579: 517,\n",
       " 6215: 518,\n",
       " 5971: 519,\n",
       " 3761: 520,\n",
       " 2781: 521,\n",
       " 9352: 522,\n",
       " 10832: 523,\n",
       " 6063: 524,\n",
       " 3887: 525,\n",
       " 543: 526,\n",
       " 507: 527,\n",
       " 4073: 528,\n",
       " 10513: 529,\n",
       " 2857: 530,\n",
       " 4574: 531,\n",
       " 7671: 532,\n",
       " 3652: 533,\n",
       " 2976: 534,\n",
       " 10812: 535,\n",
       " 10926: 536,\n",
       " 5140: 537,\n",
       " 11163: 538,\n",
       " 10695: 539,\n",
       " 1979: 540,\n",
       " 7986: 541,\n",
       " 11243: 542,\n",
       " 3846: 543,\n",
       " 6831: 544,\n",
       " 1046: 545,\n",
       " 10885: 546,\n",
       " 1674: 547,\n",
       " 5081: 548,\n",
       " 1617: 549,\n",
       " 7996: 550,\n",
       " 6871: 551,\n",
       " 5133: 552,\n",
       " 7492: 553,\n",
       " 1302: 554,\n",
       " 2444: 555,\n",
       " 1467: 556,\n",
       " 7074: 557,\n",
       " 9886: 558,\n",
       " 6314: 559,\n",
       " 346: 560,\n",
       " 1549: 561,\n",
       " 2514: 562,\n",
       " 1788: 563,\n",
       " 4671: 564,\n",
       " 8782: 565,\n",
       " 3049: 566,\n",
       " 10472: 567,\n",
       " 6387: 568,\n",
       " 3574: 569,\n",
       " 3320: 570,\n",
       " 6109: 571,\n",
       " 6097: 572,\n",
       " 9703: 573,\n",
       " 9071: 574,\n",
       " 8508: 575,\n",
       " 3583: 576,\n",
       " 4974: 577,\n",
       " 5015: 578,\n",
       " 8687: 579,\n",
       " 5056: 580,\n",
       " 9167: 581,\n",
       " 87: 582,\n",
       " 6459: 583,\n",
       " 10517: 584,\n",
       " 10737: 585,\n",
       " 8295: 586,\n",
       " 11045: 587,\n",
       " 1823: 588,\n",
       " 4183: 589,\n",
       " 143: 590,\n",
       " 7720: 591,\n",
       " 88: 592,\n",
       " 6849: 593,\n",
       " 8839: 594,\n",
       " 8123: 595,\n",
       " 6447: 596,\n",
       " 10470: 597,\n",
       " 3531: 598,\n",
       " 9666: 599,\n",
       " 9626: 600,\n",
       " 11065: 601,\n",
       " 9379: 602,\n",
       " 9597: 603,\n",
       " 9331: 604,\n",
       " 6787: 605,\n",
       " 9319: 606,\n",
       " 1437: 607,\n",
       " 5513: 608,\n",
       " 9850: 609,\n",
       " 5595: 610,\n",
       " 4485: 611,\n",
       " 9794: 612,\n",
       " 9093: 613,\n",
       " 4569: 614,\n",
       " 2671: 615,\n",
       " 6114: 616,\n",
       " 2259: 617,\n",
       " 1713: 618,\n",
       " 438: 619,\n",
       " 6194: 620,\n",
       " 10077: 621,\n",
       " 7246: 622,\n",
       " 9036: 623,\n",
       " 456: 624,\n",
       " 10103: 625,\n",
       " 8533: 626,\n",
       " 8117: 627,\n",
       " 9554: 628,\n",
       " 5367: 629,\n",
       " 1021: 630,\n",
       " 10463: 631,\n",
       " 7652: 632,\n",
       " 5319: 633,\n",
       " 9847: 634,\n",
       " 11248: 635,\n",
       " 10630: 636,\n",
       " 787: 637,\n",
       " 3614: 638,\n",
       " 10792: 639,\n",
       " 5667: 640,\n",
       " 1425: 641,\n",
       " 8768: 642,\n",
       " 2897: 643,\n",
       " 6450: 644,\n",
       " 10009: 645,\n",
       " 10047: 646,\n",
       " 4615: 647,\n",
       " 1699: 648,\n",
       " 3712: 649,\n",
       " 11237: 650,\n",
       " 2039: 651,\n",
       " 2025: 652,\n",
       " 8047: 653,\n",
       " 612: 654,\n",
       " 3999: 655,\n",
       " 500: 656,\n",
       " 1250: 657,\n",
       " 6983: 658,\n",
       " 5860: 659,\n",
       " 10860: 660,\n",
       " 1924: 661,\n",
       " 581: 662,\n",
       " 9801: 663,\n",
       " 10974: 664,\n",
       " 10352: 665,\n",
       " 5956: 666,\n",
       " 6027: 667,\n",
       " 6604: 668,\n",
       " 53: 669,\n",
       " 9431: 670,\n",
       " 2611: 671,\n",
       " 2021: 672,\n",
       " 4287: 673,\n",
       " 10625: 674,\n",
       " 2856: 675,\n",
       " 10484: 676,\n",
       " 2757: 677,\n",
       " 335: 678,\n",
       " 39: 679,\n",
       " 3463: 680,\n",
       " 6911: 681,\n",
       " 10929: 682,\n",
       " 5505: 683,\n",
       " 2690: 684,\n",
       " 5583: 685,\n",
       " 8344: 686,\n",
       " 1222: 687,\n",
       " 1603: 688,\n",
       " 10927: 689,\n",
       " 8332: 690,\n",
       " 11187: 691,\n",
       " 9060: 692,\n",
       " 7815: 693,\n",
       " 7560: 694,\n",
       " 3129: 695,\n",
       " 6881: 696,\n",
       " 6331: 697,\n",
       " 2825: 698,\n",
       " 1523: 699,\n",
       " 6258: 700,\n",
       " 6145: 701,\n",
       " 1935: 702,\n",
       " 10206: 703,\n",
       " 5845: 704,\n",
       " 2635: 705,\n",
       " 6632: 706,\n",
       " 4716: 707,\n",
       " 6267: 708,\n",
       " 10349: 709,\n",
       " 9068: 710,\n",
       " 1301: 711,\n",
       " 6992: 712,\n",
       " 8183: 713,\n",
       " 8970: 714,\n",
       " 7935: 715,\n",
       " 1810: 716,\n",
       " 10412: 717,\n",
       " 10194: 718,\n",
       " 10720: 719,\n",
       " 9500: 720,\n",
       " 3662: 721,\n",
       " 9876: 722,\n",
       " 1566: 723,\n",
       " 10062: 724,\n",
       " 5578: 725,\n",
       " 7370: 726,\n",
       " 186: 727,\n",
       " 2932: 728,\n",
       " 5539: 729,\n",
       " 5989: 730,\n",
       " 9878: 731,\n",
       " 3793: 732,\n",
       " 3896: 733,\n",
       " 8263: 734,\n",
       " 2248: 735,\n",
       " 3672: 736,\n",
       " 1137: 737,\n",
       " 4912: 738,\n",
       " 502: 739,\n",
       " 9141: 740,\n",
       " 10203: 741,\n",
       " 3506: 742,\n",
       " 6180: 743,\n",
       " 5751: 744,\n",
       " 7128: 745,\n",
       " 11263: 746,\n",
       " 3437: 747,\n",
       " 10320: 748,\n",
       " 2277: 749,\n",
       " 2314: 750,\n",
       " 9295: 751,\n",
       " 4361: 752,\n",
       " 6626: 753,\n",
       " 4387: 754,\n",
       " 6703: 755,\n",
       " 4665: 756,\n",
       " 1090: 757,\n",
       " 6633: 758,\n",
       " 4878: 759,\n",
       " 4214: 760,\n",
       " 7957: 761,\n",
       " 3997: 762,\n",
       " 6485: 763,\n",
       " 1261: 764,\n",
       " 3775: 765,\n",
       " 3157: 766,\n",
       " 10069: 767,\n",
       " 4467: 768,\n",
       " 9742: 769,\n",
       " 530: 770,\n",
       " 8227: 771,\n",
       " 7795: 772,\n",
       " 11038: 773,\n",
       " 176: 774,\n",
       " 1764: 775,\n",
       " 1873: 776,\n",
       " 5874: 777,\n",
       " 5718: 778,\n",
       " 3973: 779,\n",
       " 4234: 780,\n",
       " 3259: 781,\n",
       " 7891: 782,\n",
       " 5660: 783,\n",
       " 10886: 784,\n",
       " 3058: 785,\n",
       " 755: 786,\n",
       " 4185: 787,\n",
       " 1102: 788,\n",
       " 11233: 789,\n",
       " 5043: 790,\n",
       " 1068: 791,\n",
       " 10271: 792,\n",
       " 9925: 793,\n",
       " 6904: 794,\n",
       " 7422: 795,\n",
       " 4289: 796,\n",
       " 1174: 797,\n",
       " 7463: 798,\n",
       " 5308: 799,\n",
       " 936: 800,\n",
       " 6676: 801,\n",
       " 3535: 802,\n",
       " 13: 803,\n",
       " 3609: 804,\n",
       " 170: 805,\n",
       " 3362: 806,\n",
       " 11155: 807,\n",
       " 2699: 808,\n",
       " 6487: 809,\n",
       " 1690: 810,\n",
       " 761: 811,\n",
       " 1827: 812,\n",
       " 3740: 813,\n",
       " 6328: 814,\n",
       " 8563: 815,\n",
       " 8307: 816,\n",
       " 2930: 817,\n",
       " 10823: 818,\n",
       " 9394: 819,\n",
       " 466: 820,\n",
       " 9540: 821,\n",
       " 7894: 822,\n",
       " 4346: 823,\n",
       " 909: 824,\n",
       " 8556: 825,\n",
       " 7841: 826,\n",
       " 866: 827,\n",
       " 8264: 828,\n",
       " 1059: 829,\n",
       " 1012: 830,\n",
       " 11224: 831,\n",
       " 814: 832,\n",
       " 7518: 833,\n",
       " 1848: 834,\n",
       " 2179: 835,\n",
       " 872: 836,\n",
       " 7319: 837,\n",
       " 5217: 838,\n",
       " 3076: 839,\n",
       " 3271: 840,\n",
       " 1307: 841,\n",
       " 292: 842,\n",
       " 3988: 843,\n",
       " 1695: 844,\n",
       " 10141: 845,\n",
       " 846: 846,\n",
       " 1387: 847,\n",
       " 8127: 848,\n",
       " 10788: 849,\n",
       " 827: 850,\n",
       " 591: 851,\n",
       " 4513: 852,\n",
       " 6517: 853,\n",
       " 8646: 854,\n",
       " 9679: 855,\n",
       " 4079: 856,\n",
       " 9308: 857,\n",
       " 8161: 858,\n",
       " 10192: 859,\n",
       " 270: 860,\n",
       " 10705: 861,\n",
       " 1422: 862,\n",
       " 9210: 863,\n",
       " 10255: 864,\n",
       " 3439: 865,\n",
       " 10780: 866,\n",
       " 7647: 867,\n",
       " 7234: 868,\n",
       " 10859: 869,\n",
       " 1128: 870,\n",
       " 6040: 871,\n",
       " 9905: 872,\n",
       " 459: 873,\n",
       " 3260: 874,\n",
       " 2439: 875,\n",
       " 6201: 876,\n",
       " 9649: 877,\n",
       " 2324: 878,\n",
       " 9241: 879,\n",
       " 7280: 880,\n",
       " 1167: 881,\n",
       " 10529: 882,\n",
       " 10545: 883,\n",
       " 6611: 884,\n",
       " 3677: 885,\n",
       " 993: 886,\n",
       " 6738: 887,\n",
       " 616: 888,\n",
       " 5796: 889,\n",
       " 2524: 890,\n",
       " 1055: 891,\n",
       " 6823: 892,\n",
       " 9152: 893,\n",
       " 4500: 894,\n",
       " 1304: 895,\n",
       " 112: 896,\n",
       " 5847: 897,\n",
       " 838: 898,\n",
       " 9044: 899,\n",
       " 2551: 900,\n",
       " 10714: 901,\n",
       " 3726: 902,\n",
       " 230: 903,\n",
       " 6889: 904,\n",
       " 3882: 905,\n",
       " 11039: 906,\n",
       " 6188: 907,\n",
       " 715: 908,\n",
       " 4563: 909,\n",
       " 3337: 910,\n",
       " 6032: 911,\n",
       " 11268: 912,\n",
       " 9837: 913,\n",
       " 3515: 914,\n",
       " 7094: 915,\n",
       " 4224: 916,\n",
       " 8605: 917,\n",
       " 3474: 918,\n",
       " 5673: 919,\n",
       " 5106: 920,\n",
       " 4034: 921,\n",
       " 2014: 922,\n",
       " 2668: 923,\n",
       " 8037: 924,\n",
       " 9634: 925,\n",
       " 9346: 926,\n",
       " 10533: 927,\n",
       " 3182: 928,\n",
       " 9806: 929,\n",
       " 2213: 930,\n",
       " 7618: 931,\n",
       " 9324: 932,\n",
       " 5478: 933,\n",
       " 4491: 934,\n",
       " 10: 935,\n",
       " 5258: 936,\n",
       " 9864: 937,\n",
       " 7232: 938,\n",
       " 3190: 939,\n",
       " 8395: 940,\n",
       " 1009: 941,\n",
       " 740: 942,\n",
       " 10834: 943,\n",
       " 594: 944,\n",
       " 2624: 945,\n",
       " 1904: 946,\n",
       " 2184: 947,\n",
       " 9754: 948,\n",
       " 10133: 949,\n",
       " 6575: 950,\n",
       " 4270: 951,\n",
       " 5288: 952,\n",
       " 4362: 953,\n",
       " 3807: 954,\n",
       " 11211: 955,\n",
       " 10805: 956,\n",
       " 918: 957,\n",
       " 5780: 958,\n",
       " 11010: 959,\n",
       " 5078: 960,\n",
       " 8730: 961,\n",
       " 3039: 962,\n",
       " 8426: 963,\n",
       " 1240: 964,\n",
       " 9456: 965,\n",
       " 3055: 966,\n",
       " 5555: 967,\n",
       " 5494: 968,\n",
       " 5675: 969,\n",
       " 8005: 970,\n",
       " 8449: 971,\n",
       " 8164: 972,\n",
       " 1896: 973,\n",
       " 9350: 974,\n",
       " 5292: 975,\n",
       " 5830: 976,\n",
       " 7978: 977,\n",
       " 7658: 978,\n",
       " 888: 979,\n",
       " 8466: 980,\n",
       " 7694: 981,\n",
       " 3552: 982,\n",
       " 2255: 983,\n",
       " 11239: 984,\n",
       " 7093: 985,\n",
       " 9344: 986,\n",
       " 4911: 987,\n",
       " 6494: 988,\n",
       " 2584: 989,\n",
       " 2116: 990,\n",
       " 7427: 991,\n",
       " 5529: 992,\n",
       " 10350: 993,\n",
       " 3224: 994,\n",
       " 3798: 995,\n",
       " 2720: 996,\n",
       " 4812: 997,\n",
       " 2565: 998,\n",
       " 11063: 999,\n",
       " ...}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = newidx \n",
    "labeldict = dict(zip(newidx,idx))\n",
    "\n",
    "labeldict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a2de4c",
   "metadata": {},
   "source": [
    "Next, store the training data in your S3 bucket using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dfce15b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_features shape =  (11314, 30)\n",
      "train_labels shape =  (11314,)\n",
      "20newsgroups/knn/train\n",
      "uploaded training data location: s3://sagemaker-recom-bucket/20newsgroups/knn/train\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import sagemaker.amazon.common as smac\n",
    "\n",
    "\n",
    "print('train_features shape = ', predictions.shape)\n",
    "print('train_labels shape = ', labels.shape)\n",
    "buf = io.BytesIO()\n",
    "smac.write_numpy_to_dense_tensor(buf, predictions, labels)\n",
    "buf.seek(0)\n",
    "\n",
    "bucket = BUCKET\n",
    "prefix = PREFIX\n",
    "key = 'knn/train'\n",
    "fname = os.path.join(prefix, key)\n",
    "print(fname)\n",
    "boto3.resource('s3').Bucket(bucket).Object(fname).upload_fileobj(buf)\n",
    "s3_train_data = 's3://{}/{}/{}'.format(bucket, prefix, key)\n",
    "print('uploaded training data location: {}'.format(s3_train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88258457",
   "metadata": {},
   "source": [
    "Next, use the following helper function to create a k-NN estimator much like the NTM estimator you created in Module 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "81e33cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The method get_image_uri has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "Defaulting to the only supported framework/algorithm version: 1. Ignoring framework/algorithm version: 1.\n",
      "train_instance_count has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_instance_type has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-03 12:41:58 Starting - Starting the training job...\n",
      "2022-03-03 12:42:25 Starting - Preparing the instances for trainingProfilerReport-1646311318: InProgress\n",
      ".........\n",
      "2022-03-03 12:43:48 Downloading - Downloading input data...\n",
      "2022-03-03 12:44:22 Training - Downloading the training image...............\n",
      "2022-03-03 12:47:00 Training - Training image download completed. Training in progress..\u001b[34mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[34mRunning default environment configuration script\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:47:02 INFO 140286078224192] Reading default configuration from /opt/amazon/lib/python3.7/site-packages/algorithm/resources/default-conf.json: {'_kvstore': 'dist_async', '_log_level': 'info', '_num_gpus': 'auto', '_num_kv_servers': '1', '_tuning_objective_metric': '', '_faiss_index_nprobe': '5', 'epochs': '1', 'feature_dim': 'auto', 'faiss_index_ivf_nlists': 'auto', 'index_metric': 'L2', 'index_type': 'faiss.Flat', 'mini_batch_size': '5000', '_enable_profiler': 'false'}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:47:02 INFO 140286078224192] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {'feature_dim': '30', 'index_metric': 'COSINE', 'k': '10', 'predictor_type': 'classifier', 'sample_size': '11314'}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:47:02 INFO 140286078224192] Final configuration: {'_kvstore': 'dist_async', '_log_level': 'info', '_num_gpus': 'auto', '_num_kv_servers': '1', '_tuning_objective_metric': '', '_faiss_index_nprobe': '5', 'epochs': '1', 'feature_dim': '30', 'faiss_index_ivf_nlists': 'auto', 'index_metric': 'COSINE', 'index_type': 'faiss.Flat', 'mini_batch_size': '5000', '_enable_profiler': 'false', 'k': '10', 'predictor_type': 'classifier', 'sample_size': '11314'}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:47:10 WARNING 140286078224192] Loggers have already been setup.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:47:10 INFO 140286078224192 integration.py:636] worker started\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:47:10 INFO 140286078224192] Final configuration: {'_kvstore': 'dist_async', '_log_level': 'info', '_num_gpus': 'auto', '_num_kv_servers': '1', '_tuning_objective_metric': '', '_faiss_index_nprobe': '5', 'epochs': '1', 'feature_dim': '30', 'faiss_index_ivf_nlists': 'auto', 'index_metric': 'COSINE', 'index_type': 'faiss.Flat', 'mini_batch_size': '5000', '_enable_profiler': 'false', 'k': '10', 'predictor_type': 'classifier', 'sample_size': '11314'}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:47:10 WARNING 140286078224192] Loggers have already been setup.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:47:10 INFO 140286078224192] Launching parameter server for role scheduler\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:47:10 INFO 140286078224192] {'ENVROOT': '/opt/amazon', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'HOSTNAME': 'ip-10-2-177-113.ec2.internal', 'TRAINING_JOB_NAME': 'knn-2022-03-03-12-41-58-180', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-east-1:731670586362:training-job/knn-2022-03-03-12-41-58-180', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/proxy-8cf33c7904137c4ef7e89908acdd164498639487890ca2154e3ecdcbcdee5ddc-customer', 'CANONICAL_ENVROOT': '/opt/amazon', 'PYTHONUNBUFFERED': 'TRUE', 'NVIDIA_VISIBLE_DEVICES': 'all', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python3.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'PWD': '/', 'LANG': 'en_US.utf8', 'AWS_REGION': 'us-east-1', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'HOME': '/root', 'SHLVL': '1', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'OMP_NUM_THREADS': '2', 'DMLC_INTERFACE': 'eth0', 'SAGEMAKER_HTTP_PORT': '8080', 'SAGEMAKER_DATA_PATH': '/opt/ml'}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:47:10 INFO 140286078224192] envs={'ENVROOT': '/opt/amazon', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'HOSTNAME': 'ip-10-2-177-113.ec2.internal', 'TRAINING_JOB_NAME': 'knn-2022-03-03-12-41-58-180', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-east-1:731670586362:training-job/knn-2022-03-03-12-41-58-180', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/proxy-8cf33c7904137c4ef7e89908acdd164498639487890ca2154e3ecdcbcdee5ddc-customer', 'CANONICAL_ENVROOT': '/opt/amazon', 'PYTHONUNBUFFERED': 'TRUE', 'NVIDIA_VISIBLE_DEVICES': 'all', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python3.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'PWD': '/', 'LANG': 'en_US.utf8', 'AWS_REGION': 'us-east-1', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'HOME': '/root', 'SHLVL': '1', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'OMP_NUM_THREADS': '2', 'DMLC_INTERFACE': 'eth0', 'SAGEMAKER_HTTP_PORT': '8080', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'DMLC_ROLE': 'scheduler', 'DMLC_PS_ROOT_URI': '10.2.177.113', 'DMLC_PS_ROOT_PORT': '9000', 'DMLC_NUM_SERVER': '1', 'DMLC_NUM_WORKER': '1'}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:47:10 INFO 140286078224192] Launching parameter server for role server\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:47:10 INFO 140286078224192] {'ENVROOT': '/opt/amazon', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'HOSTNAME': 'ip-10-2-177-113.ec2.internal', 'TRAINING_JOB_NAME': 'knn-2022-03-03-12-41-58-180', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-east-1:731670586362:training-job/knn-2022-03-03-12-41-58-180', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/proxy-8cf33c7904137c4ef7e89908acdd164498639487890ca2154e3ecdcbcdee5ddc-customer', 'CANONICAL_ENVROOT': '/opt/amazon', 'PYTHONUNBUFFERED': 'TRUE', 'NVIDIA_VISIBLE_DEVICES': 'all', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python3.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'PWD': '/', 'LANG': 'en_US.utf8', 'AWS_REGION': 'us-east-1', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'HOME': '/root', 'SHLVL': '1', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'OMP_NUM_THREADS': '2', 'DMLC_INTERFACE': 'eth0', 'SAGEMAKER_HTTP_PORT': '8080', 'SAGEMAKER_DATA_PATH': '/opt/ml'}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:47:10 INFO 140286078224192] envs={'ENVROOT': '/opt/amazon', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'HOSTNAME': 'ip-10-2-177-113.ec2.internal', 'TRAINING_JOB_NAME': 'knn-2022-03-03-12-41-58-180', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-east-1:731670586362:training-job/knn-2022-03-03-12-41-58-180', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/proxy-8cf33c7904137c4ef7e89908acdd164498639487890ca2154e3ecdcbcdee5ddc-customer', 'CANONICAL_ENVROOT': '/opt/amazon', 'PYTHONUNBUFFERED': 'TRUE', 'NVIDIA_VISIBLE_DEVICES': 'all', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python3.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'PWD': '/', 'LANG': 'en_US.utf8', 'AWS_REGION': 'us-east-1', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'HOME': '/root', 'SHLVL': '1', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'OMP_NUM_THREADS': '2', 'DMLC_INTERFACE': 'eth0', 'SAGEMAKER_HTTP_PORT': '8080', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'DMLC_ROLE': 'server', 'DMLC_PS_ROOT_URI': '10.2.177.113', 'DMLC_PS_ROOT_PORT': '9000', 'DMLC_NUM_SERVER': '1', 'DMLC_NUM_WORKER': '1'}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:47:10 INFO 140286078224192] Environment: {'ENVROOT': '/opt/amazon', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'HOSTNAME': 'ip-10-2-177-113.ec2.internal', 'TRAINING_JOB_NAME': 'knn-2022-03-03-12-41-58-180', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-east-1:731670586362:training-job/knn-2022-03-03-12-41-58-180', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/proxy-8cf33c7904137c4ef7e89908acdd164498639487890ca2154e3ecdcbcdee5ddc-customer', 'CANONICAL_ENVROOT': '/opt/amazon', 'PYTHONUNBUFFERED': 'TRUE', 'NVIDIA_VISIBLE_DEVICES': 'all', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python3.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'PWD': '/', 'LANG': 'en_US.utf8', 'AWS_REGION': 'us-east-1', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'HOME': '/root', 'SHLVL': '1', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'OMP_NUM_THREADS': '2', 'DMLC_INTERFACE': 'eth0', 'SAGEMAKER_HTTP_PORT': '8080', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'DMLC_ROLE': 'worker', 'DMLC_PS_ROOT_URI': '10.2.177.113', 'DMLC_PS_ROOT_PORT': '9000', 'DMLC_NUM_SERVER': '1', 'DMLC_NUM_WORKER': '1'}\u001b[0m\n",
      "\u001b[34mProcess 50 is a shell:scheduler.\u001b[0m\n",
      "\u001b[34mProcess 59 is a shell:server.\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:47:10 INFO 140286078224192] Using default worker.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:47:10 INFO 140286078224192] Checkpoint loading and saving are disabled.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:47:10 INFO 140286078224192] nvidia-smi: took 0.049 seconds to run.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:47:10 INFO 140286078224192] nvidia-smi identified 0 GPUs.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:47:10 INFO 140286078224192] Create Store: dist_async\u001b[0m\n",
      "\u001b[34m[12:47:10] ../src/base.cc:47: Please install cuda driver for GPU use.  No cuda driver detected.\u001b[0m\n",
      "\u001b[34m[12:47:12] ../src/base.cc:47: Please install cuda driver for GPU use.  No cuda driver detected.\u001b[0m\n",
      "\u001b[34m[12:47:12] ../src/base.cc:47: Please install cuda driver for GPU use.  No cuda driver detected.\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:47:12 ERROR 140286078224192] nvidia-smi: failed to run (127): b'/bin/sh: nvidia-smi: command not found'/\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:47:12 WARNING 140286078224192] Could not determine free memory in MB for GPU device with ID (0).\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:47:12 INFO 140286078224192] Using per-worker sample size = 11314 (Available virtual memory = 15145578496 bytes, GPU free memory = 0 bytes, number of workers = 1). If an out-of-memory error occurs, choose a larger instance type, use dimension reduction, decrease sample_size, and/or decrease mini_batch_size.\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646311632.4941099, \"EndTime\": 1646311632.494188, \"Dimensions\": {\"Algorithm\": \"AWS/KNN\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"Meta\": \"init_train_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Total Batches Seen\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Max Records Seen Between Resets\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Max Batches Seen Between Resets\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Reset Count\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Number of Records Since Last Reset\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Number of Batches Since Last Reset\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}}}\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:47:12.494] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 0, \"duration\": 1726, \"num_examples\": 1, \"num_bytes\": 1440000}\u001b[0m\n",
      "\u001b[34m[2022-03-03 12:47:12.706] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 1, \"duration\": 211, \"num_examples\": 3, \"num_bytes\": 3258432}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:47:12 INFO 140286078224192] #progress_metric: host=algo-1, completed 100.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646311632.4949398, \"EndTime\": 1646311632.7069054, \"Dimensions\": {\"Algorithm\": \"AWS/KNN\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 0, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 11314.0, \"count\": 1, \"min\": 11314, \"max\": 11314}, \"Total Batches Seen\": {\"sum\": 3.0, \"count\": 1, \"min\": 3, \"max\": 3}, \"Max Records Seen Between Resets\": {\"sum\": 11314.0, \"count\": 1, \"min\": 11314, \"max\": 11314}, \"Max Batches Seen Between Resets\": {\"sum\": 3.0, \"count\": 1, \"min\": 3, \"max\": 3}, \"Reset Count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Number of Records Since Last Reset\": {\"sum\": 11314.0, \"count\": 1, \"min\": 11314, \"max\": 11314}, \"Number of Batches Since Last Reset\": {\"sum\": 3.0, \"count\": 1, \"min\": 3, \"max\": 3}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:47:12 INFO 140286078224192] #throughput_metric: host=algo-1, train throughput=53333.523034146056 records/second\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:47:12 INFO 140286078224192] Using in-memory reservoir sample from master machine...\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:47:12 INFO 140286078224192] ...Got reservoir sample from algo-1: data=(11314, 30), labels=(11314,), NaNs=0\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:47:12 INFO 140286078224192] Training index...\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:47:12 INFO 140286078224192] ...Finished training index in 0 second(s)\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:47:12 INFO 140286078224192] Adding data to index...\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:47:12 INFO 140286078224192] ...Finished adding data to index in 0 second(s)\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646311630.7672453, \"EndTime\": 1646311632.7525628, \"Dimensions\": {\"Algorithm\": \"AWS/KNN\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"initialize.time\": {\"sum\": 1707.1826457977295, \"count\": 1, \"min\": 1707.1826457977295, \"max\": 1707.1826457977295}, \"epochs\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"update.time\": {\"sum\": 211.6084098815918, \"count\": 1, \"min\": 211.6084098815918, \"max\": 211.6084098815918}, \"finalize.time\": {\"sum\": 33.85496139526367, \"count\": 1, \"min\": 33.85496139526367, \"max\": 33.85496139526367}, \"model.serialize.time\": {\"sum\": 11.313438415527344, \"count\": 1, \"min\": 11.313438415527344, \"max\": 11.313438415527344}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:47:12 INFO 140286078224192] Test data is not provided.\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1646311632.7526777, \"EndTime\": 1646311632.7557116, \"Dimensions\": {\"Algorithm\": \"AWS/KNN\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"setuptime\": {\"sum\": 21.789073944091797, \"count\": 1, \"min\": 21.789073944091797, \"max\": 21.789073944091797}, \"totaltime\": {\"sum\": 2639.7645473480225, \"count\": 1, \"min\": 2639.7645473480225, \"max\": 2639.7645473480225}}}\u001b[0m\n",
      "\u001b[34m[03/03/2022 12:47:12 INFO 140286078224192 integration.py:636] worker closed\u001b[0m\n",
      "\n",
      "2022-03-03 12:47:22 Uploading - Uploading generated training model\n",
      "2022-03-03 12:47:42 Completed - Training job completed\n",
      "ProfilerReport-1646311318: NoIssuesFound\n",
      "Training seconds: 223\n",
      "Billable seconds: 223\n"
     ]
    }
   ],
   "source": [
    "def trained_estimator_from_hyperparams(s3_train_data, hyperparams, output_path, s3_test_data=None):\n",
    "    \"\"\"\n",
    "    Create an Estimator from the given hyperparams, fit to training data, \n",
    "    and return a deployed predictor\n",
    "    \n",
    "    \"\"\"\n",
    "    # set up the estimator\n",
    "    knn = sagemaker.estimator.Estimator(get_image_uri(boto3.Session().region_name, \"knn\"),\n",
    "        get_execution_role(),\n",
    "        train_instance_count=1,\n",
    "        train_instance_type='ml.m4.xlarge',\n",
    "        output_path=output_path,\n",
    "        sagemaker_session=sagemaker.Session(),\n",
    "        use_spot_instances=True,# Shared instances amount of time we can wait for instance to be available\n",
    "        max_run=300,\n",
    "        max_wait=600)\n",
    "    knn.set_hyperparameters(**hyperparams)\n",
    "    \n",
    "    # train a model. fit_input contains the locations of the train and test data\n",
    "    fit_input = {'train': s3_train_data}\n",
    "    knn.fit(fit_input)\n",
    "    return knn\n",
    "\n",
    "hyperparams = {\n",
    "    'feature_dim': predictions.shape[1],\n",
    "    'k': NUM_NEIGHBORS,\n",
    "    'sample_size': predictions.shape[0],\n",
    "    'predictor_type': 'classifier' ,\n",
    "    'index_metric':'COSINE'\n",
    "}\n",
    "output_path = 's3://' + bucket + '/' + prefix + '/knn/output'\n",
    "knn_estimator = trained_estimator_from_hyperparams(s3_train_data, hyperparams, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4988cae7",
   "metadata": {},
   "source": [
    "While the training job runs, take a closer look at the parameters in the helper function.\n",
    "\n",
    "The Amazon SageMaker k-NN algorithm offers a number of different distance metrics for calculating the nearest neighbors. One popular metric that is used in natural language processing is the cosine distance. Mathematically, the cosine “similarity” between two vectors A and B is given by the following equation:\n",
    "\n",
    "![similarity](content-rec-cosine.png)\n",
    "\n",
    "By setting the index_metric to COSINE, Amazon SageMaker automatically uses the cosine similarity for computing the nearest neighbors. The default distance is the L2 norm, which is the standard Euclidean distance. Note that, at publication, COSINE is only supported for faiss.IVFFlat index type and not the faiss.IVFPQ indexing method.\n",
    "\n",
    "Success! Since you want this model to return the nearest neighbors given a particular test topic, you need to deploy it as a live hosted endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13fd28a",
   "metadata": {},
   "source": [
    "### 2. Deploy the content recommendation model\n",
    "\n",
    "As you did with the NTM model, define the following helper function for the k-NN model to launch the endpoint. In the helper function, the accept token **applications/jsonlines; verbose=true** tells the k-NN model to return all the cosine distances instead of just the closest neighbor. To build a recommendation engine, you need to get the top-k suggestions by the model, for which youneed to set the verbose parameter to **true**, instead of the default, false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "567a6e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up the endpoint..\n",
      "-------------!"
     ]
    }
   ],
   "source": [
    "#accept=\"application/jsonlines; verbose=true\",\n",
    "def predictor_from_estimator(knn_estimator, estimator_name, instance_type, endpoint_name=None): \n",
    "    \n",
    "    \n",
    "    knn_predictor = knn_estimator.deploy(initial_instance_count=1, instance_type=instance_type,\n",
    "                               endpoint_name=endpoint_name,\n",
    "                               serializer=CSVSerializer(),\n",
    "                               deserializer=JSONDeserializer())\n",
    "    #knn_predictor.content_type = 'text/csv'\n",
    "    #knn_predictor.serializer = csv_serializer\n",
    "    #knn_predictor.deserializer = json_deserializer\n",
    "    return knn_predictor\n",
    "import time\n",
    "\n",
    "instance_type = 'ml.m4.xlarge'\n",
    "model_name = 'knn_%s'% instance_type\n",
    "endpoint_name = 'knn-ml-m4-xlarge-%s'% (str(time.time()).replace('.','-'))\n",
    "print('setting up the endpoint..')\n",
    "knn_predictor = predictor_from_estimator(knn_estimator, model_name, instance_type, endpoint_name=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf154bc7",
   "metadata": {},
   "source": [
    "Next, preprocess the test data so that you can run inferences.\n",
    "Copy and paste the following code into your notebook and choose Run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bc99c342",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'predictions': [{'predicted_label': 699.0}]}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "40f1ba5a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-c872696482e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtopic\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_topics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mknn_predictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mcur_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mtopic_predictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_predictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'labels'"
     ]
    }
   ],
   "source": [
    "def preprocess_input(text):\n",
    "    text = strip_newsgroup_header(text)\n",
    "    text = strip_newsgroup_quoting(text)\n",
    "    text = strip_newsgroup_footer(text)\n",
    "    return text    \n",
    "    \n",
    "test_data_prep = []\n",
    "for i in range(len(newsgroups_test)):\n",
    "    test_data_prep.append(preprocess_input(newsgroups_test[i]))\n",
    "test_vectors = vectorizer.fit_transform(test_data_prep)\n",
    "\n",
    "test_vectors = np.array(test_vectors.todense())\n",
    "test_topics = []\n",
    "for vec in test_vectors:\n",
    "    test_result = ntm_predictor.predict(vec)\n",
    "    test_topics.append(test_result['predictions'][0]['topic_weights'])\n",
    "\n",
    "topic_predictions = []\n",
    "for topic in test_topics:\n",
    "    result = knn_predictor.predict(topic)\n",
    "    cur_predictions = np.array([int(result['labels'][i]) for i in range(len(result['labels']))])\n",
    "    topic_predictions.append(cur_predictions[::-1][:10]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4c9f44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "65c759b8",
   "metadata": {},
   "source": [
    "In the last step of this module, you explore your content recommendation model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a280505",
   "metadata": {},
   "source": [
    "### 3. Explore content recommendation model\n",
    "\n",
    "Now that you've obtained the predictions, you can plot the topic distributions of the test topics, compared to the closest k topics recommended by the k-NN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2d5a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set your own k.\n",
    "def plot_topic_distribution(topic_num, k = 5):\n",
    "    \n",
    "    closest_topics = [predictions[labeldict[x]] for x in topic_predictions[topic_num][:k]]\n",
    "    closest_topics.append(np.array(test_topics[topic_num]))\n",
    "    closest_topics = np.array(closest_topics)\n",
    "    df = pd.DataFrame(closest_topics.T)\n",
    "    df.rename(columns ={k:\"Test Document Distribution\"}, inplace=True)\n",
    "    fs = 12\n",
    "    df.plot(kind='bar', figsize=(16,4), fontsize=fs)\n",
    "    plt.ylabel('Topic assignment', fontsize=fs+2)\n",
    "    plt.xlabel('Topic ID', fontsize=fs+2)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31817a0",
   "metadata": {},
   "source": [
    "Run the following code to plot the topic distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359a6ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_topic_distribution(18)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173ec1ff",
   "metadata": {},
   "source": [
    "Now, try some other topics. Run the following code cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d469b0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_topic_distribution(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8ca3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_topic_distribution(5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc1cf3a",
   "metadata": {},
   "source": [
    "Your plots may look somewhat different based on the number of topics (NUM_TOPICS) you choose. But overall, these plots show that the topic distribution of the nearest neighbor documents found using Cosine similarity by the k-NN model is pretty similar to the topic distribution of the test document we fed into the model.\n",
    "\n",
    "The results suggest that k-NN may be a good way to build a semantic based information retrieval system by first embedding the documents into topic vectors and then using a k-NN model to serve the recommendations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
