{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dicovering Interpretable features\n",
    "\n",
    "In this chapter, you'll learn about a dimension reduction technique called \"Non-negative matrix factorization\" (\"NMF\") that expresses samples as combinations of interpretable parts. For example, it expresses documents as combinations of topics, and images in terms of commonly occurring visual patterns. You'll also learn to use NMF to build recommender systems that can find you similar articles to read, or musical artists that match your listening history!\n",
    "\n",
    "## Non-negative matrix factorization(NMF)\n",
    "\n",
    "* NMF is a dimension reduction techique\n",
    "* NMF models are interpretable (unlike PCA)\n",
    "* However, all sample features must be non-negative(>=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this toy dataset, in slide 6 (Discovering... pdf) there are only 4 words in the vocabulary, and these correspond to the four columns of the word-frequency array. Each row represents a document, and the entries of the array measure the frequency of each word in the document using what's known as \"tf-idf\". \"tf\" is the frequency of the word in the document. So if 10% of the words in the document are \"datacamp\", then the tf of \"datacamp\" for that document is 0.1. \"idf\" is a weighting scheme that reduces the influence of frequent words like \"the\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
